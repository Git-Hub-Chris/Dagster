{
  "parents": [{ "link": "../", "title": "Deploying Dagster" }],
  "prev": { "link": "../", "title": "Deploying Dagster" },
  "next": { "link": "../instance/", "title": "Configuring an instance" },
  "title": "Architectural overview",
  "meta": {},
  "body": "<div class=\"section\" id=\"architectural-overview\">\n<h1>Architectural overview<a class=\"headerlink\" href=\"#architectural-overview\" title=\"Permalink to this headline\">\u00b6</a></h1>\n<p>Dagster is a layered and pluggable system. It is possible to call the Dagster Python APIs directly\nfrom your own code, to call the <code class=\"docutils literal notranslate\"><span class=\"pre\">dagster</span></code> CLI, to execute GraphQL queries against Dagster using\nthe <code class=\"docutils literal notranslate\"><span class=\"pre\">dagster-graphql</span></code> CLI, to run <code class=\"docutils literal notranslate\"><span class=\"pre\">dagster-graphql</span></code> in containers that can respond to GraphQL\nqueries, to run Dagit on a standalone basis, or to compile Dagster DAGs for scheduling and execution\non Airflow. Individual pipeline runs may be executed on pluggable execution engines, including local\nor remote Dask clusters. Metadata from these executions can be streamed to pluggable local and\nremote storage backends.</p>\n<p>This allows substantial flexibility in your deployment strategies. For example, it is\npossible to point a local instance of Dagit, running on an individual developer\u2019s machine, at the\ncloud storage being used by pipelines scheduled in production in order to inspect intermediate\nartifacts.</p>\n<div class=\"section\" id=\"execution\">\n<h2>Execution<a class=\"headerlink\" href=\"#execution\" title=\"Permalink to this headline\">\u00b6</a></h2>\n<p>Dagster pipelines can be executed in a single process, in multiple processes, or on a variety of\ndistributed compute platforms, by selecting between available executors at pipeline execution time\nusing config. This makes it possible to run a pipeline locally in a single process and then remote\non a production cluster just by switching config settings in Dagit or in the environment dict\nprovided to the Python API.</p>\n<p>Dagster includes out-of-the-box support for local execution in a single process and in multiple\nprocesses with the <a class=\"reference internal\" href=\"../../api/apidocs/execution/#dagster.in_process_executor\" title=\"dagster.in_process_executor\"><code class=\"xref py py-data docutils literal notranslate\"><span class=\"pre\">in_process_executor</span></code></a> and\n<a class=\"reference internal\" href=\"../../api/apidocs/execution/#dagster.multiprocess_executor\" title=\"dagster.multiprocess_executor\"><code class=\"xref py py-data docutils literal notranslate\"><span class=\"pre\">multiprocess_executor</span></code></a>. These executors work well for pipelines of moderate\nsize or if your solids communicate with external systems or clusters (e.g., EMR or Dataproc) to\nrun heavy compute workloads.</p>\n<p>These executors are available by default when executing a pipeline using any\n<a class=\"reference internal\" href=\"../../api/apidocs/pipeline/#dagster.ModeDefinition\" title=\"dagster.ModeDefinition\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">ModeDefinition</span></code></a> that does not define its own executors. By default, in the\nabsence of specific executor config, the in-process executor will be used. To select the\nmultiprocess executor, add a fragment like the following to the config of any pipeline:</p>\n<div class=\"highlight-yaml notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"nt\">execution</span><span class=\"p\">:</span>\n  <span class=\"nt\">multiprocess</span><span class=\"p\">:</span>\n    <span class=\"nt\">max_concurrent</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">4</span>\n<span class=\"nt\">storage</span><span class=\"p\">:</span>\n  <span class=\"nt\">filesystem</span><span class=\"p\">:</span>\n</pre></div>\n</div>\n<p>Note that a persistent system storage, such as the filesystem storage, must be configured in order\nto make multiprocess execution available. This persistent system storage is used to pass\nintermediate values between solids, and incidentally makes reexecution available for all\nmultiprocess executions.</p>\n<p>The <a class=\"reference external\" href=\"https://github.com/dagster-io/dagster/tree/master/python_modules/dagster-dask\">dagster-dask</a>\nmodule makes a <a class=\"reference internal\" href=\"../../api/apidocs/dagster_dask/#dagster_dask.dask_executor\" title=\"dagster_dask.dask_executor\"><code class=\"xref py py-data docutils literal notranslate\"><span class=\"pre\">dask_executor</span></code></a> available, which can target either a local\nDask cluster or a distributed cluster. Computation is distributed across the cluster at the\nexecution step level. This is a straightforward path to testable and scalable distributed\nexecution for heavier workloads.</p>\n<p>As with the multiprocess executor, a persistent system storage must be configured for Dask\nexecution.</p>\n<p>Users can also write their own executors, which can be passed to the <code class=\"docutils literal notranslate\"><span class=\"pre\">executor_defs</span></code> argument on\n<a class=\"reference internal\" href=\"../../api/apidocs/pipeline/#dagster.ModeDefinition\" title=\"dagster.ModeDefinition\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">ModeDefinition</span></code></a>. If you\u2019re considering doing this, please reach out through our\nSlack channel so that we can provide guidance and support.</p>\n</div>\n<div class=\"section\" id=\"scheduling\">\n<h2>Scheduling<a class=\"headerlink\" href=\"#scheduling\" title=\"Permalink to this headline\">\u00b6</a></h2>\n<p>Dagster\u2019s approach to scheduling pipelines for periodic execution is also oriented toward\nextensibility. Schedules are defined in code using the <a class=\"reference internal\" href=\"../../api/apidocs/schedules/#dagster.schedules\" title=\"dagster.schedules\"><code class=\"xref py py-func docutils literal notranslate\"><span class=\"pre\">&#64;schedules</span></code></a>\nAPI and may be executed by multiple concrete schedulers.</p>\n<p>The first scheduler we\u2019ve built is in the\n<a class=\"reference external\" href=\"https://github.com/dagster-io/dagster/tree/master/python_modules/libraries/dagster-cron\">dagster-cron</a>\npackage and is backed by system cron, the <a class=\"reference internal\" href=\"../../api/apidocs/dagster_cron/#dagster_cron.SystemCronScheduler\" title=\"dagster_cron.SystemCronScheduler\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">SystemCronScheduler</span></code></a>. (See the\n<a class=\"reference external\" href=\"scheduling-pipeline-runs\">tutorial docs</a> for an example of how to schedule pipeline executions\nusing the cron-backed scheduler.)</p>\n<p>Users can also write their own schedulers. If you\u2019re considering doing this, please reach out\nthrough our Slack channel so that we can provide guidance and support.</p>\n<div class=\"section\" id=\"compiling-a-pipeline-for-execution-by-a-third-party-scheduler\">\n<h3>Compiling a pipeline for execution by a third-party scheduler<a class=\"headerlink\" href=\"#compiling-a-pipeline-for-execution-by-a-third-party-scheduler\" title=\"Permalink to this headline\">\u00b6</a></h3>\n<p>It\u2019s also possible to schedule pipelines for execution by compiling them to a format that can be\nunderstood by a third-party scheduling system, and then defining schedules within that system.</p>\n<p>This is the approach we use to deploy Dagster pipelines to Airflow (using the\n<a class=\"reference external\" href=\"https://github.com/dagster-io/dagster/tree/master/python_modules/dagster-airflow\">dagster-airflow</a>\npackage).</p>\n<p>A Dagster pipeline is first compiled with a set of config options into an execution plan,\nand then the individual execution steps are expressed as Airflow tasks using a set of custom wrapper\noperators. The resulting DAG can be deployed to an existing Airflow install and scheduled and\nmonitored using all the tools being used to existing pipelines (See the\n<a class=\"reference external\" href=\"deploying-to-airflow\">Airflow guide</a> for details.)</p>\n<p>If you\u2019re thinking of building a similar integration to target another third-party scheduler, please\nreach out through our Slack channel so that we can provide guidance and support.</p>\n</div>\n</div>\n<div class=\"section\" id=\"storage\">\n<h2>Storage<a class=\"headerlink\" href=\"#storage\" title=\"Permalink to this headline\">\u00b6</a></h2>\n<p>The Dagster tools are built so that the storage backends they use can be easily swapped. This makes\nit easy to swap S3 for GCP (or cloud storage for local) or Postgres for MySQL, guarding against\nlock-in and ensuring compatibility with a wide range of heterogeneous infrastructures. It also\nmakes some neat things possible. For example, a user running a local Dagit can point it at remote\nstorage backends in order to debug or monitor runs being executed on production infrastructure.</p>\n<div class=\"section\" id=\"the-dagsterinstance\">\n<h3>The DagsterInstance<a class=\"headerlink\" href=\"#the-dagsterinstance\" title=\"Permalink to this headline\">\u00b6</a></h3>\n<p>The <a class=\"reference internal\" href=\"../../api/apidocs/internals/#dagster.core.instance.DagsterInstance\" title=\"dagster.core.instance.DagsterInstance\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">DagsterInstance</span></code></a> organizes all of the information specific to\na particular installation or deployment of Dagster. (Locally, this usually means a particular Dagit\nprocess.)</p>\n<p>An instance controls the collection of systems that are used by Dagster for persisting\ndeployment-wide information: the history of past runs, the log of structured events created by\nthose runs, the raw stdout and stderr streams created by those runs, and configuration for the local\nstorage of intermediates.</p>\n<p>These systems are swappable in config, and users can write their own classes to handle persistence\nof any or all of this data. See the <a class=\"reference external\" href=\"configuring-an-instance\">instance guide</a> for details on\nhow to configure and customize the instance. (As always, if you\u2019re interested in extending Dagster,\nplease reach out to us.)</p>\n</div>\n<div class=\"section\" id=\"system-storage-for-intermediate-artifacts\">\n<h3>System storage for intermediate artifacts<a class=\"headerlink\" href=\"#system-storage-for-intermediate-artifacts\" title=\"Permalink to this headline\">\u00b6</a></h3>\n<p>Intermediate persistence is configurable on a per-pipeline run basis. This is so that you can run\npure in-memory tests which don\u2019t persist anything, local runs that persist artifacts to disk for\ndebugging and inspection, and production runs that persist to permanent cloud storage for audit and\nreproducibility.</p>\n<p>Intermediate persistence is governed by subclasses of <a class=\"reference internal\" href=\"../../api/apidocs/internals/#dagster.SystemStorageDefinition\" title=\"dagster.SystemStorageDefinition\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">SystemStorageDefinition</span></code></a>,\nwhich can be attached to a <a class=\"reference internal\" href=\"../../api/apidocs/pipeline/#dagster.ModeDefinition\" title=\"dagster.ModeDefinition\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">ModeDefinition</span></code></a>. By default, the</p>\n</div>\n</div>\n</div>\n",
  "metatags": "",
  "rellinks": [
    ["genindex", "General Index", "I", "index"],
    ["py-modindex", "Python Module Index", "", "modules"],
    ["sections/deploying/instance", "Configuring an instance", "N", "next"],
    ["sections/deploying/index", "Deploying Dagster", "P", "previous"]
  ],
  "sourcename": "sections/deploying/deploying.rst.txt",
  "toc": "<ul>\n<li><a class=\"reference internal\" href=\"#\">Architectural overview</a><ul>\n<li><a class=\"reference internal\" href=\"#execution\">Execution</a></li>\n<li><a class=\"reference internal\" href=\"#scheduling\">Scheduling</a><ul>\n<li><a class=\"reference internal\" href=\"#compiling-a-pipeline-for-execution-by-a-third-party-scheduler\">Compiling a pipeline for execution by a third-party scheduler</a></li>\n</ul>\n</li>\n<li><a class=\"reference internal\" href=\"#storage\">Storage</a><ul>\n<li><a class=\"reference internal\" href=\"#the-dagsterinstance\">The DagsterInstance</a></li>\n<li><a class=\"reference internal\" href=\"#system-storage-for-intermediate-artifacts\">System storage for intermediate artifacts</a></li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n",
  "display_toc": true,
  "page_source_suffix": ".rst",
  "current_page_name": "sections/deploying/deploying",
  "sidebars": ["globaltoc.html", "searchbox.html"],
  "customsidebar": null,
  "alabaster_version": "0.7.12"
}
