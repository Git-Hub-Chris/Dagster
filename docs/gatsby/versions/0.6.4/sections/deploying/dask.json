{
  "parents": [{ "link": "../deploying/", "title": "Deploying" }],
  "prev": { "link": "../airflow/", "title": "Deploying to Airflow" },
  "next": { "link": "../../community/community/", "title": "Community" },
  "title": "Executing on Dask",
  "meta": {},
  "body": "<div class=\"section\" id=\"executing-on-dask\">\n<span id=\"executing-on-dask\"></span><h1>Executing on Dask<a class=\"headerlink\" href=\"#executing-on-dask\" title=\"Permalink to this headline\">\u00b6</a></h1>\n<div class=\"section\" id=\"introduction\">\n<span id=\"introduction\"></span><h2>Introduction<a class=\"headerlink\" href=\"#introduction\" title=\"Permalink to this headline\">\u00b6</a></h2>\n<p>Dagster is designed to target a variety of execution substrates, and natively\nsupports Dask for pipeline execution.</p>\n<p>The Dagster / Dask integration lets you execute a Dagster pipeline on either local Dask or on a\nremote Dask cluster by specifying <code class=\"docutils literal notranslate\"><span class=\"pre\">dask</span></code> in the <code class=\"docutils literal notranslate\"><span class=\"pre\">execution</span></code> block of the environment config.</p>\n<p>The integration works by taking the compiled execution plan, and converting each execution step\ninto a <a class=\"reference external\" href=\"https://docs.dask.org/en/latest/futures.html\">Dask Future</a> configured with the appropriate\ntask dependencies to ensure tasks are properly sequenced. When the pipeline is executed, these\nfutures are generated and then awaited by the parent Dagster process.</p>\n<p>Data is passed between step executions via intermediate storage. As a consequence, a persistent\nshared storage must be used in a distributed execution context.</p>\n<div class=\"section\" id=\"requirements\">\n<span id=\"requirements\"></span><h3>Requirements<a class=\"headerlink\" href=\"#requirements\" title=\"Permalink to this headline\">\u00b6</a></h3>\n<p>To use <code class=\"docutils literal notranslate\"><span class=\"pre\">dagster-dask</span></code>, you\u2019ll need to install\n<a class=\"reference external\" href=\"https://distributed.readthedocs.io/en/latest/install.html\">Dask / Dask.Distributed</a>.</p>\n</div>\n</div>\n<div class=\"section\" id=\"local-execution\">\n<span id=\"local-execution\"></span><h2>Local Execution<a class=\"headerlink\" href=\"#local-execution\" title=\"Permalink to this headline\">\u00b6</a></h2>\n<p>It is relatively straightforward to set up and run a Dagster pipeline on Dask.</p>\n<p>First, run <code class=\"docutils literal notranslate\"><span class=\"pre\">pip</span> <span class=\"pre\">install</span> <span class=\"pre\">dagster</span> <span class=\"pre\">dagster-dask</span></code>.</p>\n<p>Then:</p>\n<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"c1\"># dask_hello_world.py</span>\n\n<span class=\"n\">python</span>\n<span class=\"kn\">from</span> <span class=\"nn\">dagster</span> <span class=\"kn\">import</span> <span class=\"n\">execute_pipeline</span><span class=\"p\">,</span> <span class=\"n\">ExecutionTargetHandle</span><span class=\"p\">,</span> <span class=\"n\">ModeDefinition</span><span class=\"p\">,</span> <span class=\"n\">pipeline</span><span class=\"p\">,</span> <span class=\"n\">solid</span>\n<span class=\"kn\">from</span> <span class=\"nn\">dagster.core.definitions.executor</span> <span class=\"kn\">import</span> <span class=\"n\">default_executors</span>\n<span class=\"kn\">from</span> <span class=\"nn\">dagster_dask</span> <span class=\"kn\">import</span> <span class=\"n\">dask_executor</span>\n\n\n<span class=\"nd\">@solid</span>\n<span class=\"k\">def</span> <span class=\"nf\">hello_world</span><span class=\"p\">(</span><span class=\"n\">_</span><span class=\"p\">):</span>\n    <span class=\"k\">return</span> <span class=\"s2\">&quot;Hello, World!&quot;</span>\n\n\n<span class=\"nd\">@pipeline</span><span class=\"p\">(</span><span class=\"n\">mode_defs</span><span class=\"o\">=</span><span class=\"n\">ModeDefinition</span><span class=\"p\">(</span><span class=\"n\">executor_defs</span><span class=\"o\">=</span><span class=\"n\">default_executors</span> <span class=\"o\">+</span> <span class=\"p\">[</span><span class=\"n\">dask_executor</span><span class=\"p\">]))</span>\n<span class=\"k\">def</span> <span class=\"nf\">dask_pipeline</span><span class=\"p\">():</span>\n    <span class=\"k\">return</span> <span class=\"n\">hello_world</span><span class=\"p\">()</span>\n\n\n<span class=\"n\">execute_pipeline</span><span class=\"p\">(</span>\n    <span class=\"n\">ExecutionTargetHandle</span><span class=\"o\">.</span><span class=\"n\">for_pipeline_python_file</span><span class=\"p\">(</span><span class=\"vm\">__file__</span><span class=\"p\">,</span> <span class=\"s1\">&#39;dask_pipeline&#39;</span><span class=\"p\">),</span>\n    <span class=\"n\">env_config</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s1\">&#39;storage&#39;</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s1\">&#39;filesystem&#39;</span><span class=\"p\">:</span> <span class=\"p\">{}},</span> <span class=\"s1\">&#39;execution&#39;</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s1\">&#39;dask&#39;</span><span class=\"p\">:</span> <span class=\"p\">{}}},</span>\n<span class=\"p\">)</span>\n</pre></div>\n</div>\n<p>Running <code class=\"docutils literal notranslate\"><span class=\"pre\">python</span> <span class=\"pre\">dask_hello_world.py</span></code> will spin up local Dask execution, run the Dagster pipeline,\nand exit.</p>\n</div>\n<div class=\"section\" id=\"distributed-cluster-execution\">\n<span id=\"distributed-cluster-execution\"></span><h2>Distributed Cluster Execution<a class=\"headerlink\" href=\"#distributed-cluster-execution\" title=\"Permalink to this headline\">\u00b6</a></h2>\n<p>If you want to use a Dask cluster for distributed execution, you will first need to\n<a class=\"reference external\" href=\"https://distributed.readthedocs.io/en/latest/quickstart.html#setup-dask-distributed-the-hard-way\">set up a Dask cluster</a>.\nNote that the machine running the Dagster parent process must have access to the host/port on which\nthe Dask scheduler is running.</p>\n<p>For distributing task execution on a Dask cluster, you must provide a <code class=\"docutils literal notranslate\"><span class=\"pre\">DaskConfig</span></code> object with\nthe address/port of the Dask scheduler:</p>\n<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"n\">execute_pipeline</span><span class=\"p\">(</span>\n    <span class=\"n\">ExecutionTargetHandle</span><span class=\"o\">.</span><span class=\"n\">for_pipeline_module</span><span class=\"p\">(</span><span class=\"s1\">&#39;your.python.module&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;your_pipeline_name&#39;</span><span class=\"p\">),</span>\n    <span class=\"n\">env_config</span><span class=\"o\">=</span><span class=\"p\">{</span>\n        <span class=\"s1\">&#39;storage&#39;</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s1\">&#39;s3&#39;</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s1\">&#39;config&#39;</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s1\">&#39;s3_bucket&#39;</span><span class=\"p\">:</span> <span class=\"s1\">&#39;YOUR_BUCKET_HERE&#39;</span><span class=\"p\">}}},</span>\n        <span class=\"s1\">&#39;execution&#39;</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s1\">&#39;dask&#39;</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s1\">&#39;config&#39;</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s1\">&#39;address&#39;</span><span class=\"p\">:</span> <span class=\"s1\">&#39;dask_scheduler.dns-name:8787&#39;</span><span class=\"p\">}}}</span>\n    <span class=\"p\">},</span>\n<span class=\"p\">)</span>\n</pre></div>\n</div>\n<p>Since Dask will invoke your pipeline code on the cluster workers, you must ensure that the latest\nversion of your Python code is available to all of the Dask workers\u2014ideally packaged as a Python\nmodule <code class=\"docutils literal notranslate\"><span class=\"pre\">your.python.module</span></code> that is importable on <code class=\"docutils literal notranslate\"><span class=\"pre\">PYTHONPATH</span></code>.</p>\n</div>\n<div class=\"section\" id=\"dask-resources\">\n<span id=\"dask-resources\"></span><h2>Dask Resources<a class=\"headerlink\" href=\"#dask-resources\" title=\"Permalink to this headline\">\u00b6</a></h2>\n<p>Dask has <a class=\"reference external\" href=\"https://distributed.dask.org/en/latest/resources.html\">basic support</a> for resource\nmanagement. In Dask you can specify that a particular worker node has, say, 3 GPUs, and then tasks\nwhich are specified with GPU requirements will be scheduled to respect that constraint on available\nresources.</p>\n<p>In Dask, you\u2019d set this up by launching your workers with resource specifications:</p>\n<div class=\"highlight-bash notranslate\"><div class=\"highlight\"><pre><span></span>dask-worker scheduler:8786 --resources <span class=\"s2\">&quot;GPU=2&quot;</span>\n</pre></div>\n</div>\n<p>and then when submitting tasks to the Dask cluster, specifying resource requirements:</p>\n<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"n\">client</span><span class=\"o\">.</span><span class=\"n\">submit</span><span class=\"p\">(</span><span class=\"n\">task</span><span class=\"p\">,</span> <span class=\"n\">resources</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s1\">&#39;GPU&#39;</span><span class=\"p\">:</span> <span class=\"mi\">1</span><span class=\"p\">})</span>\n</pre></div>\n</div>\n<p>Dagster has simple support for Dask resource specification at the solid level for solids that will\nbe executed on Dask clusters. In your solid definition, just add a <code class=\"docutils literal notranslate\"><span class=\"pre\">step_metadata_fn</span></code> as follows:</p>\n<div class=\"highlight-python notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"nd\">@solid</span><span class=\"p\">(</span>\n    <span class=\"o\">...</span>\n    <span class=\"n\">step_metadata_fn</span><span class=\"o\">=</span><span class=\"k\">lambda</span> <span class=\"n\">_</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s1\">&#39;dagster-dask/resource_requirements&#39;</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s1\">&#39;GPU&#39;</span><span class=\"p\">:</span> <span class=\"mi\">1</span><span class=\"p\">}},</span>\n<span class=\"p\">)</span>\n<span class=\"k\">def</span> <span class=\"nf\">my_task</span><span class=\"p\">(</span><span class=\"n\">context</span><span class=\"p\">):</span>\n    <span class=\"k\">pass</span>\n</pre></div>\n</div>\n<p>And these requirements will be passed along to Dask when executed on a Dask cluster. Note that in\nnon-Dask contexts, this key will be ignored.</p>\n</div>\n<div class=\"section\" id=\"limitations\">\n<span id=\"limitations\"></span><h2>Limitations<a class=\"headerlink\" href=\"#limitations\" title=\"Permalink to this headline\">\u00b6</a></h2>\n<ul class=\"simple\">\n<li><p>For distributed execution, you must use S3 for intermediates and run storage, as shown above.</p></li>\n<li><p>Dagster logs are not yet retrieved from Dask workers; this will be addressed in follow-up work.</p></li>\n</ul>\n<p>While this library is still nascent, we\u2019re working to improve it, and we are happy to accept\ncontributions.</p>\n</div>\n</div>\n",
  "metatags": "",
  "rellinks": [
    ["genindex", "General Index", "I", "index"],
    ["py-modindex", "Python Module Index", "", "modules"],
    ["sections/community/community", "Community", "N", "next"],
    ["sections/deploying/airflow", "Deploying to Airflow", "P", "previous"]
  ],
  "sourcename": "sections/deploying/dask.md.txt",
  "toc": "<ul>\n<li><a class=\"reference internal\" href=\"#\">Executing on Dask</a><ul>\n<li><a class=\"reference internal\" href=\"#introduction\">Introduction</a><ul>\n<li><a class=\"reference internal\" href=\"#requirements\">Requirements</a></li>\n</ul>\n</li>\n<li><a class=\"reference internal\" href=\"#local-execution\">Local Execution</a></li>\n<li><a class=\"reference internal\" href=\"#distributed-cluster-execution\">Distributed Cluster Execution</a></li>\n<li><a class=\"reference internal\" href=\"#dask-resources\">Dask Resources</a></li>\n<li><a class=\"reference internal\" href=\"#limitations\">Limitations</a></li>\n</ul>\n</li>\n</ul>\n",
  "display_toc": true,
  "page_source_suffix": ".md",
  "current_page_name": "sections/deploying/dask",
  "sidebars": ["globaltoc.html", "searchbox.html"],
  "customsidebar": null,
  "alabaster_version": "0.7.12"
}
