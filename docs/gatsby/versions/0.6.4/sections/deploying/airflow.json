{
  "parents": [{ "link": "../deploying/", "title": "Deploying" }],
  "prev": { "link": "../dagit/", "title": "Standalone Dagit" },
  "next": { "link": "../dask/", "title": "Executing on Dask" },
  "title": "Deploying to Airflow",
  "meta": {},
  "body": "<div class=\"section\" id=\"deploying-to-airflow\">\n<h1>Deploying to Airflow<a class=\"headerlink\" href=\"#deploying-to-airflow\" title=\"Permalink to this headline\">\u00b6</a></h1>\n<p>Dagster is designed for incremental adoption, and to work with all of your existing Airflow\ninfrastructure.</p>\n<p>You can use all of Dagster\u2019s features and abstractions\u2014the programming model, type systems, etc.\nwhile scheduling, executing, and monitoring your Dagster pipelines with Airflow, right alongside all\nof your existing Airflow DAGs.</p>\n<p>This integration is fairly simple. As with vanilla Dagster pipeline execution, Dagster compiles your\npipeline and configuration together into an execution plan. In the case of Airflow, these execution\nplans are then mapped to a DAG, with a bijection between solids and Airflow operators.</p>\n<div class=\"section\" id=\"requirements\">\n<h2>Requirements<a class=\"headerlink\" href=\"#requirements\" title=\"Permalink to this headline\">\u00b6</a></h2>\n<ul class=\"simple\">\n<li><p>The <code class=\"docutils literal notranslate\"><span class=\"pre\">dagster-airflow</span></code> library requires a preexisting Airflow install.</p></li>\n</ul>\n<div class=\"section\" id=\"overview\">\n<h3>Overview<a class=\"headerlink\" href=\"#overview\" title=\"Permalink to this headline\">\u00b6</a></h3>\n<p>We support two modes of execution:</p>\n<ol class=\"arabic simple\">\n<li><p><strong>Uncontainerized [Default]</strong>: Tasks are invoked directly on the Airflow executors.</p></li>\n<li><p><strong>Containerized</strong>: Tasks are executed in Docker containers.</p></li>\n</ol>\n<p>Instructions on setting up Dagster + Airflow in these modes are provided in the next two sections.</p>\n</div>\n<div class=\"section\" id=\"running-uncontainerized\">\n<h3>Running Uncontainerized<a class=\"headerlink\" href=\"#running-uncontainerized\" title=\"Permalink to this headline\">\u00b6</a></h3>\n<p>To define an Airflow DAG corresponding to a Dagster pipeline, you\u2019ll put a new Python file defining\nyour DAG in the directory in which Airflow looks for DAGs \u2013 this is typically <code class=\"docutils literal notranslate\"><span class=\"pre\">$AIRFLOW_HOME/dags</span></code>.</p>\n<p>You can automatically scaffold this file from your Python code with the <code class=\"docutils literal notranslate\"><span class=\"pre\">dagster-airflow</span></code> CLI tool.\nFor example, if you\u2019ve checked out Dagster to <code class=\"docutils literal notranslate\"><span class=\"pre\">$DAGSTER_ROOT</span></code>:</p>\n<div class=\"highlight-shell notranslate\"><div class=\"highlight\"><pre><span></span>$ pip install -e dagster/examples/\n$ dagster-airflow scaffold <span class=\"se\">\\</span>\n    --module-name dagster_examples.toys.sleepy <span class=\"se\">\\</span>\n    --pipeline-name sleepy_pipeline\n</pre></div>\n</div>\n<p>This will create a file in your local <code class=\"docutils literal notranslate\"><span class=\"pre\">$AIRFLOW_HOME/dags</span></code> folder named <code class=\"docutils literal notranslate\"><span class=\"pre\">sleepy_pipeline.py</span></code>. You\ncan simply edit this file, supplying the appropriate environment configuration and Airflow\n<code class=\"docutils literal notranslate\"><span class=\"pre\">DEFAULT_ARGS</span></code> for your particular Airflow instance. When Airflow sweeps this directory looking for\nDAGs, it will find and execute this code, dynamically creating an Airflow DAG and steps\ncorresponding to your Dagster pipeline.</p>\n<p>These are ordinary Airflow objects, and you can do eveything you would expect with them \u2013 for example,\nadding <a class=\"reference external\" href=\"https://airflow.apache.org/_api/airflow/sensors/external_task_sensor/index.html#airflow.sensors.external_task_sensor.ExternalTaskSensor\" title=\"(in Airflow v1.10.6)\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">ExternalTaskSensor</span></code></a>\ndependencies between the dynamically generated Airflow operators in this DAG and operators that you\ndefine in your other existing Airflow DAGs.</p>\n</div>\n</div>\n<div class=\"section\" id=\"using-presets\">\n<h2>Using Presets<a class=\"headerlink\" href=\"#using-presets\" title=\"Permalink to this headline\">\u00b6</a></h2>\n<p>The Airflow scaffold utility also supports using presets when generating an Airflow DAG:</p>\n<div class=\"highlight-shell notranslate\"><div class=\"highlight\"><pre><span></span>$ pip install -e dagster/examples/\n$ dagster-airflow scaffold <span class=\"se\">\\</span>\n    --module-name dagster_examples.toys.error_monster <span class=\"se\">\\</span>\n    --pipeline-name error_monster <span class=\"se\">\\</span>\n    --preset passing\n</pre></div>\n</div>\n</div>\n<div class=\"section\" id=\"implementation-notes\">\n<h2>Implementation Notes<a class=\"headerlink\" href=\"#implementation-notes\" title=\"Permalink to this headline\">\u00b6</a></h2>\n<ul class=\"simple\">\n<li><p>We use a <code class=\"docutils literal notranslate\"><span class=\"pre\">DagsterPythonOperator</span></code> to wrap Dagster solids and define an Airflow DAG that corresponds\nto a Dagster pipeline and can run in your Airflow environment uncontainerized.</p></li>\n<li><p>Note that an extra <code class=\"docutils literal notranslate\"><span class=\"pre\">storage</span></code> parameter will be injected into your environment dict if it is not set.\nYou can set this for any Dagster pipeline (and intermediate values will be automatically\nmaterialized in either <code class=\"docutils literal notranslate\"><span class=\"pre\">filesystem</span></code> or <code class=\"docutils literal notranslate\"><span class=\"pre\">s3</span></code> storage), but you <strong>must</strong> set it when converting a\npipeline to an Airflow DAG.</p></li>\n<li><p>To execute your pipeline, you will also need to make sure that all of the Python and system\nrequirements that your Dagster pipeline requires are available in your Airflow environment; if\nyou\u2019re running Airflow on multiple nodes with the Celery executor, this will be true for the Airflow\nmaster and all workers.</p></li>\n</ul>\n<div class=\"section\" id=\"running-containerized\">\n<h3>Running Containerized<a class=\"headerlink\" href=\"#running-containerized\" title=\"Permalink to this headline\">\u00b6</a></h3>\n<p>We use a <code class=\"docutils literal notranslate\"><span class=\"pre\">DagsterDockerOperator</span></code>, based on the ordinary Airflow\n<a class=\"reference external\" href=\"https://airflow.apache.org/_api/airflow/operators/docker_operator/index.html#airflow.operators.docker_operator.DockerOperator\" title=\"(in Airflow v1.10.6)\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">DockerOperator</span></code></a>, to wrap Dagster\npipelines. In order to run containerized Dagster pipelines, you must have Docker running in your\nAirflow environment (just as with the ordinary Airflow\n<a class=\"reference external\" href=\"https://airflow.apache.org/_api/airflow/operators/docker_operator/index.html#airflow.operators.docker_operator.DockerOperator\" title=\"(in Airflow v1.10.6)\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">DockerOperator</span></code></a>).</p>\n<p>During execution, Dagster caches and transfers intermediate state between execution steps. This\nfeature enables quick re-execution of execution steps from the Dagit UI.</p>\n<p>When running uncontainerized on a single machine, this transfer can take place in memory or on the\nlocal file system, but running in a containerized context requires a persistent intermediate storage\nlayer available to the Dagster containers.</p>\n<p>Presently, we support S3 for persisting this intermediate state. To use it, you\u2019ll just need to set\nup an S3 bucket and expose AWS credentials via the usual Boto credentials chain. We plan on\nsupporting other persistence targets like GCS, HDFS, and NFS in the future\u2014please reach out to us if\nyou require a different intermediate store for your use case.</p>\n<p>We use the <code class=\"docutils literal notranslate\"><span class=\"pre\">DagsterDockerOperator</span></code> to define an Airflow DAG that can run in completely isolated\ncontainers corresponding to your Dagster solids. To run containerized, you\u2019ll first need to\ncontainerize your repository. Then, you can define your Airflow DAG.</p>\n</div>\n</div>\n<div class=\"section\" id=\"containerizing-your-repository\">\n<h2>Containerizing your repository<a class=\"headerlink\" href=\"#containerizing-your-repository\" title=\"Permalink to this headline\">\u00b6</a></h2>\n<p>Make sure you have Docker installed, and write a Dockerfile like the following:</p>\n<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"c1\"># You may use any base container with a supported Python runtime: 2.7, 3.5, 3.6, or 3.7</span>\n<span class=\"n\">FROM</span> <span class=\"n\">python</span><span class=\"p\">:</span><span class=\"mf\">3.7</span>\n\n<span class=\"c1\"># Install any OS-level requirements (e.g. using apt, yum, apk, etc.) that the pipelines in your</span>\n<span class=\"c1\"># repository require to run</span>\n<span class=\"c1\"># RUN apt-get install some-package some-other-package</span>\n\n<span class=\"c1\"># Set environment variables that you&#39;d like to have available in the built image.</span>\n<span class=\"c1\"># ENV IMPORTANT_OPTION=yes</span>\n\n<span class=\"c1\"># If you would like to set secrets at build time (with --build-arg), set args</span>\n<span class=\"c1\"># ARG super_secret</span>\n\n<span class=\"c1\"># Install dagster_graphql</span>\n<span class=\"n\">RUN</span> <span class=\"n\">pip</span> <span class=\"n\">install</span> <span class=\"n\">dagster_graphql</span>\n\n<span class=\"c1\"># Install any Python requirements that the pipelines in your repository require to run</span>\n<span class=\"n\">ADD</span> <span class=\"o\">/</span><span class=\"n\">path</span><span class=\"o\">/</span><span class=\"n\">to</span><span class=\"o\">/</span><span class=\"n\">requirements</span><span class=\"o\">.</span><span class=\"n\">txt</span> <span class=\"o\">.</span>\n<span class=\"n\">RUN</span> <span class=\"n\">pip</span> <span class=\"n\">install</span> <span class=\"o\">-</span><span class=\"n\">r</span> <span class=\"n\">requirements</span><span class=\"o\">.</span><span class=\"n\">txt</span>\n\n<span class=\"c1\"># Add your repository.yaml file so that dagster_graphql knows where to look to find your repository,</span>\n<span class=\"c1\"># the Python file in which your repository is defined, and any local dependencies (e.g., unpackaged</span>\n<span class=\"c1\"># Python files from which your repository definition imports, or local packages that cannot be</span>\n<span class=\"c1\"># installed using the requirements.txt).</span>\n<span class=\"n\">ADD</span> <span class=\"o\">/</span><span class=\"n\">path</span><span class=\"o\">/</span><span class=\"n\">to</span><span class=\"o\">/</span><span class=\"n\">repository</span><span class=\"o\">.</span><span class=\"n\">yaml</span> <span class=\"o\">.</span>\n<span class=\"n\">ADD</span> <span class=\"o\">/</span><span class=\"n\">path</span><span class=\"o\">/</span><span class=\"n\">to</span><span class=\"o\">/</span><span class=\"n\">repository_definition</span><span class=\"o\">.</span><span class=\"n\">py</span> <span class=\"o\">.</span>\n<span class=\"c1\"># ADD /path/to/additional_file.py .</span>\n\n<span class=\"c1\"># The dagster-airflow machinery will use dagster_graphql to execute steps in your pipelines, so we</span>\n<span class=\"c1\"># need to run dagster_graphql when the container starts up</span>\n<span class=\"n\">ENTRYPOINT</span> <span class=\"p\">[</span> <span class=\"s2\">&quot;dagster_graphql&quot;</span> <span class=\"p\">]</span>\n</pre></div>\n</div>\n<p>Of course, you may expand on this Dockerfile in any way that suits your needs.</p>\n<p>Once you\u2019ve written your Dockerfile, you can build your Docker image. You\u2019ll need the name of the\nDocker image (<code class=\"docutils literal notranslate\"><span class=\"pre\">-t</span></code>) that contains your repository later so that the docker-airflow machinery knows\nwhich image to run. E.g., if you want your image to be called <code class=\"docutils literal notranslate\"><span class=\"pre\">dagster-airflow-demo-repository</span></code>:</p>\n<div class=\"highlight-shell notranslate\"><div class=\"highlight\"><pre><span></span>$ docker build -t dagster-airflow-demo-repository -f /path/to/Dockerfile .\n</pre></div>\n</div>\n<p>If you want your containerized pipeline to be available to Airflow operators running on other\nmachines (for example, in environments where Airflow workers are running remotely) you\u2019ll need to\npush your Docker image to a Docker registry so that remote instances of Docker can pull the image by\nname.</p>\n<p>For most production applications, you\u2019ll probably want to use a private Docker registry, rather than\nthe public DockerHub, to store your containerized pipelines.</p>\n</div>\n<div class=\"section\" id=\"defining-your-pipeline-as-a-containerized-airflow-dag\">\n<h2>Defining your pipeline as a containerized Airflow DAG<a class=\"headerlink\" href=\"#defining-your-pipeline-as-a-containerized-airflow-dag\" title=\"Permalink to this headline\">\u00b6</a></h2>\n<p>As in the uncontainerized case, you\u2019ll put a new Python file defining your DAG in the directory in\nwhich Airflow looks for DAGs.</p>\n<div class=\"highlight-python notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"kn\">from</span> <span class=\"nn\">dagster_airflow.factory</span> <span class=\"kn\">import</span> <span class=\"n\">make_airflow_dag_containerized</span>\n\n<span class=\"kn\">from</span> <span class=\"nn\">my_package</span> <span class=\"kn\">import</span> <span class=\"n\">define_my_pipeline</span>\n\n<span class=\"n\">pipeline</span> <span class=\"o\">=</span> <span class=\"n\">define_my_pipeline</span><span class=\"p\">()</span>\n\n<span class=\"n\">image</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;dagster-airflow-demo-repository&#39;</span>\n\n<span class=\"n\">dag</span><span class=\"p\">,</span> <span class=\"n\">steps</span> <span class=\"o\">=</span> <span class=\"n\">make_airflow_dag_containerized</span><span class=\"p\">(</span>\n    <span class=\"n\">pipeline</span><span class=\"p\">,</span>\n    <span class=\"n\">image</span><span class=\"p\">,</span>\n    <span class=\"n\">environment_dict</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s1\">&#39;storage&#39;</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s1\">&#39;filesystem&#39;</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s1\">&#39;config&#39;</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s1\">&#39;base_dir&#39;</span><span class=\"p\">:</span> <span class=\"s1\">&#39;/tmp&#39;</span><span class=\"p\">}}}},</span>\n    <span class=\"n\">dag_id</span><span class=\"o\">=</span><span class=\"kc\">None</span><span class=\"p\">,</span>\n    <span class=\"n\">dag_description</span><span class=\"o\">=</span><span class=\"kc\">None</span><span class=\"p\">,</span>\n    <span class=\"n\">dag_kwargs</span><span class=\"o\">=</span><span class=\"kc\">None</span><span class=\"p\">,</span>\n    <span class=\"n\">op_kwargs</span><span class=\"o\">=</span><span class=\"kc\">None</span>\n<span class=\"p\">)</span>\n</pre></div>\n</div>\n<p>You can pass <code class=\"docutils literal notranslate\"><span class=\"pre\">op_kwargs</span></code> through to the the <code class=\"docutils literal notranslate\"><span class=\"pre\">DagsterDockerOperator</span></code> to use custom TLS settings, the\nprivate registry of your choice, etc., just as you would configure the ordinary Airflow\n<a class=\"reference external\" href=\"https://airflow.apache.org/_api/airflow/operators/docker_operator/index.html#airflow.operators.docker_operator.DockerOperator\" title=\"(in Airflow v1.10.6)\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">DockerOperator</span></code></a>.</p>\n</div>\n<div class=\"section\" id=\"docker-bind-mount-for-filesystem-intermediate-storage\">\n<h2>Docker bind-mount for filesystem intermediate storage<a class=\"headerlink\" href=\"#docker-bind-mount-for-filesystem-intermediate-storage\" title=\"Permalink to this headline\">\u00b6</a></h2>\n<p>By default, the DagsterDockerOperator will bind-mount <code class=\"docutils literal notranslate\"><span class=\"pre\">/tmp</span></code> on the host into <code class=\"docutils literal notranslate\"><span class=\"pre\">/tmp</span></code> in the Docker\ncontainer. You can control this by setting the <code class=\"docutils literal notranslate\"><span class=\"pre\">op_kwargs</span></code> in\n<a class=\"reference internal\" href=\"../../api/apidocs/dagster_airflow/#dagster_airflow.make_airflow_dag\" title=\"dagster_airflow.make_airflow_dag\"><code class=\"xref py py-func docutils literal notranslate\"><span class=\"pre\">make_airflow_dag</span></code></a>. For instance, if\nyou\u2019d prefer to mount <code class=\"docutils literal notranslate\"><span class=\"pre\">/host_tmp</span></code> on the host into <code class=\"docutils literal notranslate\"><span class=\"pre\">/container_tmp</span></code> in the container, and use this\nvolume for intermediate storage, you can run:</p>\n<div class=\"highlight-python notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"n\">dag</span><span class=\"p\">,</span> <span class=\"n\">steps</span> <span class=\"o\">=</span> <span class=\"n\">make_airflow_dag</span><span class=\"p\">(</span>\n    <span class=\"n\">pipeline</span><span class=\"p\">,</span>\n    <span class=\"n\">image</span><span class=\"p\">,</span>\n    <span class=\"n\">environment_dict</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s1\">&#39;storage&#39;</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s1\">&#39;filesystem&#39;</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s1\">&#39;config&#39;</span> <span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s1\">&#39;base_dir&#39;</span><span class=\"p\">:</span> <span class=\"s1\">&#39;/container_tmp&#39;</span><span class=\"p\">}}}},</span>\n    <span class=\"n\">dag_id</span><span class=\"o\">=</span><span class=\"kc\">None</span><span class=\"p\">,</span>\n    <span class=\"n\">dag_description</span><span class=\"o\">=</span><span class=\"kc\">None</span><span class=\"p\">,</span>\n    <span class=\"n\">dag_kwargs</span><span class=\"o\">=</span><span class=\"kc\">None</span><span class=\"p\">,</span>\n    <span class=\"n\">op_kwargs</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s1\">&#39;host_tmp_dir&#39;</span><span class=\"p\">:</span> <span class=\"s1\">&#39;/host_tmp&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;tmp_dir&#39;</span><span class=\"p\">:</span> <span class=\"s1\">&#39;/container_tmp&#39;</span><span class=\"p\">}</span>\n<span class=\"p\">)</span>\n</pre></div>\n</div>\n</div>\n<div class=\"section\" id=\"using-s3-with-dagster-airflow\">\n<h2>Using S3 with dagster-airflow<a class=\"headerlink\" href=\"#using-s3-with-dagster-airflow\" title=\"Permalink to this headline\">\u00b6</a></h2>\n<p>You can also use S3 for dagster-airflow intermediate storage, and you <strong>must</strong> use S3 when running your\nDAGs with distributed executors.</p>\n<p>You\u2019ll need to create an S3 bucket, and provide AWS credentials granting read and write permissions\nto this bucket within your Docker containers. We recommend that you use credentials for an IAM user\nwhich has the\n<a class=\"reference external\" href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html#grant-least-privilege\">least privilege</a>\nrequired to access the S3 bucket for dagster-airflow.</p>\n<p>You can configure S3 storage as follows:</p>\n<div class=\"highlight-python notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"p\">{</span><span class=\"s1\">&#39;storage&#39;</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s1\">&#39;s3&#39;</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s1\">&#39;s3_bucket&#39;</span><span class=\"p\">:</span> <span class=\"s1\">&#39;my-cool-bucket&#39;</span><span class=\"p\">}}}</span>\n</pre></div>\n</div>\n</div>\n<div class=\"section\" id=\"compatibility\">\n<h2>Compatibility<a class=\"headerlink\" href=\"#compatibility\" title=\"Permalink to this headline\">\u00b6</a></h2>\n<p>Note that Airflow versions less than 1.10.3 are incompatible with Python 3.7+.</p>\n</div>\n</div>\n",
  "metatags": "",
  "rellinks": [
    ["genindex", "General Index", "I", "index"],
    ["py-modindex", "Python Module Index", "", "modules"],
    ["sections/deploying/dask", "Executing on Dask", "N", "next"],
    ["sections/deploying/dagit", "Standalone Dagit", "P", "previous"]
  ],
  "sourcename": "sections/deploying/airflow.rst.txt",
  "toc": "<ul>\n<li><a class=\"reference internal\" href=\"#\">Deploying to Airflow</a><ul>\n<li><a class=\"reference internal\" href=\"#requirements\">Requirements</a><ul>\n<li><a class=\"reference internal\" href=\"#overview\">Overview</a></li>\n<li><a class=\"reference internal\" href=\"#running-uncontainerized\">Running Uncontainerized</a></li>\n</ul>\n</li>\n<li><a class=\"reference internal\" href=\"#using-presets\">Using Presets</a></li>\n<li><a class=\"reference internal\" href=\"#implementation-notes\">Implementation Notes</a><ul>\n<li><a class=\"reference internal\" href=\"#running-containerized\">Running Containerized</a></li>\n</ul>\n</li>\n<li><a class=\"reference internal\" href=\"#containerizing-your-repository\">Containerizing your repository</a></li>\n<li><a class=\"reference internal\" href=\"#defining-your-pipeline-as-a-containerized-airflow-dag\">Defining your pipeline as a containerized Airflow DAG</a></li>\n<li><a class=\"reference internal\" href=\"#docker-bind-mount-for-filesystem-intermediate-storage\">Docker bind-mount for filesystem intermediate storage</a></li>\n<li><a class=\"reference internal\" href=\"#using-s3-with-dagster-airflow\">Using S3 with dagster-airflow</a></li>\n<li><a class=\"reference internal\" href=\"#compatibility\">Compatibility</a></li>\n</ul>\n</li>\n</ul>\n",
  "display_toc": true,
  "page_source_suffix": ".rst",
  "current_page_name": "sections/deploying/airflow",
  "sidebars": ["globaltoc.html", "searchbox.html"],
  "customsidebar": null,
  "alabaster_version": "0.7.12"
}
