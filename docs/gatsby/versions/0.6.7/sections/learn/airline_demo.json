{
  "parents": [{ "link": "../learn/", "title": "Learn" }],
  "prev": {
    "link": "../tutorial/intro_airflow/",
    "title": "Deploying to Airflow"
  },
  "next": { "link": "../principles/", "title": "Principles" },
  "title": "Airline Demo",
  "meta": {},
  "body": "<div class=\"section\" id=\"airline-demo\">\n<h1>Airline Demo<a class=\"headerlink\" href=\"#airline-demo\" title=\"Permalink to this headline\">\u00b6</a></h1>\n<p>This repository is intended to provide a fleshed-out demo of Dagster and Dagit capabilities. It defines two\nrealistic data pipelines corresponding to download/ingest and analysis phases of\ntypical data science workflows, using real-world airline data. Although the view of the pipelines\nprovided by the Dagster tooling is unified, in typical practice we expect that each pipeline is\nlikely to be the responsibility of individuals with more or less clearly distinguished roles.</p>\n<p>Use the airline demo to familiarize yourself with the features of Dagster in a more\nfleshed-out context than the introductory tutorial, and as a reference when building your own\nfirst production pipelines in the system. Comments and suggestions are enthusiastically encouraged.</p>\n<div class=\"section\" id=\"getting-started\">\n<h2>Getting Started<a class=\"headerlink\" href=\"#getting-started\" title=\"Permalink to this headline\">\u00b6</a></h2>\n<p>To run the airline demo pipelines locally, you\u2019ll need:</p>\n<ul class=\"simple\">\n<li><p>To be running python 3.5 or greater (the airline demo uses python 3 type annotations)</p></li>\n<li><p>AWS credentials in the ordinary <a class=\"reference external\" href=\"https://boto3.amazonaws.com/v1/documentation/api/latest/guide/configuration.html\">boto3 credential chain</a>.</p></li>\n<li><p>A running Postgres database available at <code class=\"docutils literal notranslate\"><span class=\"pre\">postgresql://test:test&#64;127.0.0.1:5432/test</span></code>. (A\ndocker-compose file is provided in this repo at <code class=\"docutils literal notranslate\"><span class=\"pre\">dagster/examples/</span></code>).</p></li>\n</ul>\n<p>To get up and running:</p>\n<div class=\"highlight-shell notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"c1\"># Clone Dagster</span>\ngit clone git@github.com:dagster-io/dagster.git\n<span class=\"nb\">cd</span> dagster/examples\n\n<span class=\"c1\"># Install all dependencies</span>\npip install -e .<span class=\"o\">[</span>full<span class=\"o\">]</span>\n\n<span class=\"c1\"># Start a local PostgreSQL database</span>\ndocker-compose up -d\n\n<span class=\"c1\"># Load the airline demo in Dagit</span>\n<span class=\"nb\">cd</span> dagster_examples/airline_demo\ndagit\n</pre></div>\n</div>\n<p>You can now view the airline demo in Dagit at <a class=\"reference external\" href=\"http://127.0.0.1:3000/\">http://127.0.0.1:3000/</a>.</p>\n</div>\n<div class=\"section\" id=\"pipelines-config\">\n<h2>Pipelines &amp; Config<a class=\"headerlink\" href=\"#pipelines-config\" title=\"Permalink to this headline\">\u00b6</a></h2>\n<p>The demo defines a single repository with two pipelines, in <code class=\"docutils literal notranslate\"><span class=\"pre\">airline_demo/pipelines.py</span></code>:</p>\n<ul class=\"simple\">\n<li><p><strong>airline_demo_ingest_pipeline</strong>: This pipeline grabs data archives from S3 and unzips them,\nreads the raw data into Spark, performs some typical manipulations on the data, and then loads\ntables into a data warehouse.</p></li>\n<li><p><strong>airline_demo_warehouse_pipeline</strong>: This pipeline performs typical in-warehouse analysis and\nmanipulations using SQL, and then generates and archives analysis artifacts and plots using\nJupyter notebooks.</p></li>\n</ul>\n<p>Default configuration is provided for both of these pipelines using\n<a class=\"reference external\" href=\"https://dagster.readthedocs.io/en/stable/sections/api/apidocs/pipeline.html\">Presets</a>, and the\ncorresponding YAML files can be found under the <code class=\"docutils literal notranslate\"><span class=\"pre\">environments/</span></code> folder.</p>\n</div>\n<div class=\"section\" id=\"the-ingest-pipeline\">\n<h2>The Ingest Pipeline<a class=\"headerlink\" href=\"#the-ingest-pipeline\" title=\"Permalink to this headline\">\u00b6</a></h2>\n<div class=\"figure align-center\">\n<img alt=\"Ingest Pipeline\" src=\"https://user-images.githubusercontent.com/609349/62327985-c5d14180-b466-11e9-9e9f-e714f23ceeb0.png\" />\n</div>\n<p>The <code class=\"docutils literal notranslate\"><span class=\"pre\">airline_demo_ingest_pipeline</span></code> models the first stage of most project-oriented data science\nworkflows, in which raw data is consumed from a variety of sources.</p>\n<p>For demo purposes, we\u2019ve put our source files in a publicly-readable S3 bucket. In practice,\nthese might be files in S3 or other cloud storage systems; publicly available datasets downloaded\nover http; or batch files in an SFTP drop.</p>\n<div class=\"section\" id=\"running-the-pipeline-locally-with-test-config\">\n<h3>Running the pipeline locally with test config<a class=\"headerlink\" href=\"#running-the-pipeline-locally-with-test-config\" title=\"Permalink to this headline\">\u00b6</a></h3>\n<p>You can run this pipeline in Dagit by selecting the \u201cExecute\u201d tab, clicking the preset-load button on the top right of\nthe editor, selecting the <code class=\"docutils literal notranslate\"><span class=\"pre\">local_fast</span></code> preset, and then selecting \u201cStart Execution\u201d:</p>\n<div class=\"figure align-center\">\n<img alt=\"Load Preset &amp; Execute\" src=\"https://user-images.githubusercontent.com/609349/62328360-cf0ede00-b467-11e9-883e-dfcceed8687b.png\" />\n</div>\n<p><strong>Note</strong>: local pipeline execution requires a running PostgreSQL database via the <code class=\"docutils literal notranslate\"><span class=\"pre\">docker-compose</span></code>\ncommand shown above.</p>\n<p>This preset configures the pipeline to consume cut-down versions of the original data files on S3.\nIt can be good practice to maintain similar test sets so that you can run fast versions of your data\npipelines locally or in test environments. While there are many faults that will not be caught by\nusing small cut-down or synthetic data sets\u2014for example, data issues that may only appear in\nproduction data or at scale\u2014this practice will allow you to verify the integrity of your pipeline\nconstruction and to catch at least some semantic issues.</p>\n<p>The pipeline should now execute and complete in the \u201cRuns\u201d tab of Dagit:</p>\n<div class=\"figure align-center\">\n<img alt=\"Pipeline Execution\" src=\"https://user-images.githubusercontent.com/609349/62328637-8572c300-b468-11e9-977e-b67ec4a5eaff.png\" />\n</div>\n</div>\n<div class=\"section\" id=\"reusable-components-in-dagster\">\n<h3>Reusable Components in Dagster<a class=\"headerlink\" href=\"#reusable-components-in-dagster\" title=\"Permalink to this headline\">\u00b6</a></h3>\n<p>Dagster heavily emphasizes building reusable, composable pipelines, and we rely on that\nfunctionality in several places throughout this pipeline.</p>\n<p>Let\u2019s start by looking at the pipeline definition (in <code class=\"docutils literal notranslate\"><span class=\"pre\">airline_demo/pipelines.py</span></code>); you\u2019ll notice\nthat we use solid aliasing to reuse the <code class=\"docutils literal notranslate\"><span class=\"pre\">s3_to_df</span></code> solid for several ingest steps:</p>\n<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span>    <span class=\"n\">load_data_to_database_from_spark</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">&#39;load_q2_on_time_data&#39;</span><span class=\"p\">)(</span>\n        <span class=\"n\">data_frame</span><span class=\"o\">=</span><span class=\"n\">join_q2_data</span><span class=\"p\">(</span>\n            <span class=\"n\">april_data</span><span class=\"o\">=</span><span class=\"n\">s3_to_df</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">&#39;april_on_time_s3_to_df&#39;</span><span class=\"p\">)(),</span>\n            <span class=\"n\">may_data</span><span class=\"o\">=</span><span class=\"n\">s3_to_df</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">&#39;may_on_time_s3_to_df&#39;</span><span class=\"p\">)(),</span>\n            <span class=\"n\">june_data</span><span class=\"o\">=</span><span class=\"n\">s3_to_df</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">&#39;june_on_time_s3_to_df&#39;</span><span class=\"p\">)(),</span>\n            <span class=\"n\">master_cord_data</span><span class=\"o\">=</span><span class=\"n\">s3_to_df</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">&#39;master_cord_s3_to_df&#39;</span><span class=\"p\">)(),</span>\n        <span class=\"p\">)</span>\n    <span class=\"p\">)</span>\n</pre></div>\n</div>\n<p>In general, you won\u2019t want every data science user in your organization to have to roll their own\nimplementation of common functionality like downloading and unzipping files. Instead, you\u2019ll want to\nabstract common functionality into reusable solids, separating task-specific parameters out into\ndeclarative config, and building up a library of building blocks for new data pipelines. Here,\nthat\u2019s been accomplished by implementing the shared functionality in a <code class=\"docutils literal notranslate\"><span class=\"pre\">s3_to_df</span></code> solid and then\nre-using that for ingesting several datasets.</p>\n<p>Digging deeper, you\u2019ll notice that these solids in Dagit are shown with an \u201cExpand\u201d button:</p>\n<div class=\"figure align-center\">\n<img alt=\"Expand\" src=\"https://user-images.githubusercontent.com/609349/62329597-442fe280-b46b-11e9-9b56-39401bbae50e.png\" />\n</div>\n<p>That\u2019s because these are actually <a class=\"reference external\" href=\"https://dagster.readthedocs.io/en/stable/sections/reference/reference.html#composite-solids\">Composite\nSolids</a>:\nsolids that contain other solids. Dagster supports arbitrarily nested, composed solid hierarchies,\nwhich can be really useful for encapsulating functionality as we\u2019ve done here. Click Expand and\nyou\u2019ll see the solids contained within <code class=\"docutils literal notranslate\"><span class=\"pre\">s3_to_df</span></code>:</p>\n<div class=\"figure align-center\">\n<img alt=\"Composite Solid\" src=\"https://user-images.githubusercontent.com/609349/62330149-a806db00-b46c-11e9-8404-0667fa5524c8.png\" />\n</div>\n<p>You can also names and types of all the inputs/outputs of this composite solid and how they are\nconnected to the inputs/outputs of the solids it contains.</p>\n</div>\n<div class=\"section\" id=\"strongly-typed-config-and-outputs\">\n<h3>Strongly typed config and outputs<a class=\"headerlink\" href=\"#strongly-typed-config-and-outputs\" title=\"Permalink to this headline\">\u00b6</a></h3>\n<p>The config for each of our solids specifies everything it needs in order to interact with the\nexternal environment. In YAML, an entry in the config for one of our solids aliasing\n<code class=\"docutils literal notranslate\"><span class=\"pre\">s3_to_df</span></code> looks like this:</p>\n<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span>  <span class=\"n\">april_on_time_s3_to_df</span><span class=\"p\">:</span>\n    <span class=\"n\">inputs</span><span class=\"p\">:</span>\n      <span class=\"n\">s3_coordinate</span><span class=\"p\">:</span>\n        <span class=\"n\">bucket</span><span class=\"p\">:</span> <span class=\"n\">dagster</span><span class=\"o\">-</span><span class=\"n\">airline</span><span class=\"o\">-</span><span class=\"n\">demo</span><span class=\"o\">-</span><span class=\"n\">source</span><span class=\"o\">-</span><span class=\"n\">data</span>\n        <span class=\"n\">key</span><span class=\"p\">:</span> <span class=\"n\">test</span><span class=\"o\">/</span><span class=\"n\">On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2018_4</span><span class=\"o\">.</span><span class=\"n\">zip</span>\n      <span class=\"n\">archive_member</span><span class=\"p\">:</span>\n        <span class=\"n\">value</span><span class=\"p\">:</span> <span class=\"n\">On_Time_Reporting_Carrier_On_Time_Performance_</span><span class=\"p\">(</span><span class=\"mi\">1987</span><span class=\"n\">_present</span><span class=\"p\">)</span><span class=\"n\">_2018_4</span><span class=\"o\">.</span><span class=\"n\">csv</span>\n</pre></div>\n</div>\n<p>Because each of these values is strongly typed, we\u2019ll get rich error information in the dagit\nconfig editor (or when running pipelines from the command line) when a config value is incorrectly\nspecified:</p>\n<div class=\"figure align-center\">\n<img alt=\"Error\" src=\"https://user-images.githubusercontent.com/609349/62331551-18176000-b471-11e9-9233-760e3bbea609.png\" />\n</div>\n<p>While this may seem like a mere convenience, in production contexts it can dramatically reduce\navoidable errors. Consider boto3\u2019s <a class=\"reference external\" href=\"https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.Client.put_object\">S3.Client.put_object()</a>\nmethod, which has 28 parameters, many restricted in obscure ways (for example,\n<code class=\"docutils literal notranslate\"><span class=\"pre\">ServerSideEncryption</span></code> must be one of <code class=\"docutils literal notranslate\"><span class=\"pre\">'AES256'|'aws:kms'</span></code>). Strongly typed config schemas can catch\nany error of this type.</p>\n<p>By setting the <code class=\"docutils literal notranslate\"><span class=\"pre\">description</span></code> on each of our config members, we also get easily navigable\ndocumentation in dagit. Users of library solids no longer need to investigate implementations in\norder to understand what values are required, or what they do\u2014enabling more confident reuse.</p>\n<div class=\"figure align-center\">\n<img alt=\"Download pipeline config docs\" src=\"https://user-images.githubusercontent.com/609349/62331694-86f4b900-b471-11e9-9702-0b117219f7e6.png\" />\n</div>\n</div>\n<div class=\"section\" id=\"using-modes-for-production-data\">\n<h3>Using Modes for Production Data<a class=\"headerlink\" href=\"#using-modes-for-production-data\" title=\"Permalink to this headline\">\u00b6</a></h3>\n<p>Don\u2019t worry, we\u2019ve got plenty of big(gish) data to run through this pipeline. Instead of the\n<code class=\"docutils literal notranslate\"><span class=\"pre\">local_fast_ingest.yaml</span></code> config fragment, use <code class=\"docutils literal notranslate\"><span class=\"pre\">local_full_ingest.yaml</span></code>\u2014but be prepared to wait.\nYou can use this pattern to run your Dagster pipelines against synthetic, anonymized, or subsampled\ndatasets in test and development environments.</p>\n<p>In practice, you\u2019ll want to run your pipelines in environments that vary widely in their available\nfacilities. For example, when running an ingestion pipeline locally for test or development, you\nmay want to use a local Spark cluster; but when running in production, you will probably want to\ntarget something heftier, like an ephemeral EMR cluster or your organization\u2019s on-prem Spark\ncluster. Or, you may want to target a locally-running Postgres as your \u201cdata warehouse\u201d in local\ntest/dev, but target a Redshift cluster in production and production tests. Ideally, we want this\nto be a matter of flipping a config switch.</p>\n<p>Dagster decouples the instantiation of external resources like these from the business logic of your\ndata pipelines. In Dagster, a set of resources configured for a particular environment is called a\n<a class=\"reference external\" href=\"https://dagster.readthedocs.io/en/latest/sections/api/apidocs/pipeline.html#modes\">Mode</a>.</p>\n<p>Let\u2019s look at how we make configurable resources available to our pipelines with modes. In\n<code class=\"docutils literal notranslate\"><span class=\"pre\">airline_demo/pipelines.py</span></code>, you\u2019ll find that we define multiple modes within which our pipelines\nmay run:</p>\n<div class=\"highlight-python notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"nd\">@pipeline</span><span class=\"p\">(</span>\n    <span class=\"c1\"># ordered so the local is first and therefore the default</span>\n    <span class=\"n\">mode_defs</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"n\">local_mode</span><span class=\"p\">,</span> <span class=\"n\">test_mode</span><span class=\"p\">,</span> <span class=\"n\">prod_mode</span><span class=\"p\">],</span>\n    <span class=\"o\">...</span>\n<span class=\"p\">)</span>\n</pre></div>\n</div>\n<p>This is intended to mimic a typical setup where you may have pipelines running locally on developer\nmachines, often with a (anonymized or scrubbed) subset of data and with limited compute resources;\nremotely in CI/CD, with access to a production or replica environment, but where speed is of the\nessence; and remotely in production on live data.</p>\n<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"n\">local_mode</span> <span class=\"o\">=</span> <span class=\"n\">ModeDefinition</span><span class=\"p\">(</span>\n    <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s1\">&#39;local&#39;</span><span class=\"p\">,</span>\n    <span class=\"n\">resource_defs</span><span class=\"o\">=</span><span class=\"p\">{</span>\n        <span class=\"s1\">&#39;spark&#39;</span><span class=\"p\">:</span> <span class=\"n\">pyspark_resource</span><span class=\"p\">,</span>\n        <span class=\"s1\">&#39;s3&#39;</span><span class=\"p\">:</span> <span class=\"n\">s3_resource</span><span class=\"p\">,</span>\n        <span class=\"s1\">&#39;db_info&#39;</span><span class=\"p\">:</span> <span class=\"n\">postgres_db_info_resource</span><span class=\"p\">,</span>\n        <span class=\"s1\">&#39;tempfile&#39;</span><span class=\"p\">:</span> <span class=\"n\">tempfile_resource</span><span class=\"p\">,</span>\n        <span class=\"s1\">&#39;file_cache&#39;</span><span class=\"p\">:</span> <span class=\"n\">fs_file_cache</span><span class=\"p\">,</span>\n    <span class=\"p\">},</span>\n    <span class=\"n\">system_storage_defs</span><span class=\"o\">=</span><span class=\"n\">s3_plus_default_storage_defs</span><span class=\"p\">,</span>\n<span class=\"p\">)</span>\n\n\n<span class=\"n\">prod_mode</span> <span class=\"o\">=</span> <span class=\"n\">ModeDefinition</span><span class=\"p\">(</span>\n    <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s1\">&#39;prod&#39;</span><span class=\"p\">,</span>\n    <span class=\"n\">resource_defs</span><span class=\"o\">=</span><span class=\"p\">{</span>\n        <span class=\"s1\">&#39;spark&#39;</span><span class=\"p\">:</span> <span class=\"n\">pyspark_resource</span><span class=\"p\">,</span>  <span class=\"c1\"># FIXME</span>\n        <span class=\"s1\">&#39;s3&#39;</span><span class=\"p\">:</span> <span class=\"n\">s3_resource</span><span class=\"p\">,</span>\n        <span class=\"s1\">&#39;db_info&#39;</span><span class=\"p\">:</span> <span class=\"n\">redshift_db_info_resource</span><span class=\"p\">,</span>\n        <span class=\"s1\">&#39;tempfile&#39;</span><span class=\"p\">:</span> <span class=\"n\">tempfile_resource</span><span class=\"p\">,</span>\n        <span class=\"s1\">&#39;file_cache&#39;</span><span class=\"p\">:</span> <span class=\"n\">s3_file_cache</span><span class=\"p\">,</span>\n    <span class=\"p\">},</span>\n    <span class=\"n\">system_storage_defs</span><span class=\"o\">=</span><span class=\"n\">s3_plus_default_storage_defs</span><span class=\"p\">,</span>\n<span class=\"p\">)</span>\n</pre></div>\n</div>\n<p>Here we\u2019ve defined a <code class=\"docutils literal notranslate\"><span class=\"pre\">db_info</span></code> resource that exposes a unified API to our solid logic, but that\nwraps two very different underlying assets\u2014in one case, a Postgres database, and in the other,\na Redshift cluster. Let\u2019s look more closely at how this is implemented for the Postgres case.</p>\n<p>First, we define the <code class=\"docutils literal notranslate\"><span class=\"pre\">db_info</span></code> resource itself:</p>\n<div class=\"highlight-python notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"n\">DbInfo</span> <span class=\"o\">=</span> <span class=\"n\">namedtuple</span><span class=\"p\">(</span><span class=\"s1\">&#39;DbInfo&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;engine url jdbc_url dialect load_table&#39;</span><span class=\"p\">)</span>\n</pre></div>\n</div>\n<p>This resource exposes a SQLAlchemy engine, the URL of the database (in two forms), metadata about\nthe SQL dialect that the database speaks, and a utility function, <code class=\"docutils literal notranslate\"><span class=\"pre\">load_table</span></code>, which loads a\nSpark data frame into the target database. In practice, we would probably find that over time we\nwanted to add or subtract from this interface, rework its implementations, or factor it into\nmultiple resources. Because the type definition, config definitions, and implementations are\ncentralized\u2014rather than spread out across the internals of many solids\u2014this is a relatively\neasy task.</p>\n<p>Next, we define the config required to instantiate our resource:</p>\n<div class=\"highlight-python notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"n\">PostgresConfigData</span> <span class=\"o\">=</span> <span class=\"n\">Dict</span><span class=\"p\">(</span>\n    <span class=\"p\">{</span>\n        <span class=\"s1\">&#39;postgres_username&#39;</span><span class=\"p\">:</span> <span class=\"n\">Field</span><span class=\"p\">(</span><span class=\"n\">String</span><span class=\"p\">),</span>\n        <span class=\"s1\">&#39;postgres_password&#39;</span><span class=\"p\">:</span> <span class=\"n\">Field</span><span class=\"p\">(</span><span class=\"n\">String</span><span class=\"p\">),</span>\n        <span class=\"s1\">&#39;postgres_hostname&#39;</span><span class=\"p\">:</span> <span class=\"n\">Field</span><span class=\"p\">(</span><span class=\"n\">String</span><span class=\"p\">),</span>\n        <span class=\"s1\">&#39;postgres_db_name&#39;</span><span class=\"p\">:</span> <span class=\"n\">Field</span><span class=\"p\">(</span><span class=\"n\">String</span><span class=\"p\">),</span>\n    <span class=\"p\">}</span>\n<span class=\"p\">)</span>\n</pre></div>\n</div>\n<p>Obviously, this config will differ for Redshift, as it might if we had to reach our database through\na proxy server, or using a different authentication schema.</p>\n<p>Finally, we bring it all together in the <code class=\"docutils literal notranslate\"><span class=\"pre\">postgres_db_info_resource</span></code>:</p>\n<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"nd\">@resource</span><span class=\"p\">(</span>\n    <span class=\"p\">{</span>\n        <span class=\"s1\">&#39;postgres_username&#39;</span><span class=\"p\">:</span> <span class=\"n\">Field</span><span class=\"p\">(</span><span class=\"nb\">str</span><span class=\"p\">),</span>\n        <span class=\"s1\">&#39;postgres_password&#39;</span><span class=\"p\">:</span> <span class=\"n\">Field</span><span class=\"p\">(</span><span class=\"nb\">str</span><span class=\"p\">),</span>\n        <span class=\"s1\">&#39;postgres_hostname&#39;</span><span class=\"p\">:</span> <span class=\"n\">Field</span><span class=\"p\">(</span><span class=\"nb\">str</span><span class=\"p\">),</span>\n        <span class=\"s1\">&#39;postgres_db_name&#39;</span><span class=\"p\">:</span> <span class=\"n\">Field</span><span class=\"p\">(</span><span class=\"nb\">str</span><span class=\"p\">),</span>\n    <span class=\"p\">}</span>\n<span class=\"p\">)</span>\n<span class=\"k\">def</span> <span class=\"nf\">postgres_db_info_resource</span><span class=\"p\">(</span><span class=\"n\">init_context</span><span class=\"p\">):</span>\n    <span class=\"n\">host</span> <span class=\"o\">=</span> <span class=\"n\">init_context</span><span class=\"o\">.</span><span class=\"n\">resource_config</span><span class=\"p\">[</span><span class=\"s1\">&#39;postgres_hostname&#39;</span><span class=\"p\">]</span>\n    <span class=\"n\">db_name</span> <span class=\"o\">=</span> <span class=\"n\">init_context</span><span class=\"o\">.</span><span class=\"n\">resource_config</span><span class=\"p\">[</span><span class=\"s1\">&#39;postgres_db_name&#39;</span><span class=\"p\">]</span>\n    <span class=\"n\">db_url_jdbc</span> <span class=\"o\">=</span> <span class=\"n\">create_postgres_db_url</span><span class=\"p\">(</span>\n        <span class=\"n\">init_context</span><span class=\"o\">.</span><span class=\"n\">resource_config</span><span class=\"p\">[</span><span class=\"s1\">&#39;postgres_username&#39;</span><span class=\"p\">],</span>\n        <span class=\"n\">init_context</span><span class=\"o\">.</span><span class=\"n\">resource_config</span><span class=\"p\">[</span><span class=\"s1\">&#39;postgres_password&#39;</span><span class=\"p\">],</span>\n        <span class=\"n\">host</span><span class=\"p\">,</span>\n        <span class=\"n\">db_name</span><span class=\"p\">,</span>\n    <span class=\"p\">)</span>\n\n    <span class=\"n\">db_url</span> <span class=\"o\">=</span> <span class=\"n\">create_postgres_db_url</span><span class=\"p\">(</span>\n        <span class=\"n\">init_context</span><span class=\"o\">.</span><span class=\"n\">resource_config</span><span class=\"p\">[</span><span class=\"s1\">&#39;postgres_username&#39;</span><span class=\"p\">],</span>\n        <span class=\"n\">init_context</span><span class=\"o\">.</span><span class=\"n\">resource_config</span><span class=\"p\">[</span><span class=\"s1\">&#39;postgres_password&#39;</span><span class=\"p\">],</span>\n        <span class=\"n\">host</span><span class=\"p\">,</span>\n        <span class=\"n\">db_name</span><span class=\"p\">,</span>\n        <span class=\"n\">jdbc</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">,</span>\n    <span class=\"p\">)</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">_do_load</span><span class=\"p\">(</span><span class=\"n\">data_frame</span><span class=\"p\">,</span> <span class=\"n\">table_name</span><span class=\"p\">):</span>\n        <span class=\"n\">data_frame</span><span class=\"o\">.</span><span class=\"n\">write</span><span class=\"o\">.</span><span class=\"n\">option</span><span class=\"p\">(</span><span class=\"s1\">&#39;driver&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;org.postgresql.Driver&#39;</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">mode</span><span class=\"p\">(</span><span class=\"s1\">&#39;overwrite&#39;</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">jdbc</span><span class=\"p\">(</span>\n            <span class=\"n\">db_url_jdbc</span><span class=\"p\">,</span> <span class=\"n\">table_name</span>\n        <span class=\"p\">)</span>\n\n    <span class=\"k\">return</span> <span class=\"n\">DbInfo</span><span class=\"p\">(</span>\n        <span class=\"n\">url</span><span class=\"o\">=</span><span class=\"n\">db_url</span><span class=\"p\">,</span>\n        <span class=\"n\">jdbc_url</span><span class=\"o\">=</span><span class=\"n\">db_url_jdbc</span><span class=\"p\">,</span>\n        <span class=\"n\">engine</span><span class=\"o\">=</span><span class=\"n\">create_postgres_engine</span><span class=\"p\">(</span><span class=\"n\">db_url</span><span class=\"p\">),</span>\n        <span class=\"n\">dialect</span><span class=\"o\">=</span><span class=\"s1\">&#39;postgres&#39;</span><span class=\"p\">,</span>\n        <span class=\"n\">load_table</span><span class=\"o\">=</span><span class=\"n\">_do_load</span><span class=\"p\">,</span>\n        <span class=\"n\">host</span><span class=\"o\">=</span><span class=\"n\">host</span><span class=\"p\">,</span>\n        <span class=\"n\">db_name</span><span class=\"o\">=</span><span class=\"n\">db_name</span><span class=\"p\">,</span>\n    <span class=\"p\">)</span>\n</pre></div>\n</div>\n<p>By providing strongly typed configuration fields to the <code class=\"docutils literal notranslate\"><span class=\"pre\">&#64;resource</span></code> decorator, we now have typeahead\nsupport in dagit and rich error messages for the configuration of our external resources. This can\nbe extremely valuable in the case of notoriously complex configuration formats, such as Spark\u2019s.</p>\n<p>Note that Dagit is aware of all modes defined for your pipelines, and will present these to you\n(along with the resources they provide) in the pipeline view sidebar:</p>\n<div class=\"figure align-center\">\n<img alt=\"Modes\" src=\"https://user-images.githubusercontent.com/609349/62332217-7e9d7d80-b473-11e9-8eb7-13198d44eec6.png\" />\n</div>\n</div>\n<div class=\"section\" id=\"ingesting-data-to-spark-data-frames\">\n<h3>Ingesting data to Spark data frames<a class=\"headerlink\" href=\"#ingesting-data-to-spark-data-frames\" title=\"Permalink to this headline\">\u00b6</a></h3>\n<p>Returning to our <code class=\"docutils literal notranslate\"><span class=\"pre\">s3_to_df</span></code> solid, note the type signature of these solids.</p>\n<div class=\"figure align-center\">\n<img alt=\"Ingest pipeline type signature\" src=\"https://user-images.githubusercontent.com/609349/62332521-e56f6680-b474-11e9-80e7-1b977b8c5ac5.png\" />\n</div>\n<p>The output has type <code class=\"docutils literal notranslate\"><span class=\"pre\">DataFrame</span></code>, a Dagster type that wraps a <code class=\"docutils literal notranslate\"><span class=\"pre\">pyspark.sql.DataFrame</span></code>.\nDefining types of this kind is straightforward and provides additional safety when your solids pass\nPython objects to each other:</p>\n<div class=\"highlight-python notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"n\">DataFrame</span> <span class=\"o\">=</span> <span class=\"n\">as_dagster_type</span><span class=\"p\">(</span>\n    <span class=\"n\">NativeSparkDataFrame</span><span class=\"p\">,</span> <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s1\">&#39;DataFrame&#39;</span><span class=\"p\">,</span> <span class=\"n\">description</span><span class=\"o\">=</span><span class=\"s1\">&#39;A Pyspark data frame.&#39;</span>\n<span class=\"p\">)</span>\n</pre></div>\n</div>\n<p>The transformation solids that follow all use the <code class=\"docutils literal notranslate\"><span class=\"pre\">DataFrame</span></code> for their intermediate results.\nYou might also build DAGs where Pandas data frames, or some other in-memory Python object, are the\ncommon intermediate representation.</p>\n</div>\n<div class=\"section\" id=\"loading-data-to-the-warehouse\">\n<h3>Loading data to the warehouse<a class=\"headerlink\" href=\"#loading-data-to-the-warehouse\" title=\"Permalink to this headline\">\u00b6</a></h3>\n<p>The terminal nodes of this pipeline are all aliased instances of\n<code class=\"docutils literal notranslate\"><span class=\"pre\">load_data_to_database_from_spark</span></code>:</p>\n<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"nd\">@solid</span><span class=\"p\">(</span>\n    <span class=\"n\">output_defs</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"n\">OutputDefinition</span><span class=\"p\">(</span><span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s1\">&#39;table_name&#39;</span><span class=\"p\">,</span> <span class=\"n\">dagster_type</span><span class=\"o\">=</span><span class=\"n\">String</span><span class=\"p\">)],</span>\n    <span class=\"n\">config_field</span><span class=\"o\">=</span><span class=\"n\">Field</span><span class=\"p\">(</span><span class=\"n\">Dict</span><span class=\"p\">(</span><span class=\"n\">fields</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s1\">&#39;table_name&#39;</span><span class=\"p\">:</span> <span class=\"n\">Field</span><span class=\"p\">(</span><span class=\"n\">String</span><span class=\"p\">,</span> <span class=\"n\">description</span><span class=\"o\">=</span><span class=\"s1\">&#39;&#39;</span><span class=\"p\">)})),</span>\n<span class=\"p\">)</span>\n<span class=\"k\">def</span> <span class=\"nf\">load_data_to_database_from_spark</span><span class=\"p\">(</span><span class=\"n\">context</span><span class=\"p\">,</span> <span class=\"n\">data_frame</span><span class=\"p\">:</span> <span class=\"n\">DataFrame</span><span class=\"p\">):</span>\n    <span class=\"n\">context</span><span class=\"o\">.</span><span class=\"n\">resources</span><span class=\"o\">.</span><span class=\"n\">db_info</span><span class=\"o\">.</span><span class=\"n\">load_table</span><span class=\"p\">(</span><span class=\"n\">data_frame</span><span class=\"p\">,</span> <span class=\"n\">context</span><span class=\"o\">.</span><span class=\"n\">solid_config</span><span class=\"p\">[</span><span class=\"s1\">&#39;table_name&#39;</span><span class=\"p\">])</span>\n\n    <span class=\"n\">table_name</span> <span class=\"o\">=</span> <span class=\"n\">context</span><span class=\"o\">.</span><span class=\"n\">solid_config</span><span class=\"p\">[</span><span class=\"s1\">&#39;table_name&#39;</span><span class=\"p\">]</span>\n    <span class=\"k\">yield</span> <span class=\"n\">Materialization</span><span class=\"p\">(</span>\n        <span class=\"n\">label</span><span class=\"o\">=</span><span class=\"s1\">&#39;Table: </span><span class=\"si\">{table_name}</span><span class=\"s1\">&#39;</span><span class=\"o\">.</span><span class=\"n\">format</span><span class=\"p\">(</span><span class=\"n\">table_name</span><span class=\"o\">=</span><span class=\"n\">table_name</span><span class=\"p\">),</span>\n        <span class=\"n\">description</span><span class=\"o\">=</span><span class=\"p\">(</span>\n            <span class=\"s1\">&#39;Persisted table </span><span class=\"si\">{table_name}</span><span class=\"s1\"> in database configured in the db_info resource.&#39;</span>\n        <span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">format</span><span class=\"p\">(</span><span class=\"n\">table_name</span><span class=\"o\">=</span><span class=\"n\">table_name</span><span class=\"p\">),</span>\n        <span class=\"n\">metadata_entries</span><span class=\"o\">=</span><span class=\"p\">[</span>\n            <span class=\"n\">EventMetadataEntry</span><span class=\"o\">.</span><span class=\"n\">text</span><span class=\"p\">(</span><span class=\"n\">label</span><span class=\"o\">=</span><span class=\"s1\">&#39;Host&#39;</span><span class=\"p\">,</span> <span class=\"n\">text</span><span class=\"o\">=</span><span class=\"n\">context</span><span class=\"o\">.</span><span class=\"n\">resources</span><span class=\"o\">.</span><span class=\"n\">db_info</span><span class=\"o\">.</span><span class=\"n\">host</span><span class=\"p\">),</span>\n            <span class=\"n\">EventMetadataEntry</span><span class=\"o\">.</span><span class=\"n\">text</span><span class=\"p\">(</span><span class=\"n\">label</span><span class=\"o\">=</span><span class=\"s1\">&#39;Db&#39;</span><span class=\"p\">,</span> <span class=\"n\">text</span><span class=\"o\">=</span><span class=\"n\">context</span><span class=\"o\">.</span><span class=\"n\">resources</span><span class=\"o\">.</span><span class=\"n\">db_info</span><span class=\"o\">.</span><span class=\"n\">db_name</span><span class=\"p\">),</span>\n        <span class=\"p\">],</span>\n    <span class=\"p\">)</span>\n    <span class=\"k\">yield</span> <span class=\"n\">Output</span><span class=\"p\">(</span><span class=\"n\">value</span><span class=\"o\">=</span><span class=\"n\">table_name</span><span class=\"p\">,</span> <span class=\"n\">output_name</span><span class=\"o\">=</span><span class=\"s1\">&#39;table_name&#39;</span><span class=\"p\">)</span>\n</pre></div>\n</div>\n<p>which abstracts the operation of loading a Spark data frame to a database\u2014either our production\nRedshift cluster or our local Postgres in test.</p>\n<p>Note how using the <code class=\"docutils literal notranslate\"><span class=\"pre\">db_info</span></code> resource simplifies this operation. There\u2019s no need to pollute the\nimplementation of our DAGs with specifics about how to connect to outside databases, credentials,\nformatting details, retry or batching logic, etc. This greatly reduces the opportunities for\nimplementations of these core external operations to drift and introduce subtle bugs, and cleanly\nseparates infrastructure concerns from the logic of any particular data processing pipeline.</p>\n</div>\n</div>\n<div class=\"section\" id=\"the-warehouse-pipeline\">\n<h2>The Warehouse Pipeline<a class=\"headerlink\" href=\"#the-warehouse-pipeline\" title=\"Permalink to this headline\">\u00b6</a></h2>\n<div class=\"figure align-center\">\n<img alt=\"Warehouse Pipeline\" src=\"https://user-images.githubusercontent.com/609349/62332824-f076c680-b475-11e9-9a58-646401b145f5.png\" />\n</div>\n<p>The <code class=\"docutils literal notranslate\"><span class=\"pre\">airline_demo_warehouse_pipeline</span></code> models the analytics stage of a typical data science workflow.\nThis is a heterogeneous-by-design process in which analysts, data scientists, and ML engineers\nincrementally derive and formalize insights and analytic products (charts, data frames, models, and\nmetrics) using a wide range of tools\u2014from SQL run directly against the warehouse to Jupyter\nnotebooks in Python or R.</p>\n<div class=\"section\" id=\"the-sql-solid-wrapping-foreign-code-in-a-solid\">\n<h3>The <code class=\"docutils literal notranslate\"><span class=\"pre\">sql_solid</span></code>: wrapping foreign code in a solid<a class=\"headerlink\" href=\"#the-sql-solid-wrapping-foreign-code-in-a-solid\" title=\"Permalink to this headline\">\u00b6</a></h3>\n<p>How do we actually package the SQL for execution and display with Dagster and Dagit? We\u2019ve built a\nfunction, <code class=\"docutils literal notranslate\"><span class=\"pre\">sql_solid</span></code>, that is designed to take a SQL query and return a solid.</p>\n<p>Fundamentally, all this is doing is invoking <code class=\"docutils literal notranslate\"><span class=\"pre\">execute</span></code> on the <code class=\"docutils literal notranslate\"><span class=\"pre\">db_info.engine</span></code> provided by the\ncontext:</p>\n<div class=\"highlight-python notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"n\">context</span><span class=\"o\">.</span><span class=\"n\">resources</span><span class=\"o\">.</span><span class=\"n\">db_info</span><span class=\"o\">.</span><span class=\"n\">engine</span><span class=\"o\">.</span><span class=\"n\">execute</span><span class=\"p\">(</span><span class=\"n\">text</span><span class=\"p\">(</span><span class=\"n\">sql_statement</span><span class=\"p\">))</span>\n</pre></div>\n</div>\n<div class=\"figure align-center\">\n<img alt=\"Warehouse Pipeline SQL\" src=\"https://user-images.githubusercontent.com/609349/62580646-93a35380-b85b-11e9-8118-8c4058ebb55d.png\" />\n</div>\n<p>Also, note how straightforward the interface to <code class=\"docutils literal notranslate\"><span class=\"pre\">sql_solid</span></code> is. To define a new solid executing a\nrelatively complex computation against the data warehouse, an analyst only needs to supply light\nmetadata along with their SQL query:</p>\n<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"n\">delays_vs_fares</span> <span class=\"o\">=</span> <span class=\"n\">sql_solid</span><span class=\"p\">(</span>\n    <span class=\"s1\">&#39;delays_vs_fares&#39;</span><span class=\"p\">,</span>\n    <span class=\"sd\">&#39;&#39;&#39;</span>\n<span class=\"sd\">    with avg_fares as (</span>\n<span class=\"sd\">        select</span>\n<span class=\"sd\">            tickets.origin,</span>\n<span class=\"sd\">            tickets.dest,</span>\n<span class=\"sd\">            avg(cast(tickets.itinfare as float)) as avg_fare,</span>\n<span class=\"sd\">            avg(cast(tickets.farepermile as float)) as avg_fare_per_mile</span>\n<span class=\"sd\">        from tickets_with_destination as tickets</span>\n<span class=\"sd\">        where origin = &#39;SFO&#39;</span>\n<span class=\"sd\">        group by (tickets.origin, tickets.dest)</span>\n<span class=\"sd\">    )</span>\n<span class=\"sd\">    select</span>\n<span class=\"sd\">        avg_fares.*,</span>\n<span class=\"sd\">        avg(avg_delays.arrival_delay) as avg_arrival_delay,</span>\n<span class=\"sd\">        avg(avg_delays.departure_delay) as avg_departure_delay</span>\n<span class=\"sd\">    from</span>\n<span class=\"sd\">        avg_fares,</span>\n<span class=\"sd\">        average_sfo_outbound_avg_delays_by_destination as avg_delays</span>\n<span class=\"sd\">    where</span>\n<span class=\"sd\">        avg_fares.origin = avg_delays.origin and</span>\n<span class=\"sd\">        avg_fares.dest = avg_delays.destination</span>\n<span class=\"sd\">    group by (</span>\n<span class=\"sd\">        avg_fares.avg_fare,</span>\n<span class=\"sd\">        avg_fares.avg_fare_per_mile,</span>\n<span class=\"sd\">        avg_fares.origin,</span>\n<span class=\"sd\">        avg_delays.origin,</span>\n<span class=\"sd\">        avg_fares.dest,</span>\n<span class=\"sd\">        avg_delays.destination</span>\n<span class=\"sd\">    )</span>\n<span class=\"sd\">    &#39;&#39;&#39;</span><span class=\"p\">,</span>\n    <span class=\"s1\">&#39;table&#39;</span><span class=\"p\">,</span>\n    <span class=\"n\">table_name</span><span class=\"o\">=</span><span class=\"s1\">&#39;delays_vs_fares&#39;</span><span class=\"p\">,</span>\n    <span class=\"n\">input_defs</span><span class=\"o\">=</span><span class=\"p\">[</span>\n        <span class=\"n\">InputDefinition</span><span class=\"p\">(</span><span class=\"s1\">&#39;tickets_with_destination&#39;</span><span class=\"p\">,</span> <span class=\"n\">SqlTableName</span><span class=\"p\">),</span>\n        <span class=\"n\">InputDefinition</span><span class=\"p\">(</span><span class=\"s1\">&#39;average_sfo_outbound_avg_delays_by_destination&#39;</span><span class=\"p\">,</span> <span class=\"n\">SqlTableName</span><span class=\"p\">),</span>\n    <span class=\"p\">],</span>\n<span class=\"p\">)</span>\n</pre></div>\n</div>\n<p>This kind of interface can supercharge the work of analysts who are highly skilled in SQL, but for\nwhom fully general-purpose programming in Python may be uncomfortable. Analysts need only master a\nvery constrained interface in order to define their SQL statements\u2019 data dependencies and outputs in\na way that allows them to be orchestrated and explored in heterogeneous DAGs containing other forms\nof computation, and must make only minimal changes to code (in some cases, no changes) in order to\ntake advantage of centralized infrastructure work.</p>\n</div>\n<div class=\"section\" id=\"dagstermill-executing-and-checkpointing-notebooks\">\n<h3>Dagstermill: executing and checkpointing notebooks<a class=\"headerlink\" href=\"#dagstermill-executing-and-checkpointing-notebooks\" title=\"Permalink to this headline\">\u00b6</a></h3>\n<p>Notebooks are a flexible way and increasingly ubiquitous way to explore datasets and build data\nproducts, from transformed or augmented datasets to charts and metrics to deployable machine\nlearning models. But their very flexibility\u2014the way they allow analysts to move seamlessly from\ninteractive exploration of data to scratch code and finally to actionable results\u2014can make them\ndifficult to productionize.</p>\n<p>Our approach, built on <a class=\"reference external\" href=\"https://github.com/nteract/papermill\">papermill</a>, embraces the diversity\nof code in notebooks. Rather than requiring notebooks to be rewritten or \u201ccleaned up\u201d in order to\nconsume inputs from other nodes in a pipeline or to produce outputs for downstream nodes, we can\njust add a few cells to provide a thin wrapper around an existing notebook.</p>\n<p>Notebook solids can be identified by the small red <code class=\"docutils literal notranslate\"><span class=\"pre\">ipynb</span></code> tag in Dagit. As with SQL solids, they\ncan be explored from within Dagit by drilling down on the tag:</p>\n<div class=\"figure align-center\">\n<img alt=\"Warehouse Pipeline Notebook\" src=\"https://user-images.githubusercontent.com/609349/62580619-80908380-b85b-11e9-9d6d-c52796da8ffd.png\" />\n</div>\n<p>Let\u2019s start with the definition of our <code class=\"docutils literal notranslate\"><span class=\"pre\">notebook_solid</span></code> helper:</p>\n<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span>    <span class=\"k\">return</span> <span class=\"n\">define_dagstermill_solid</span><span class=\"p\">(</span><span class=\"n\">name</span><span class=\"p\">,</span> <span class=\"n\">_notebook_path</span><span class=\"p\">(</span><span class=\"n\">notebook_path</span><span class=\"p\">),</span> <span class=\"n\">input_defs</span><span class=\"p\">,</span> <span class=\"n\">output_defs</span><span class=\"p\">)</span>\n\n</pre></div>\n</div>\n<p>This is just a wrapper around Dagstermill\u2019s <code class=\"docutils literal notranslate\"><span class=\"pre\">define_dagstermill_solid</span></code> which tells Dagstermill\nwhere to look for the notebooks. We define a new solid by using this function and referencing\na notebook file:</p>\n<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"n\">delays_by_geography</span> <span class=\"o\">=</span> <span class=\"n\">notebook_solid</span><span class=\"p\">(</span>\n    <span class=\"s1\">&#39;delays_by_geography&#39;</span><span class=\"p\">,</span>\n    <span class=\"s1\">&#39;Delays_by_Geography.ipynb&#39;</span><span class=\"p\">,</span>\n    <span class=\"n\">input_defs</span><span class=\"o\">=</span><span class=\"p\">[</span>\n        <span class=\"n\">InputDefinition</span><span class=\"p\">(</span>\n            <span class=\"s1\">&#39;westbound_delays&#39;</span><span class=\"p\">,</span>\n            <span class=\"n\">SqlTableName</span><span class=\"p\">,</span>\n            <span class=\"n\">description</span><span class=\"o\">=</span><span class=\"s1\">&#39;The SQL table containing westbound delays.&#39;</span><span class=\"p\">,</span>\n        <span class=\"p\">),</span>\n        <span class=\"n\">InputDefinition</span><span class=\"p\">(</span>\n            <span class=\"s1\">&#39;eastbound_delays&#39;</span><span class=\"p\">,</span>\n            <span class=\"n\">SqlTableName</span><span class=\"p\">,</span>\n            <span class=\"n\">description</span><span class=\"o\">=</span><span class=\"s1\">&#39;The SQL table containing eastbound delays.&#39;</span><span class=\"p\">,</span>\n        <span class=\"p\">),</span>\n    <span class=\"p\">],</span>\n    <span class=\"n\">output_defs</span><span class=\"o\">=</span><span class=\"p\">[</span>\n        <span class=\"n\">OutputDefinition</span><span class=\"p\">(</span>\n            <span class=\"n\">dagster_type</span><span class=\"o\">=</span><span class=\"n\">FileHandle</span><span class=\"p\">,</span>\n            <span class=\"c1\"># name=&#39;plots_pdf_path&#39;,</span>\n            <span class=\"n\">description</span><span class=\"o\">=</span><span class=\"s1\">&#39;The saved PDF plots.&#39;</span><span class=\"p\">,</span>\n        <span class=\"p\">)</span>\n    <span class=\"p\">],</span>\n<span class=\"p\">)</span>\n</pre></div>\n</div>\n<p>As usual, we define the inputs and outputs of the new solid. Within the notebook itself, we only\nneed to add a single line as the first cell:</p>\n<div class=\"highlight-python notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"kn\">import</span> <span class=\"nn\">dagstermill</span>\n</pre></div>\n</div>\n<p>Then, in a cell with the <code class=\"docutils literal notranslate\"><span class=\"pre\">parameters</span></code> tag, we write:</p>\n<div class=\"highlight-python notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"n\">context</span> <span class=\"o\">=</span> <span class=\"n\">dagstermill</span><span class=\"o\">.</span><span class=\"n\">get_context</span><span class=\"p\">()</span>\n\n<span class=\"n\">eastbound_delays</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;eastbound_delays&#39;</span>\n<span class=\"n\">westbound_delays</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;westbound_delays&#39;</span>\n</pre></div>\n</div>\n<p>When dagstermill executes this notebook as part of a pipeline, values for these inputs will be\ninjected from the output of upstream solids.</p>\n<p>Finally, at the very end of the notebook, we yield the result back to the pipeline:</p>\n<div class=\"highlight-python notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"n\">dagstermill</span><span class=\"o\">.</span><span class=\"n\">yield_result</span><span class=\"p\">(</span><span class=\"n\">LocalFileHandle</span><span class=\"p\">(</span><span class=\"n\">pdf_path</span><span class=\"p\">))</span>\n</pre></div>\n</div>\n</div>\n</div>\n</div>\n",
  "metatags": "",
  "rellinks": [
    ["genindex", "General Index", "I", "index"],
    ["py-modindex", "Python Module Index", "", "modules"],
    ["sections/learn/principles", "Principles", "N", "next"],
    [
      "sections/learn/tutorial/intro_airflow",
      "Deploying to Airflow",
      "P",
      "previous"
    ]
  ],
  "sourcename": "sections/learn/airline_demo.rst.txt",
  "toc": "<ul>\n<li><a class=\"reference internal\" href=\"#\">Airline Demo</a><ul>\n<li><a class=\"reference internal\" href=\"#getting-started\">Getting Started</a></li>\n<li><a class=\"reference internal\" href=\"#pipelines-config\">Pipelines &amp; Config</a></li>\n<li><a class=\"reference internal\" href=\"#the-ingest-pipeline\">The Ingest Pipeline</a><ul>\n<li><a class=\"reference internal\" href=\"#running-the-pipeline-locally-with-test-config\">Running the pipeline locally with test config</a></li>\n<li><a class=\"reference internal\" href=\"#reusable-components-in-dagster\">Reusable Components in Dagster</a></li>\n<li><a class=\"reference internal\" href=\"#strongly-typed-config-and-outputs\">Strongly typed config and outputs</a></li>\n<li><a class=\"reference internal\" href=\"#using-modes-for-production-data\">Using Modes for Production Data</a></li>\n<li><a class=\"reference internal\" href=\"#ingesting-data-to-spark-data-frames\">Ingesting data to Spark data frames</a></li>\n<li><a class=\"reference internal\" href=\"#loading-data-to-the-warehouse\">Loading data to the warehouse</a></li>\n</ul>\n</li>\n<li><a class=\"reference internal\" href=\"#the-warehouse-pipeline\">The Warehouse Pipeline</a><ul>\n<li><a class=\"reference internal\" href=\"#the-sql-solid-wrapping-foreign-code-in-a-solid\">The <code class=\"docutils literal notranslate\"><span class=\"pre\">sql_solid</span></code>: wrapping foreign code in a solid</a></li>\n<li><a class=\"reference internal\" href=\"#dagstermill-executing-and-checkpointing-notebooks\">Dagstermill: executing and checkpointing notebooks</a></li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n",
  "display_toc": true,
  "page_source_suffix": ".rst",
  "current_page_name": "sections/learn/airline_demo",
  "sidebars": ["globaltoc.html", "searchbox.html"],
  "customsidebar": null,
  "alabaster_version": "0.7.12"
}
