---
title: "Dagster Embedded ELT: dlt"
description: Ingest data with ease using Dagster and dlt
---

## dlt

To get started with `dagster-embedded-elt` and _dlt_, first familiarize yourself with the quick [introduction to dlt](https://dlthub.com/docs/intro). When using the _dlt_ integration, you can either write your own sources, or use one of the many pre-made [verified sources](https://dlthub.com/docs/dlt-ecosystem/verified-sources/) in the _dlt_ ecosystem.

## Background

The [data load tool (dlt)](https://dlthub.com/) open-source library defines a standardized approach for creating data pipelines that load often messy data sources into well structured data sets. It offers many advanced features, such as handling of connection secrets, converting data into the structure required for a destination, and incremental updates and merges. Additionally, _dlt_ provides a large collection of [verified sources](https://dlthub.com/docs/dlt-ecosystem/verified-sources/) and [destinations](https://dlthub.com/docs/dlt-ecosystem/destinations/), meaning there is a good chance you won't have to write any code, and can instead leverage the work of the community!

The Dagster _dlt_ integration makes use of [multi-assets](/concepts/assets/multi-assets#multi-assets), a single definition that results in multiple assets. These assets are derived from the `DltSource`. Let's take a look at an example _dlt_ source definition. In this example we are defining a source that is comprised of two resources. Each of resources queries an API endpoint and yields the data that we wish to load into our data warehouse. The two _resources_ defined on this _source_ will map to Dagster assets.

```python
@dlt.source
def example(api_key: str = dlt.secrets.value):
    @dlt.resource(primary_key="id", write_disposition="merge")
    def courses():
        response = requests.get(url=BASE_URL + "courses")
        response.raise_for_status()
        yield response.json().get("items")

    @dlt.resource(primary_key="id", write_disposition="merge")
    def users():
        for page in _paginate(BASE_URL + "users"):
            yield page

    return courses, users
```

Next, we can define a _pipeline_ specifying how we would like to load this data.

```python
pipeline = dlt.pipeline(
    pipeline_name="example_pipeline", destination="snowflake", dataset_name="example_data"
)
```

These are the two components required to load data using _dlt_, a _source_ and a _pipeline_. To integrate with Dagster, these will be the parameters to our multi-asset decorator.

Next, we will discuss how to setup your Dagster project for _dlt_, and walk through how you can leverage a pre-defined _verified source_.

## Tutorial

## 1. Configuring your Dagster project to support `dlt`

You will need to define a location for the `dlt` code used for ingesting data. We recommend creating a `dlt_sources` directory at the root of your Dagster project, however, this code can reside anywhere within your Python project.

```bash
cd $DAGSTER_HOME && mkdir dlt_sources
```

## 2. Initialize `dlt` ingestion code

In your `dlt_sources/` directory, you can write your own ingestion code following the [dlt tutorial](https://dlthub.com/docs/tutorial/load-data-from-an-api), or you can use a verified source. Here, we are going to demonstrate how you integrate the [GitHub source](https://dlthub.com/docs/dlt-ecosystem/verified-sources/github) provided by `dlt`.

First, install the _dlt_ command line utility.

```bash
$ pip install dlt
```

Then, create a location for your _dlt_ source code, and initialize the GitHub source.

```bash
$ cd dlt_sources/

$ dlt init github snowflake
Looking up the init scripts in https://github.com/dlt-hub/verified-sources.git...
Cloning and configuring a verified source github (Source that load github issues, pull requests and reactions for a specific repository via customizable graphql query. Loads events incrementally.)
Do you want to proceed? [Y/n]: y

Verified source github was added to your project!
* See the usage examples and code snippets to copy from github_pipeline.py
* Add credentials for snowflake and other secrets in ./.dlt/secrets.toml
* requirements.txt was created. Install it with:
pip3 install -r requirements.txt
* Read https://dlthub.com/docs/walkthroughs/create-a-pipeline for more information
```

This has pulled down the code required to easily collect data from the GitHub API. It has also created a `requirements.txt` and a `.dlt/` configuration. In this case, we can remove these files, as we will be handling configuration with environment variables, and the requirements are to be defined in Dagster's dependencies specification.

```bash
$ tree -a
.
├── .dlt               # can be removed
│   ├── .sources
│   ├── config.toml
│   └── secrets.toml
├── .gitignore
├── github
│   ├── README.md
│   ├── __init__.py
│   ├── helpers.py
│   ├── queries.py
│   └── settings.py
├── github_pipeline.py
└── requirements.txt   # can be removed
```

## 3. Define environment variables required by `dlt`

This integration manages connections and secrets using environment variables as `dlt`. The `dlt` library is able to "magically" infer required environment variables used by its sources and resources. See [Secrets and Configs](https://dlthub.com/docs/general-usage/credentials/configuration) for more information.

In this example, the `github_reactions` source requires a GitHub access token, and the _snowflake_ destination requires database connection details. This results in the following required environment variables:

```bash
SOURCES__GITHUB__ACCESS_TOKEN=""
DESTINATION__SNOWFLAKE__CREDENTIALS__DATABASE=""
DESTINATION__SNOWFLAKE__CREDENTIALS__PASSWORD=""
DESTINATION__SNOWFLAKE__CREDENTIALS__USERNAME=""
DESTINATION__SNOWFLAKE__CREDENTIALS__HOST=""
DESTINATION__SNOWFLAKE__CREDENTIALS__WAREHOUSE=""
DESTINATION__SNOWFLAKE__CREDENTIALS__ROLE=""
```

Ensure that these are defined in your environment.

## 4. Define a `DagsterDltResource`

The <PyObject module="dagster_embedded_elt.dlt" object="DagsterDltResource" /> provides a wrapper of a _dlt_ pipeline runner. This can be shared across _dlt_ pipelines, and is a required parameter to our _dlt_ asset definition.

```python
from dagster_embedded_elt.dlt import DagsterDltResource

dlt_resource = DagsterDltResource()
```

## 5. Create a `dlt_assets` definition for GitHub

The `dlt_assets` decorator takes a `dlt_source` and `dlt_pipeline` parameter. In this example, we will use the `github_reactions` source and create a `dlt_pipeline` to ingest from Github to Snowflake.

```python
@dlt_assets(
    dlt_source=github_reactions(
        "dagster-io", "dagster", max_items=250
    ),
    dlt_pipeline=pipeline(
        pipeline_name="github_issues",
        dataset_name="github",
        destination="snowflake",
    ),
    name="github",
    group_name="github",
)
def github_reactions_dagster_assets(context: AssetExecutionContext, dlt: DagsterDltResource):
    yield from dlt.run(context=context)


defs = Definitions(
    assets=[
        dagster_github_assets,
    ],
    resources={
        "dlt": dlt_resource,
    },
)
```

And that's it! You should now have two assets that load data to corresponding Snowflake tables; One for issues, and the other for pull requests.

If you would like to see more real-world examples of using `dlt` in production, see how we're using it internally in [Dagster Open Platform](https://github.com/dagster-io/dagster-open-platform).

## Relevant APIs

| Name                                                                         | Description                                                    |
| ---------------------------------------------------------------------------- | -------------------------------------------------------------- |
| <PyObject module="dagster_embedded_elt.dlt" object="dlt_assets" decorator /> | The core dlt asset factory for building ingestion jobs         |
| <PyObject module="dagster_embedded_elt.dlt" object="DagsterDltResource" />   | The dlt resource for running ingestions.                       |
| <PyObject module="dagster_embedded_elt.dlt" object="DagsterDltTranslator" /> | A translator for specifying how to map between dlt and Dagster |
