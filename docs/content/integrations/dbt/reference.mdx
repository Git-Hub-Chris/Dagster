---
title: "dagster-dbt integration reference"
description: Dagster can orchestrate dbt alongside other technologies.
---

# dagster-dbt integration reference

<Note>
  Using dbt Cloud? Check out the{" "}
  <a href="/integrations/dbt-cloud">dbt Cloud with Dagster guide</a>!
</Note>

This reference provides a high-level look at working with dbt models through Dagster's [software-defined assets](/concepts/assets/software-defined-assets) framework using the [`dagster-dbt` integration library](/\_apidocs/libraries/dagster-dbt).

For a step-by-step implementation walkthrough, refer to the [Using dbt with Dagster software-defined assets tutorial](/integrations/dbt/using-dbt-with-dagster).

---

## Loading dbt models from a dbt project

The `dagster-dbt` library offers two methods of loading dbt models from a project into Dagster:

- **For smaller dbt projects** where compilation time isn't a concern, we recommend the <PyObject module="dagster_dbt" object="load_assets_from_dbt_project" /> function, or
- **For larger dbt projects**, we recommend the <PyObject module="dagster_dbt" object="load_assets_from_dbt_manifest" /> function to load from an existing dbt `manifest.json` file

### Loading models using load_assets_from_dbt_project

<Note>
  Check out{" "}
  <a href="/integrations/dbt/using-dbt-with-dagster/load-dbt-models">
    part two of the dbt + Dagster tutorial
  </a>{" "}
  to see this concept in context.
</Note>

For smaller dbt projects where compilation time is not a concern, the simplest way to load your dbt assets into Dagster is the following:

```python startafter=start_load_assets_from_dbt_project endbefore=end_load_assets_from_dbt_project file=/integrations/dbt/dbt.py dedent=4
from dagster_dbt import load_assets_from_dbt_project

dbt_assets = load_assets_from_dbt_project(project_dir="path/to/dbt/project")
```

The <PyObject module="dagster_dbt" object="load_assets_from_dbt_project" /> function:

1. Compiles your dbt project,
2. Parses the metadata that dbt provides, and
3. Generates a set of software-defined assets that reflect the models in the project. These assets will share the same underlying operation, which will invoke dbt to run the models represented by the loaded assets.

### Loading models using load_assets_from_dbt_manifest

For larger dbt projects, the overhead involved with recompiling the entire project may be a concern. In these cases, you can load dbt models from an existing dbt `manifest.json` file using the <PyObject module="dagster_dbt" object="load_assets_from_dbt_manifest" /> function:

```python startafter=start_load_assets_from_dbt_manifest endbefore=end_load_assets_from_dbt_manifest file=/integrations/dbt/dbt.py dedent=4
import json

from dagster_dbt import load_assets_from_dbt_manifest

with open("path/to/dbt/manifest.json") as f:
    manifest_json = json.load(f)

dbt_assets = load_assets_from_dbt_manifest(manifest_json)
```

If you make any changes to your dbt project that change the structure of the project (such as changing the dependencies of a model or adding a new one), you'll need to regenerate your manifest file for those changes to be reflected in Dagster.

---

## Using the DbtCliResource for dbt CLI commands

<Note>
  Check out{" "}
  <a href="/integrations/dbt/using-dbt-with-dagster/upstream-assets">
    part three of the dbt + Dagster tutorial
  </a>{" "}
  to see this concept in context.
</Note>

Assets loaded from dbt require a dbt resource, which is responsible for firing off dbt CLI commands. The `dagster-dbt` integration provides the <PyObject module="dagster_dbt" object="DbtCliResource" /> for this purpose. This resource can be configured with CLI flags that are passed into every dbt invocation.

The most important flag to set is the `project_dir` flag, which points Dagster at the directory of your dbt project. You can also use the `target` flag to point at an explicit dbt [target](https://docs.getdbt.com/reference/dbt-jinja-functions/target). For a full list of configuration options, refer to the <PyObject module="dagster_dbt" object="DbtCliResource" /> API docs.

You can configure this resource and add it to your dbt assets by doing the following:

```python startafter=start_dbt_cli_resource endbefore=end_dbt_cli_resource file=/integrations/dbt/dbt.py dedent=4
import os

from dagster_dbt import DbtCliResource, load_assets_from_dbt_project

from dagster import Definitions

DBT_PROJECT_PATH = "path/to/dbt_project"
DBT_TARGET = "hive" if os.getenv("EXECUTION_ENV") == "prod" else "duckdb"

defs = Definitions(
    assets=load_assets_from_dbt_project(DBT_PROJECT_PATH),
    resources={
        "dbt": DbtCliResource(project_dir=DBT_PROJECT_PATH, target=DBT_TARGET),
    },
)
```

---

## Scheduling dbt jobs

Once you have your dbt assets, you can define a job that runs some or all of these assets on a schedule:

```python startafter=start_schedule_assets endbefore=end_schedule_assets file=/integrations/dbt/dbt.py dedent=4
from dagster import ScheduleDefinition, define_asset_job, Definitions

run_everything_job = define_asset_job("run_everything", selection="*")

# only `order_stats` and its children
run_something_job = define_asset_job("run_something", selection="order_stats*")

defs = Definitions(
    assets=dbt_assets,
    schedules=[
        ScheduleDefinition(
            job=run_something_job,
            cron_schedule="@daily",
        ),
        ScheduleDefinition(
            job=run_everything_job,
            cron_schedule="@weekly",
        ),
    ],
)
```

Refer to the [Schedule documentation](/concepts/partitions-schedules-sensors/schedules#running-the-scheduler) for more info on running jobs on a schedule.

---

## Understanding asset definition attributes

In Dagster, each asset definition has attributes. Dagster automatically generates these attributes for each software-defined asset loaded from the dbt project. These attributes can optionally be overridden by the user.

- [Customizing asset keys](#customizing-asset-keys)
- [Customizing group names](#customizing-group-names)
- [Customizing descriptions](#customizing-descriptions)
- [Customizing metadata](#customizing-metadata)

### Customizing asset keys

For dbt models, seeds, and snapshots, the default asset key will be the configured schema for that node, concatenated with the name of the node.

| dbt node type         | Schema    | Model name   | Resulting asset key |
| --------------------- | --------- | ------------ | ------------------- |
| model, seed, snapshot | `None`    | `MODEL_NAME` | `MODEL_NAME`        |
|                       | `SCHEMA`  | `MODEL_NAME` | `SCHEMA/MODEL_NAME` |
|                       | `None`    | my_model     | some_model          |
|                       | marketing | my_model     | marketing/my_model  |

For dbt sources, the default asset key will be the name of the source concatenated with the name of the source table.

| dbt node type | Source name   | Table name   | Resulting asset key      |
| ------------- | ------------- | ------------ | ------------------------ |
| source        | `SOURCE_NAME` | `TABLE_NAME` | `SOURCE_NAME/TABLE_NAME` |
|               | jaffle_shop   | orders       | jaffle_shop/orders       |

There are two ways to customize the asset keys generated by Dagster for dbt assets:

1. Defining [meta config](https://docs.getdbt.com/reference/resource-configs/meta) on your dbt node, or
2. Overriding Dagster's asset key generation by implementing a custom <PyObject module="dagster_dbt" object="DagsterDbtTranslator" />.

To override an asset key generated by Dagster for a dbt node, you can define a `meta` key on your dbt node's `.yml` file. The following example overrides the asset key for a source and table as `snowflake/jaffle_shop/orders`:

```yaml
sources:
  - name: jaffle_shop
    tables:
      - name: orders
        meta:
          dagster:
            asset_key: ["snowflake", "jaffle_shop", "orders"]
```

Alternatively, to override the asset key generation for all dbt nodes in your dbt project, you can create a custom <PyObject module="dagster_dbt" object="DagsterDbtTranslator" /> and implement <PyObject module="dagster_dbt" object="DagsterDbtTranslator" method="get_asset_key"/>. The following example adds a `snowflake` prefix to the default generated asset key:

```python startafter=start_custom_asset_key_dagster_dbt_translator endbefore=end_custom_asset_key_dagster_dbt_translator file=/integrations/dbt/dbt.py dedent=4
from pathlib import Path
from dagster import AssetKey, OpExecutionContext
from dagster_dbt import DagsterDbtTranslator, DbtCliResource, dbt_assets
from typing import Any, Mapping

manifest_path = Path("path/to/dbt_project/target/manifest.json")

class CustomDagsterDbtTranslator(DagsterDbtTranslator):
    def get_asset_key(self, dbt_resource_props: Mapping[str, Any]) -> AssetKey:
        return self.get_asset_key(dbt_resource_props).with_prefix("snowflake")

@dbt_assets(
    manifest=manifest_path,
    dagster_dbt_translator=CustomDagsterDbtTranslator(),
)
def my_dbt_assets(context: OpExecutionContext, dbt: DbtCliResource):
    yield from dbt.cli(["build"], context=context).stream()
```

### Customizing group names

For dbt models, seeds, and snapshots, the default Dagster group name will be the [dbt group](https://docs.getdbt.com/docs/build/groups) defined for that node.

| dbt node type         | dbt group name | Resulting Dagster group name |
| --------------------- | -------------- | ---------------------------- |
| model, seed, snapshot | `GROUP_NAME`   | `GROUP_NAME`                 |
|                       | `None`         | `None`                       |
|                       | finance        | finance                      |

There are two ways to customize the asset keys generated by Dagster for dbt assets:

1. Defining [meta config](https://docs.getdbt.com/reference/resource-configs/meta) on your dbt node, or
2. Overriding Dagster's group name generation by implementing a custom <PyObject module="dagster_dbt" object="DagsterDbtTranslator" />

To override the group name generated by Dagster for a dbt node, you can define a `meta` key in your dbt project file, on your dbt node's property file, or on the node's in-file config block. The following example overrides the Dagster group name for the following model as `marketing`:

```yaml
models:
  - name: customers
    config:
      meta:
        dagster:
          group: marketing
```

Alternatively, to override the Dagster group name generation for all dbt nodes in your dbt project, you can create a custom <PyObject module="dagster_dbt" object="DagsterDbtTranslator" /> and implement <PyObject module="dagster_dbt" object="DagsterDbtTranslator" method="get_group_name"/>. The following example defines `snowflake` as the group name for all dbt nodes:

```python startafter=start_custom_group_name_dagster_dbt_translator endbefore=end_custom_group_name_dagster_dbt_translator file=/integrations/dbt/dbt.py dedent=4
from pathlib import Path
from dagster import OpExecutionContext
from dagster_dbt import DagsterDbtTranslator, DbtCliResource, dbt_assets
from typing import Any, Mapping, Optional

manifest_path = Path("path/to/dbt_project/target/manifest.json")

class CustomDagsterDbtTranslator(DagsterDbtTranslator):
    def get_group_name(
        self, dbt_resource_props: Mapping[str, Any]
    ) -> Optional[str]:
        return "snowflake"

@dbt_assets(
    manifest=manifest_path,
    dagster_dbt_translator=CustomDagsterDbtTranslator(),
)
def my_dbt_assets(context: OpExecutionContext, dbt: DbtCliResource):
    yield from dbt.cli(["build"], context=context).stream()
```

### Customizing descriptions

For dbt models, seeds, and snapshots, the default Dagster description will be the dbt node's description.

To override the Dagster description for all dbt nodes in your dbt project, you can create a custom <PyObject module="dagster_dbt" object="DagsterDbtTranslator" /> and implement <PyObject module="dagster_dbt" object="DagsterDbtTranslator" method="get_description"/>. The following example defines the raw SQL of the dbt node as the Dagster description:

```python startafter=start_custom_description_dagster_dbt_translator endbefore=end_custom_description_dagster_dbt_translator file=/integrations/dbt/dbt.py dedent=4
import textwrap
from pathlib import Path
from dagster import OpExecutionContext
from dagster_dbt import DagsterDbtTranslator, DbtCliResource, dbt_assets
from typing import Any, Mapping

manifest_path = Path("path/to/dbt_project/target/manifest.json")

class CustomDagsterDbtTranslator(DagsterDbtTranslator):
    def get_description(self, dbt_resource_props: Mapping[str, Any]) -> str:
        return textwrap.indent(dbt_resource_props.get("raw_sql", ""), "\t")

@dbt_assets(
    manifest=manifest_path,
    dagster_dbt_translator=CustomDagsterDbtTranslator(),
)
def my_dbt_assets(context: OpExecutionContext, dbt: DbtCliResource):
    yield from dbt.cli(["build"], context=context).stream()
```

### Customizing metadata

For dbt models, seeds, and snapshots, the default Dagster metadata will be the dbt node's table schema.

To override the Dagster metadata for all dbt nodes in your dbt project, you can create a custom <PyObject module="dagster_dbt" object="DagsterDbtTranslator" /> and implement <PyObject module="dagster_dbt" object="DagsterDbtTranslator" method="get_metadata"/>. The following example defines the metadata of the dbt node as the Dagster metadata:

```python startafter=start_custom_metadata_dagster_dbt_translator endbefore=end_custom_metadata_dagster_dbt_translator file=/integrations/dbt/dbt.py dedent=4
from pathlib import Path
from dagster import MetadataValue, OpExecutionContext
from dagster_dbt import DagsterDbtTranslator, DbtCliResource, dbt_assets
from typing import Any, Mapping

manifest_path = Path("path/to/dbt_project/target/manifest.json")

class CustomDagsterDbtTranslator(DagsterDbtTranslator):
    def get_metadata(
        self, dbt_resource_props: Mapping[str, Any]
    ) -> Mapping[str, Any]:
        return {"meta": MetadataValue.json(dbt_resource_props.get("meta", {}))}

@dbt_assets(
    manifest=manifest_path,
    dagster_dbt_translator=CustomDagsterDbtTranslator(),
)
def my_dbt_assets(context: OpExecutionContext, dbt: DbtCliResource):
    yield from dbt.cli(["build"], context=context).stream()
```

---

## dbt models, code versions, and staleness

Note that Dagster allows the optional specification of a [`code_version`](/concepts/assets/software-defined-assets#asset-code-versions) for each software-defined asset, which are used to track changes. The `code_version` for an asset arising from a dbt model is defined automatically as the hash of the SQL defining the DBT model. This allows the asset graph in the UI to indicate which dbt models have new SQL since they were last materialized.

---

## Defining dependencies

- [Upstream dependencies](#upstream-dependencies)
- [Downstream dependencies](#downstream-dependencies)

### Upstream dependencies

<Note>
  Check out parts{" "}
  <a href="/integrations/dbt/using-dbt-with-dagster/load-dbt-models">two</a> and{" "}
  <a href="/integrations/dbt/using-dbt-with-dagster/upstream-assets">
    three of the dbt + Dagster tutorial
  </a>{" "}
  to see this concept in context.
</Note>

Dagster parses information about assets that are upstream of specific dbt models from the dbt project itself. Whenever a model is downstream of a [dbt source](https://docs.getdbt.com/docs/building-a-dbt-project/using-sources), that source will be parsed as an upstream asset.

For example, if you defined a source in your `sources.yml` file like this:

```yaml
sources:
  - name: jaffle_shop
    tables:
      - name: orders
```

and use it in a model:

```sql
select *
  from {{ source("jaffle_shop", "orders") }}
 where foo=1
```

Then the asset created for that model will be given an upstream asset key of `jaffle_shop/orders`. In many cases, this upstream asset might also be managed by Dagster.

If you add an asset definition to your repository which produces `jaffle_shop/orders`, then this asset will be upstream of your dbt model:

```python startafter=start_upstream_asset endbefore=end_upstream_asset file=/integrations/dbt/dbt.py dedent=4
@asset(key_prefix="jaffle_shop")
def orders():
    return ...
```

### Downstream dependencies

<Note>
  Check out{" "}
  <a href="/integrations/dbt/using-dbt-with-dagster/downstream-assets">
    part four of the dbt + Dagster tutorial
  </a>{" "}
  to see this concept in context.
</Note>

Dagster allows you to define assets that are downstream of specific dbt models. One property of dbt-based assets is that the external tool - in this case, dbt - handles storing each model in the database internally, rather than Dagster directly storing the tables that are updated.

This means that there's a range of ways to load a dbt model as input to a Python function. For example, you might want to load the contents as a Pandas dataframe or into a PySpark session. You can specify this loading behavior on each downstream asset.

For example, if you wanted to consume a dbt model with the asset key `my_dbt_model` as a Pandas dataframe, that would look something like the following:

```python startafter=start_downstream_asset endbefore=end_downstream_asset file=/integrations/dbt/dbt.py dedent=4
@asset(
    ins={"my_dbt_model": AssetIn(input_manager_key="pandas_df_manager")},
)
def my_downstream_asset(my_dbt_model):
    # my_dbt_model is a Pandas dataframe
    return my_dbt_model.where(foo="bar")
```

#### Defining an I/O manager

To materialize your dbt assets, you need to tell Dagster how to handle the assets' inputs and outputs. You can do this using an [I/O manager](/concepts/io-management/io-managers).

The implementation of your I/O manager depends on:

- The Python object you want to use to represent your table, and
- The database that dbt writes tables to

A simple I/O manager implementation that loads data from a dbt-managed table into a Pandas dataframe would look something like the following:

```python startafter=start_input_manager endbefore=end_input_manager file=/integrations/dbt/dbt.py dedent=4
import pandas as pd

from dagster import ConfigurableIOManager

class PandasIOManager(ConfigurableIOManager):
    connection_str: str

    def handle_output(self, context, obj):
        # dbt handles outputs for us
        pass

    def load_input(self, context) -> pd.DataFrame:
        """Load the contents of a table as a pandas DataFrame."""
        table_name = context.asset_key.path[-1]
        return pd.read_sql(f"SELECT * FROM {table_name}", con=self.connection_str)
```

Once the I/O manager is defined, you can supply it like any other resource when calling <PyObject object="with_resources"/> :

```python startafter=start_input_manager_resources endbefore=end_input_manager_resources file=/integrations/dbt/dbt.py dedent=4
from dagster_dbt import DbtCliResource, load_assets_from_dbt_project

from dagster import Definitions

defs = Definitions(
    assets=load_assets_from_dbt_project(...),
    resources={
        "dbt": DbtCliResource(project_dir="path/to/dbt_project"),
        "pandas_df_manager": PandasIOManager(connection_str=...),
    },
)
```

---

## Related

<ArticleList>
  <ArticleListItem
    title="Using dbt with Dagster software-defined assets"
    href="/integrations/dbt/using-dbt-with-dagster"
  ></ArticleListItem>
  <ArticleListItem
    title="dagster-dbt API reference"
    href="/\_apidocs/libraries/dagster-dbt"
  ></ArticleListItem>
  <ArticleListItem
    title="Using dbt Cloud with Dagster"
    href="/integrations/dbt-cloud"
  ></ArticleListItem>
</ArticleList>
