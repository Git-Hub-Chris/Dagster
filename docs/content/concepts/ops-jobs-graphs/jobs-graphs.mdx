---
title: Jobs | Dagster
description: A job is an executable graph of ops, with optional resources and configuration.
---

# Jobs

Jobs are the main unit of execution and monitoring in Dagster. The core of a job is a [graph](/concepts/ops-jobs-graphs/graphs) of [ops](/concepts/ops-jobs-graphs/ops) connected via data dependencies.

Ops are linked together by defining the dependencies between their inputs and outputs. An important difference between Dagster and other workflow systems is that, in Dagster, op dependencies are expressed as data dependencies, not just execution dependencies.

This difference enables Dagster to support richer modeling of dependencies. Instead of merely ensuring that the order of execution is correct, dependencies in Dagster provide a variety of compile and run-time checks.

Using jobs, you can:

- Determine op behavior using [resources](/concepts/resources) and [configuration](/concepts/configuration/config-schema)
- Define [schedules](/concepts/partitions-schedules-sensors/schedules) to execute jobs at fixed intervals
- Define [sensors](/concepts/partitions-schedules-sensors/sensors) to trigger jobs when external changes occur

---

## Relevant APIs

| Name                                | Description                                                                                                                                                     |
| ----------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| <PyObject object="job" decorator /> | The decorator used to create a job.                                                                                                                             |
| <PyObject object="JobDefinition" /> | A job definition. Jobs are the main unit of execution and monitoring in Dagster. Typically constructed using the <PyObject object="job" decorator /> decorator. |
|                              |

---

## Creating jobs

Jobs can be created in several ways:

- [Using the <PyObject object="job" decorator /> decorator](#using-the-job-decorator)
- [From assets](#from-assets)
- [From a graph](#from-a-graph)

### Using the @job decorator

The simplest way to create a job is to use the <PyObject object="job"/> decorator.

Within the decorated function body, you can use function calls to indicate the dependency structure between the ops/graphs. In this example, the `add_one` op depends on the `return_five` op's output. Because this data dependency exists, the `add_one` op executes after `return_five` runs successfully and emits the required output.

```python file=/concepts/ops_jobs_graphs/simple_job.py
from dagster import job, op


@op
def return_five():
    return 5


@op
def add_one(arg):
    return arg + 1


@job
def do_stuff():
    add_one(return_five())
```

When defining a job, you can provide [resources](/concepts/resources), [configuration](/concepts/configuration/config-schema), [hooks](/concepts/ops-jobs-graphs/op-hooks), tags, and an [executor](/deployment/executors) (follow the links for explanation of how to use each of these).

### From assets

<TODO />

### From a graph

Dagster is built around the observation that any data DAG typically contains a logical core of data transformation, which is reusable across a set of environments (e.g. prod, local, staging) or scenarios. The graph usually needs to be "customized" for each environment, by plugging in configuration and services that are specific to that environment.

You can model this by building multiple jobs that use the same underlying graph of ops. The graph represents the logical core of data transformation, and the configuration and resources on each job customize the behavior of that job for its environment.

To do this, you first define a graph with the <PyObject object="graph" decorator /> decorator.

```python file=/concepts/ops_jobs_graphs/jobs_from_graphs.py startafter=start_define_graph endbefore=end_define_graph
from dagster import graph, op


@op(required_resource_keys={"server"})
def interact_with_server(context):
    context.resources.server.ping_server()


@graph
def do_stuff():
    interact_with_server()
```

Then you build jobs from it using the <PyObject object="GraphDefinition" method="to_job" /> method:

```python file=/concepts/ops_jobs_graphs/jobs_from_graphs.py startafter=start_define_jobs endbefore=end_define_jobs
from dagster import ResourceDefinition

prod_server = ResourceDefinition.mock_resource()
local_server = ResourceDefinition.mock_resource()

prod_job = do_stuff.to_job(resource_defs={"server": prod_server}, name="do_stuff_prod")
local_job = do_stuff.to_job(
    resource_defs={"server": local_server}, name="do_stuff_local"
)
```

`to_job` accepts the same arguments as the <PyObject object="job" decorator /> decorator: you can provide [resources](/concepts/resources), [configuration](/concepts/configuration/config-schema), [hooks](/concepts/ops-jobs-graphs/op-hooks), tags, and an [executor](/deployment/executors).

---

## Building jobs

This section covers a few basic patterns you can use to build more complex graphs and jobs.

- [That use the same op twice]()
- [With multiple inputs]()
- [With conditional branching]()
- [With a fixed fan-in]()

### That use the same op twice

You can use the same op definition multiple times in the same graph/job.

```python file=/concepts/ops_jobs_graphs/jobs.py startafter=start_multiple_usage_pipeline endbefore=end_multiple_usage_pipeline
@job
def multiple_usage():
    add_one(add_one(return_one()))
```

To differentiate between the two invocations of `add_one`, Dagster automatically aliases the op names to be `add_one` and `add_one_2`.

You can also manually define the alias by using the `.alias` method on the op invocation.

```python file=/concepts/ops_jobs_graphs/jobs.py startafter=start_alias_pipeline endbefore=end_alias_pipeline
@job
def alias():
    add_one.alias("second_addition")(add_one(return_one()))
``` 

### With multiple inputs

<Image
alt="Multiple Inputs"
src="/images/concepts/pipelines/multi-inputs.png"
width={1000}
height={250}
/>

A single output can be passed to multiple inputs on downstream ops. In this example, the output from the first op is passed to two different ops. The outputs of those ops are combined and passed to the final op.

```python file=/concepts/ops_jobs_graphs/multiple_io_job.py startafter=start_marker endbefore=end_marker
from dagster import job, op


@op
def return_one(context) -> int:
    return 1


@op
def add_one(context, number: int):
    return number + 1


@op
def adder(context, a: int, b: int) -> int:
    return a + b


@job
def inputs_and_outputs():
    value = return_one()
    a = add_one(value)
    b = add_one(value)
    adder(a, b)
```

### With conditional branching

<Image
alt="Conditional Branch"
src="/images/concepts/pipelines/conditional.png"
width={1000}
height={250}
/>

An op only starts to execute once all of its inputs have been resolved. We can use this behavior to model conditional execution of ops.

In this example, the `branching_op` outputs either the `branch_1` result or `branch_2` result. Since op execution is skipped for ops that have unresolved inputs, only one of the downstream ops will execute.

```python file=/concepts/ops_jobs_graphs/branching_job.py startafter=start_marker endbefore=end_marker
import random

from dagster import Out, Output, job, op


@op(out={"branch_1": Out(is_required=False), "branch_2": Out(is_required=False)})
def branching_op():
    num = random.randint(0, 1)
    if num == 0:
        yield Output(1, "branch_1")
    else:
        yield Output(2, "branch_2")


@op
def branch_1_op(_input):
    pass


@op
def branch_2_op(_input):
    pass


@job
def branching():
    branch_1, branch_2 = branching_op()
    branch_1_op(branch_1)
    branch_2_op(branch_2)
```

When using conditional branching, <PyObject object="Output" /> objects must be yielded instead of returned.

### With a fixed fan-in

<Image
alt="Fixed Fan-in"
src="/images/concepts/pipelines/fixed-fan-in.png"
width={1000}
height={250}
/>

If you have a fixed set of op that all return the same output type, you can collect all the outputs into a list and pass them into a single downstream op.

The downstream op executes only if all of the outputs were successfully created by the upstream op.

```python file=/concepts/ops_jobs_graphs/fan_in_job.py startafter=start_marker endbefore=end_marker
from typing import List

from dagster import job, op


@op
def return_one() -> int:
    return 1


@op
def sum_fan_in(nums: List[int]) -> int:
    return sum(nums)


@job
def fan_in():
    fan_outs = []
    for i in range(0, 10):
        fan_outs.append(return_one.alias(f"return_one_{i}")())
    sum_fan_in(fan_outs)
```

In this example, we have 10 ops that all output the number `1`. The `sum_fan_in` op takes all of these outputs as a list and sums them.

---

## Configuring jobs

Ops and resources often accept [configuration](/concepts/configuration/config-schema) that determines how they behave. By default, you supply configuration for these ops and resources at the time you launch the job.

When constructing a job, you can customize how that configuration will be satisfied, by passing a value to the `config` parameter of the <PyObject object="GraphDefinition" method="to_job" /> method or the <PyObject object="job" decorator /> decorator. The options are discussed below:

- [Hardcoded configuration](#hardcoded-configuration)
- [Partitioned configuration](#partitioned-configuration)
- [Config mapping](#config-mapping)

### Hardcoded configuration

You can supply a config dictionary. The supplied dictionary will be used to configure the job whenever the job is launched. It will show up in the Dagit Launchpad and can be overridden.

```python file=/concepts/ops_jobs_graphs/jobs_with_default_config.py
from dagster import job, op


@op(config_schema={"config_param": str})
def do_something(context):
    context.log.info("config_param: " + context.op_config["config_param"])


default_config = {"ops": {"do_something": {"config": {"config_param": "stuff"}}}}


@job(config=default_config)
def do_it_all_with_default_config():
    do_something()


if __name__ == "__main__":
    # Will log "config_param: stuff"
    do_it_all_with_default_config.execute_in_process()
```

### Partitioned jobs

You can supply a <PyObject object="PartitionedConfig" />. It defines a discrete set of "partitions", along with a function for generating config for a partition. You can configure a job run by selecting a partition.

For more information on how this works, take a look at the [Partitions concept page](/concepts/partitions-schedules-sensors/partitions).

### Config mapping

You can supply a <PyObject object="ConfigMapping" />. This allows you to expose a narrower config interface to your job. Instead of needing to configure every op and resource individually when launching the job, you can supply a smaller number of values to the outer config, and the <PyObject object="ConfigMapping" /> can translate it into config for all the job's ops and resources.

```python file=/concepts/ops_jobs_graphs/jobs_with_config_mapping.py
from dagster import config_mapping, job, op


@op(config_schema={"config_param": str})
def do_something(context):
    context.log.info("config_param: " + context.op_config["config_param"])


@config_mapping(config_schema={"simplified_param": str})
def simplified_config(val):
    return {
        "ops": {"do_something": {"config": {"config_param": val["simplified_param"]}}}
    }


@job(config=simplified_config)
def do_it_all_with_simplified_config():
    do_something()


if __name__ == "__main__":
    # Will log "config_param: stuff"
    do_it_all_with_simplified_config.execute_in_process(
        run_config={"simplified_param": "stuff"}
    )
```

---

## Including jobs in repositories

You make jobs available to Dagit, GraphQLs, and the command line by including them inside [repositories](/concepts/repositories-workspaces/repositories). If you include schedules or sensors in a repository, the repository will automatically include jobs that those schedules or sensors target.

```python file=/concepts/ops_jobs_graphs/repo_with_job.py
from dagster import job, repository


@job
def do_it_all():
    ...


@repository
def my_repo():
    return [do_it_all]
```

---

## Testing jobs

Dagster has built-in support for testing your data applications, including separating business logic from environments and setting explicit expectations on uncontrollable inputs. Refer to the [Testing guide](/concepts/testing) for more info and examples.

---

## Executing jobs

You can run a job in a variety of ways:

- In the Python process where it's defined
- Via the command line
- Via a GraphQL API, or 
- In [Dagit](/concepts/dagit/dagit). Dagit centers on jobs, making it a one-stop-shop - you can manually kick off runs for a job and view all historical runs.

Refer to the [Job execution guide](/concepts/ops-jobs-graphs/job-execution) for more info and examples.

---

## See it in action

For more examples of jobs, check out the following in our [Hacker News example](https://github.com/dagster-io/dagster/tree/master/examples/hacker_news):

- [Creating multiple jobs from a graph](https://github.com/dagster-io/dagster/blob/master/examples/hacker_news/hacker_news/jobs/dbt_metrics.py)
- [Specifying config on a job](https://github.com/dagster-io/dagster/blob/master/examples/hacker_news/hacker_news/jobs/hacker_news_api_download.py)

Our [New York Times example](https://github.com/dagster-io/dagster/tree/master/examples/nyt-feed) covers:

- [Conditional branching](https://github.com/dagster-io/dagster/blob/master/examples/nyt-feed/nyt_feed/nyt_feed_job.py)
- [Using the same op twice](https://github.com/dagster-io/dagster/blob/master/examples/nyt-feed/nyt_feed/nyt_feed_job.py)
