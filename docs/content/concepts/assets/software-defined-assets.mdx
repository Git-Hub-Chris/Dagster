---
title: Software-Defined Assets | Dagster
description: A software-defined asset is a description of how to compute the contents of a particular data asset.
---

# Software-Defined Assets

An **asset** is an object in persistent storage, such as a table, file, or persisted machine learning model. A **software-defined asset** is a Dagster object that couples an asset to the function and upstream assets that are used to produce its contents.

Software-defined assets enable a declarative approach to data management, in which code is the source of truth on what data assets should exist and how those assets are computed.

A software-defined asset includes the following:

- An <PyObject object="AssetKey" />, which is a handle for referring to the asset.
- A set of upstream asset keys, which refer to assets that the contents of the software-defined asset are derived from.
- An [op](/concepts/ops-jobs-graphs/ops), which is a function responsible for computing the contents of the asset from its upstream dependencies.

  **Note**: A crucial distinction between software-defined assets and [ops](/concepts/ops-jobs-graphs/ops) is that software-defined assets know about their dependencies, while ops do not. Ops aren't connected to dependencies until they're placed inside a [graph](/concepts/ops-jobs-graphs/jobs-graphs).

**Materializing** an asset is the act of running its op and saving the results to persistent storage. You can initiate materializations from [Dagit](/concepts/dagit/dagit) or by invoking Python APIs. By default, assets are materialized to pickle files on your local filesystem, but materialization behavior is [fully customizable](#customizing-how-assets-are-materialized-with-io-managers). It's possible to materialize an asset in multiple storage environments, such as production and staging.

---

## Relevant APIs

| Name                                  | Description                                                                                                                                                                                                                  |
| ------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| <PyObject object="asset" decorator /> | A decorator used to define assets.                                                                                                                                                                                           |
| <PyObject object="SourceAsset" />     | A class that describes an asset, but doesn't define how to compute it. <PyObject object="SourceAsset" />s are used to represent assets that other assets depend on, in settings where they can't be materialized themselves. |

---

## Defining assets

Once you've defined a set of assets, you can:

- [View them in Dagit](#viewing-assets-in-dagit).
- [Materialize an ad-hoc set of them in Dagit](#materializing-assets-in-dagit).
- [Build a job](#building-jobs-that-materialize-assets), which materializes a fixed selection of the assets, and can be put on a schedule or sensor.

### A basic software-defined asset

The easiest way to create a software-defined asset is with the <PyObject object="asset" decorator /> decorator.

```python file=/concepts/assets/basic_asset_definition.py
from dagster import asset


@asset
def my_asset():
    return [1, 2, 3]
```

By default, the name of the decorated function, `my_asset`, is used as the asset key. The decorated function forms the asset's op: it's responsible for producing the asset's contents. This asset doesn't depend on any other assets.

### Assets with dependencies

- [Basic dependencies](#defining-basic-dependencies)
- [Explicit dependencies](#defining-explicit-dependencies)
- [External asset dependencies](#defining-external-asset-dependencies)

#### Defining basic dependencies

Software-defined assets can depend on other software-defined assets. The easiest way to define an asset dependency is to include an upstream asset name as an argument to the decorated function.

In the following example, `downstream_asset` depends on `upstream_asset`. That means that the contents of `upstream_asset` are provided to the function that computes the contents of `downstream_asset`.

```python file=/concepts/assets/asset_dependency.py startafter=start_marker endbefore=end_marker
@asset
def upstream_asset():
    return [1, 2, 3]


@asset
def downstream_asset(upstream_asset):
    return upstream_asset + [4]
```

#### Defining explicit dependencies

If defining dependencies by matching argument names to upstream asset names feels too magical for your tastes, you can also define dependencies in a more explicit way:

```python file=/concepts/assets/explicit_string_asset_dependency.py
from dagster import AssetIn, asset


@asset
def upstream_asset():
    return [1, 2, 3]


@asset(ins={"upstream": AssetIn("upstream_asset")})
def downstream_asset(upstream):
    return upstream + [4]
```

In this case, `ins={"upstream": AssetIn("upstream_asset")}` declares that the contents of the asset with the key `upstream_asset` will be provided to the function argument named `upstream`.

Asset keys can also be provided to <PyObject object="AssetIn" /> to explicitly identify the asset. For example:

```python file=/concepts/assets/explicit_asset_dependency_asset_keys.py
from dagster import AssetIn, AssetKey, asset

# One way of providing explicit asset keys:


@asset(ins={"upstream": AssetIn(asset_key="upstream_asset")})
def downstream_asset(upstream):
    return upstream + [4]


# Another way:


@asset(ins={"upstream": AssetIn(asset_key=AssetKey("upstream_asset"))})
def another_downstream_asset(upstream):
    return upstream + [10]
```

#### Defining external asset dependencies

Software-defined assets frequently depend on assets that are generated elsewhere. Using <PyObject object="SourceAsset" />, you can include these external assets and allow your other assets to depend on them.

For example:

```python file=/concepts/assets/source_asset.py startafter=start_marker endbefore=end_marker
from dagster import AssetKey, SourceAsset, asset

my_source_asset = SourceAsset(key=AssetKey("a_source_asset"))


@asset
def my_derived_asset(a_source_asset):
    return a_source_asset + [4]
```

**Note**: The source asset's asset key must be provided as the argument to downstream assets. In the previous example, the asset key is `a_source_asset` and not `my_source_asset`.

You can also re-use assets across repositories by including them as source assets:

```python file=/concepts/assets/cross_repository_asset.py
from dagster import AssetKey, SourceAsset, asset, repository


@asset
def repository_a_asset():
    return 5


@repository
def repository_a():
    return [repository_a_asset]


repository_a_source_asset = SourceAsset(key=AssetKey("repository_a_asset"))


@asset
def repository_b_asset(repository_a_asset):
    return repository_a_asset + 6


@repository
def repository_b():
    return [repository_b_asset, repository_a_source_asset]
```

Using source assets has a few advantages over having the code inside of an asset's op load the data:

- **Dagit can show asset lineage that includes the source assets**. If a different asset definition in a different repository in the same [workspace](/concepts/repositories-workspaces/workspaces) has the same asset key as a <PyObject object="SourceAsset" />, Dagit can represent the asset lineage across those repositories.
- **Dagster can use data-loading code factored into an <PyObject object="IOManager" /> to load the contents of the source asset**.
- **Asset dependencies can be written in a consistent way,** independent of whether they're downstream from a source asset or a derived asset. This makes it easy to swap out a source asset for a derived asset and vice versa.

#### Non-argument dependencies

Alternatively, you can define dependencies where data from an upstream asset doesn’t need to be loaded by Dagster to compute the output of a downstream asset. When used, `non_argument_deps` defines the dependency between assets but doesn’t pass data through Dagster.

Consider the following example:

1. `upstream_asset` creates a new table (`sugary_cereals`) by selecting records from the `cereals` table
2. `downstream_asset` then creates a new table (`shopping_list`) by selecting records from `sugary_cereals`

```python file=/concepts/assets/non_argument_deps.py startafter=start_marker endbefore=end_marker
from dagster import asset


@asset
def upstream_asset():
    execute_query("CREATE TABLE sugary_cereals AS SELECT * FROM cereals")


@asset(non_argument_deps={"upstream_asset"})
def downstream_asset():
    execute_query("CREATE TABLE shopping_list AS SELECT * FROM sugary_cereals")
```

In this example, Dagster doesn’t need to load data from `upstream_asset` to successfully compute the `downstream_asset`. While `downstream_asset` does depend on `upstream_asset`, the key difference with `non_argument_deps` is that data isn’t being passed between the functions. Specifically, the data from the `sugary_cereals` table isn't being passed as an argument to `downstream_asset`.

### Graph-backed assets

[Basic software-defined assets](#a-basic-software-defined-asset) can only produce one data artifact. If generating an asset involves multiple discrete computations, you can use graph-backed assets by separating each computation into an op and building a graph to combine your computations. This way, each discrete computation can be reused in other assets and jobs.

This approach to asset creation allows you to define a graph of ops which can produce one or multiple assets. **Note**: To use graphs to create an asset, the graph **must return an output**.

To define a graph-backed asset, use the `from_graph` attribute on the `AssetsDefinition` object:

```python file=/concepts/assets/graph_backed_asset.py startafter=start example endbefore=end example
@op(required_resource_keys={"slack"})
def fetch_files_from_slack(context) -> DataFrame:
    files = context.resources.slack.files_list(channel="#random")
    return DataFrame(
        [
            {
                "id": file.get("id"),
                "created": file.get("created"),
                "title": file.get("title"),
                "permalink": file.get("permalink"),
            }
            for file in files
        ]
    )


@op
def store_files(files):
    return files.to_sql(name="slack_files", con=create_db_connection())


@graph
def store_slack_files_in_sql():
    store_files(fetch_files_from_slack())


graph_asset = AssetsDefinition.from_graph(store_slack_files_in_sql)
```

**Note**: All output assets must be selected when using a graph-backed asset to create a job. Dagster will select all graph output automatically upon creating a job.

### Asset context

Since a software-defined asset contains an op, all the typical functionality of an op - like the use of [resources](/concepts/resources) and [configuration](#asset-configuration) - is available to an asset. Supplying the `context` parameter provides access to system information for the op, for example:

```python file=/concepts/assets/asset_w_context.py startafter=start_w_context endbefore=end_w_context
@asset(required_resource_keys={"api"})
def my_asset(context):
    # fetches contents of an asset
    return context.resources.api.fetch_table("my_asset")
```

### Asset configuration

Like ops, configuration is also supported for assets. Configuration is accessible through the asset context at runtime and can be used to specify behavior. Note that asset configuration behaves the same as configuration for ops.

For example, the following asset queries an API endpoint defined through configuration:

```python file=/concepts/assets/asset_config.py startafter=start_example endbefore=end_example
@asset(config_schema={"api_endpoint": str})
def my_configurable_asset(context):
    api_endpoint = context.op_config["api_endpoint"]
    data = requests.get(f"{api_endpoint}/data").json()
    return data
```

Refer to the [Config schema documentation](/concepts/configuration/config-schema) for more configuration info and examples.

---

## Viewing and materializing assets in Dagit

### Loading assets into Dagit

To view and materialize assets in Dagit, you can point it at a module that contains asset definitions or lists of asset definitions as module-level attributes:

    dagit -m module_with_assets

If you want Dagit to contain both assets and [jobs](/concepts/ops-jobs-graphs/jobs-graphs) that target the assets, you can place the assets and jobs together inside a [repository](/concepts/repositories-workspaces/repositories).

### Viewing assets in Dagit

- [All assets](#all-assets)
- [Details for an asset](#asset-details)
- [Dependency graph](#dependency-graph)
- [Upstream changes](#upstream-changed-indicator)

#### All assets

To view a list of all your assets, click **Assets** in the top-right corner of the page. This opens the Asset Catalog:

<img
alt="Asset Catalog"
src="/images/concepts/assets/software-defined-assets/catalog.png"
/>

#### Asset details

View the Asset Details page for an asset by clicking on its name:

<img
alt="Asset Details"
src="/images/concepts/assets/software-defined-assets/details.png"
/>

#### Dependency graph

To view a graph of all assets and their dependencies, you can:

- Click the graph icon to the upper-left of the Asset Catalog
- Click **View in Graph** on any asset

<img
alt="Asset Graph"
src="/images/concepts/assets/software-defined-assets/graph.png"
/>

#### Upstream changed indicator

On occasion, you might see an **upstream changed** indicator on an asset in the dependency graph or on the Asset Details page:

<img
alt="Asset Graph with an upstream changed indicator"
src="/images/concepts/assets/software-defined-assets/upstream-changed.png"
/>

This occurs when a downstream asset's last materialization took place **earlier than the asset it depends on.** Dagit displays this alert to notify you that the contents of an asset may be stale. For example:

- `comments` is upstream of `comment_stories`
- `comment_stories` depends on `comments`
- `comment_stories` was last materialized on February 25 at **5:30PM**
- `comments` was last materialized on February 25 at **7:05PM**

In this case, the contents of `comment_stories` may be outdated, as the most recent data from `comments` wasn't used to compute them.

You can resolve this issue by re-materializing the downstream asset. This will re-compute the contents with the most recent data/changes to its upstream dependency.

<Note>
  Currently, the <strong>upstream changed</strong> indicator won't display in
  the following scenarios:
  <ul>
    <li>The upstream asset is in another repository or job</li>
    <li>The assets are partitioned</li>
  </ul>
</Note>

### Materializing assets in Dagit

There are a couple ways in Dagit to launch a run that materializes assets:

- Navigate to the Asset Details Page for the asset and click the "Materialize" button in the upper right corner.
- In the graph view of the Asset Catalog page, click the "Materialize" button in the upper right corner. You can click on assets to collect a subset to materialize.

## Customizing how assets are materialized with IO managers

By default, materializing an asset will pickle it to a local file named "my_asset", in a temporary directory. You can specify this directory by providing a value for the `local_artifact_storage` property in your dagster.yaml file.

[IO managers](/concepts/io-management/io-managers) enable fully overriding this behavior and storing asset contents in any way you wish - e.g. writing them as tables in a database or as objects in a cloud object store. Dagster also provides built-in IO managers that pickle assets to AWS S3 (<PyObject module="dagster_aws.s3" object="s3_pickle_io_manager" />), Azure Blob Storage (<PyObject module="dagster_azure.adls2" object="adls2_pickle_io_manager" />), and GCS (<PyObject module="dagster_gcp.gcs" object="gcs_pickle_io_manager" />), or you can write your own.

To apply an IO manager to a set of assets, you can use <PyObject object="with_resources" />:

```python file=/concepts/assets/asset_io_manager.py startafter=start_marker endbefore=end_marker
from dagster_aws.s3 import s3_pickle_io_manager, s3_resource

from dagster import asset, with_resources


@asset
def upstream_asset():
    return [1, 2, 3]


@asset
def downstream_asset(upstream_asset):
    return upstream_asset + [4]


assets_with_io_manager = with_resources(
    [upstream_asset, downstream_asset],
    resource_defs={"io_manager": s3_pickle_io_manager, "s3": s3_resource},
)
```

This example also includes `"s3": s3_resource`, because the `s3_pickle_io_manager` depends on an s3 resource.

When `upstream_asset` is materialized, the value `[1, 2, 3]` will be will be pickled and stored in an object on S3. When `downstream_asset` is materialized, the value of `upstream_asset` will be read from S3 and depickled, and `[1, 2, 3, 4]` will be pickled and stored in a different object on S3.

Different assets can have different IO managers:

```python file=/concepts/assets/asset_different_io_managers.py startafter=start_marker endbefore=end_marker
from dagster_aws.s3 import s3_pickle_io_manager, s3_resource

from dagster import asset, fs_io_manager, with_resources


@asset(io_manager_key="s3_io_manager")
def upstream_asset():
    return [1, 2, 3]


@asset(io_manager_key="fs_io_manager")
def downstream_asset(upstream_asset):
    return upstream_asset + [4]


assets_with_io_managers = with_resources(
    [upstream_asset, downstream_asset],
    resource_defs={
        "s3_io_manager": s3_pickle_io_manager,
        "s3": s3_resource,
        "fs_io_manager": fs_io_manager,
    },
)
```

When `upstream_asset` is materialized, the value `[1, 2, 3]` will be will be pickled and stored in an object on S3. When `downstream_asset` is materialized, the value of `upstream_asset` will be read from S3 and depickled, and `[1, 2, 3, 4]` will be pickled and stored in a file on the local filesystem.

The same assets can be bound to different resources and IO managers in different environments. For example, for local development, you might want to store assets on your local filesystem while, in production, you might want to store the assets in S3.

```python file=/concepts/assets/asset_io_manager_prod_local.py startafter=start_marker endbefore=end_marker
from dagster_aws.s3 import s3_pickle_io_manager, s3_resource

from dagster import asset, fs_io_manager, with_resources


@asset
def upstream_asset():
    return [1, 2, 3]


@asset
def downstream_asset(upstream_asset):
    return upstream_asset + [4]


prod_assets = with_resources(
    [upstream_asset, downstream_asset],
    resource_defs={"io_manager": s3_pickle_io_manager, "s3": s3_resource},
)

local_assets = with_resources(
    [upstream_asset, downstream_asset],
    resource_defs={"io_manager": fs_io_manager},
)
```

## Building jobs that materialize assets

You can define a job that materializes a fixed selection of assets each time it runs. Multiple jobs within the same repository can target overlapping sets of assets.

```python file=/concepts/assets/build_job.py startafter=start_marker endbefore=end_marker
from dagster import asset, define_asset_job, repository


@asset
def asset1():
    return [1, 2, 3]


@asset
def asset2(asset1):
    return asset1 + [4]


all_assets_job = define_asset_job(name="all_assets_job")
asset1_job = define_asset_job(name="asset1_job", selection="asset1")


@repository
def repo():
    return [asset1, asset2, all_assets_job, asset1_job]
```

Like regular jobs, asset jobs can be placed on [schedules](/concepts/partitions-schedules-sensors/schedules) and [sensors](/concepts/partitions-schedules-sensors/sensors).

## Testing

When writing unit tests, you can treat the function decorated by `@asset` as a regular python function.

Consider a simple asset with no upstream dependencies:

```python file=/concepts/assets/asset_testing.py startafter=start_simple_asset endbefore=end_simple_asset
@asset
def my_simple_asset():
    return [1, 2, 3]
```

When writing a unit test, you can directly invoke the decorated function.

```python file=/concepts/assets/asset_testing.py startafter=start_test_simple_asset endbefore=end_test_simple_asset
def test_my_simple_asset():
    result = my_simple_asset()
    assert result == [1, 2, 3]
```

If you have an asset with upstream dependencies:

```python file=/concepts/assets/asset_testing.py startafter=start_more_complex_asset endbefore=end_more_complex_asset
@asset
def more_complex_asset(my_simple_asset):
    return my_simple_asset + [4, 5, 6]
```

You can manually provide values for those dependencies in your unit test. This allows you to test assets in isolation from one another.

```python file=/concepts/assets/asset_testing.py startafter=start_test_more_complex_asset endbefore=end_test_more_complex_asset
def test_more_complex_asset():
    result = more_complex_asset([0])
    assert result == [0, 4, 5, 6]
```

If you use a context object in your function, `@asset` will provide the correct context during execution. When writing a unit test, you can mock it with <PyObject object="build_op_context" />. You can use <PyObject object="build_op_context" /> to generate the `context` object because under the hood the function decorated by `@asset` is an op.

Consider this asset that uses a resource:

```python file=/concepts/assets/asset_testing.py startafter=start_with_context_asset endbefore=end_with_context_asset
@asset
def uses_context(context):
    return context.resources.foo
```

When writing a unit test, use <PyObject object="build_op_context" /> to mock the `context` and provide values for testing.

```python file=/concepts/assets/asset_testing.py startafter=start_test_with_context_asset endbefore=end_test_with_context_asset
def test_uses_context():
    context = build_op_context(resources={"foo": "bar"})
    result = uses_context(context)
    assert result == "bar"
```

## Examples

### Multi-component asset keys

Assets are often objects in systems with hierarchical namespaces, like filesystems. Because of this, it often makes sense for an asset key to be a list of strings, instead of just a single string. To define an asset with a multi-part asset key, use the `key_prefix` argument-- this can be either a list of strings or a single string with segments delimited by "/". The full asset key is formed by prepending the `key_prefix` to the asset name (which defaults to the name of the decorated function).

```python file=/concepts/assets/multi_component_asset_key.py startafter=start_marker endbefore=end_marker
from dagster import AssetIn, asset


@asset(key_prefix=["one", "two", "three"])
def upstream_asset():
    return [1, 2, 3]


@asset(ins={"upstream_asset": AssetIn(key_prefix="one/two/three")})
def downstream_asset(upstream_asset):
    return upstream_asset + [4]
```

### Attaching metadata

Dagster supports attaching arbitrary [metadata](/\_apidocs/ops#dagster.MetadataEntry) to assets. To attach metadata, supply a `metadata` dictionary to the asset:

```python file=/concepts/assets/asset_metadata.py startafter=start_example endbefore=end_example
@asset(metadata={"cereal_name": "Sugar Sprinkles"})
def cereal_asset():
    return 5
```

Asset metadata can be viewed in Dagit on the [Asset Detail page](/concepts/dagit/dagit#asset-details).

## Further Reading

Interested in learning more about software-defined assets and working through a more complex example? Check out our [guide on software-defined assets](/guides/dagster/software-defined-assets) and our [example project](https://github.com/dagster-io/dagster/tree/master/examples/modern_data_stack_assets) that integrates software-defined assets with other Modern Data Stack tools.

## See it in action

For more examples of software-defined assets, check out the following in our [SDA Hacker News example](https://github.com/dagster-io/dagster/tree/master/examples/hacker_news_assets):

- [Defining an asset](https://github.com/dagster-io/dagster/blob/master/examples/hacker_news_assets/hacker_news_assets/activity_analytics/assets/activity_forecast.py)
- [Loading assets from dbt](https://github.com/dagster-io/dagster/blob/master/examples/hacker_news_assets/hacker_news_assets/activity_analytics/\__init\_\_.py)
- [Per-asset IO manager](https://github.com/dagster-io/dagster/blob/master/examples/hacker_news_assets/hacker_news_assets/core/assets/items.py)
- [Partitioned assets](https://github.com/dagster-io/dagster/blob/master/examples/hacker_news_assets/hacker_news_assets/core/assets/items.py)

Our [Modern Data Stack example](https://github.com/dagster-io/dagster/tree/master/examples/modern_data_stack_assets) also covers:

- [Defining assets](https://github.com/dagster-io/dagster/blob/master/examples/modern_data_stack_assets/modern_data_stack_assets/assets.py)
- [Loading assets from dbt](https://github.com/dagster-io/dagster/blob/master/examples/modern_data_stack_assets/modern_data_stack_assets/assets.py)
- [Loading assets from Airbyte](https://github.com/dagster-io/dagster/blob/master/examples/modern_data_stack_assets/modern_data_stack_assets/assets.py)

Our [Bollinger example](https://github.com/dagster-io/dagster/tree/master/examples/bollinger) also covers software-defined assets
