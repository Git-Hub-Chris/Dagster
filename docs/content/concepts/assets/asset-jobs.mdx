---
title: Asset jobs | Dagster
description: Asset jobs are the main unit for materializing and monitoring Software-defined assets in Dagster.
---

# Asset jobs

Jobs are the main unit for executing and monitoring Software-defined assets in Dagster. An asset job materializes a selection of [Software-defined Assets](/concepts/assets/software-defined-assets). It can be launched in a few different ways:

- Manually from the Dagster UI
- At fixed intervals, by [schedules](/concepts/partitions-schedules-sensors/schedules)
- When external changes occur, using [sensors](/concepts/partitions-schedules-sensors/sensors)

<!-- TODO -->

<Note>
  Looking for executeing a <a href="/concepts/ops-jobs-graphs/graphs">graph</a>{" "}
  of <a href="/concepts/ops-jobs-graphs/ops">ops</a>, which are not tied to
  Software-defined Assets? Visit the{" "}
  <a href="/concepts/ops-jobs-graphs/jobs">Op jobs</a> page.
</Note>

---

## Relevant APIs

| Name                                   | Description                                               |
| -------------------------------------- | --------------------------------------------------------- |
| <PyObject object="define_asset_job" /> | A function for defining a job from a selection of assets. |

---

## Creating asset jobs

Asset jobs materialize a fixed set of assets each time they run. Additionally, multiple jobs can target overlapping sets of assets:

```python file=/concepts/assets/build_job.py startafter=start_marker endbefore=end_marker
from dagster import Definitions, asset, define_asset_job


@asset
def asset1():
    return [1, 2, 3]


@asset
def asset2(asset1):
    return asset1 + [4]


all_assets_job = define_asset_job(name="all_assets_job")
asset1_job = define_asset_job(name="asset1_job", selection="asset1")


defs = Definitions(
    assets=[asset1, asset2],
    jobs=[all_assets_job, asset1_job],
)
```

Unlike jobs created using the <PyObject object="job" decorator /> decorator where you explicitly define the dependencies when you create the job, the topology of an asset-based job is based on the [assets](/concepts/assets/software-defined-assets) and their dependencies.

<!-- ## Configuring jobs

Ops, software-defined assets, and resources often accept [configuration](/concepts/configuration/config-schema) that determines how they behave. By default, you supply configuration for these ops and resources at the time you launch the job.

When constructing a job, you can customize how that configuration will be satisfied, by passing a value to the `config` parameter of the <PyObject object="GraphDefinition" method="to_job" /> method or the <PyObject object="job" decorator /> decorator. The options are discussed below:

- [Hardcoded configuration](#hardcoded-configuration)
- [Partitioned configuration](#partitioned-configuration)
- [Config mapping](#config-mapping)

### Hardcoded configuration

You can supply a <PyObject object="RunConfig"/> object or raw config dictionary. The supplied config will be used to configure the job whenever the job is launched. It will show up in the UI Launchpad and can be overridden.

```python file=/concepts/ops_jobs_graphs/jobs_with_default_config.py
from dagster import Config, RunConfig, job, op


class DoSomethingConfig(Config):
    config_param: str


@op
def do_something(context, config: DoSomethingConfig):
    context.log.info("config_param: " + config.config_param)


default_config = RunConfig(
    ops={"do_something": DoSomethingConfig(config_param="stuff")}
)


@job(config=default_config)
def do_it_all_with_default_config():
    do_something()


if __name__ == "__main__":
    # Will log "config_param: stuff"
    do_it_all_with_default_config.execute_in_process()
```

### Partitioned configuration

For op-based jobs, you can supply a <PyObject object="PartitionedConfig" /> to create a partitioned job. This defines a discrete set of partitions along with a function for generating config for a partition. Job runs can be configured by selecting a partition.

Refer to the [Partitions documentation](/concepts/partitions-schedules-sensors/partitions) for more info and examples.

### Config mapping

You can supply a <PyObject object="ConfigMapping" />. This allows you to expose a narrower config interface to your job. Instead of needing to configure every op and resource individually when launching the job, you can supply a smaller number of values to the outer config, and the <PyObject object="ConfigMapping" /> can translate it into config for all the job's ops and resources.

```python file=/concepts/ops_jobs_graphs/jobs_with_config_mapping.py
from dagster import Config, RunConfig, config_mapping, job, op


class DoSomethingConfig(Config):
    config_param: str


@op
def do_something(context, config: DoSomethingConfig) -> None:
    context.log.info("config_param: " + config.config_param)


class SimplifiedConfig(Config):
    simplified_param: str


@config_mapping
def simplified_config(val: SimplifiedConfig) -> RunConfig:
    return RunConfig(
        ops={"do_something": DoSomethingConfig(config_param=val.simplified_param)}
    )


@job(config=simplified_config)
def do_it_all_with_simplified_config():
    do_something()


if __name__ == "__main__":
    # Will log "config_param: stuff"
    do_it_all_with_simplified_config.execute_in_process(
        run_config={"simplified_param": "stuff"}
    )
``` -->

---

## Making jobs available to Dagster tools

You make jobs available to the UI, GraphQL, and the command line by including them in <PyObject object="Definitions"/> object at the top-level of Python module or file. The tool loads that module as a code location. If you include schedules or sensors, the code location will automatically include jobs that those schedules or sensors target.

```python file=/concepts/ops_jobs_graphs/repo_with_job.py
from dagster import Definitions, job


@job
def do_it_all():
    ...


defs = Definitions(jobs=[do_it_all])
```

---

## Testing jobs

Dagster has built-in support for testing, including separating business logic from environments and setting explicit expectations on uncontrollable inputs. Refer to the [Testing guide](/concepts/testing) for more info and examples.

---

## Executing jobs

You can run a job in a variety of ways:

- In the Python process where it's defined
- Via the command line
- Via the GraphQL API
- In [the UI](/concepts/webserver/ui). The UI centers on jobs, making it a one-stop-shop - you can manually kick off runs for a job and view all historical runs.

Refer to the [Job execution guide](/concepts/ops-jobs-graphs/job-execution) for more info and examples.

---

## See it in action

For more examples of jobs, check out the following in our [Hacker News example](https://github.com/dagster-io/dagster/tree/master/examples/project_fully_featured):

- [Building a job that targets all the assets in a group](https://github.com/dagster-io/dagster/blob/master/examples/project_fully_featured/project_fully_featured/jobs.py)
