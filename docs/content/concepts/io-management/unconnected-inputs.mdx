---
title: Unconnected Inputs | Dagster
description:
---

# Unconnected Inputs

## Relevant APIs

| Name                                                                 | Description                                                                 |
| -------------------------------------------------------------------- | --------------------------------------------------------------------------- |
| <PyObject module="dagster" object="dagster_type_loader" decorator /> | The decorator used to define a Dagster Type Loader.                         |
| <PyObject module="dagster" object="DagsterTypeLoader" />             | The base class used to specify how to load inputs that depends on the type. |

## Overview

Ops in a job may have input definitions that don't correspond to the outputs of upstream ops. You can provide values for these inputs in a few different ways. Dagster checks each, in order, and uses the first that's available:

- **Input Manager** - If the input to a job comes from an external source, such as a table in a database, often it makes sense to define a resource that's responsible for loading it. This makes it easy to swap out implementations in different jobs and mock it in tests. A special IOManager, which can be referenced from <PyObject module="dagster" object="In" />s, can be used to loads unconnected inputs.
- **Dagster Type Loader** - A <PyObject module="dagster" object="DagsterTypeLoader" /> provides a way to specify how to load inputs that depends on a type. A <PyObject module="dagster" object="DagsterTypeLoader" /> can be placed on <PyObject module="dagster" object="DagsterType" />, which can be placed on <PyObject module="dagster" object="In" />.
- **Default Values** - <PyObject module="dagster" object="In" /> accepts a `default_value` argument.

## Examples

### Loading a built-in dagster type from config

When you have an op at the beginning of a job that operates on a built-in dagster type like string or int, you can provide a value for that input via run config.

Here's a basic job with an unconnected string input:

```python file=/concepts/io_management/load_from_config.py startafter=def_start_marker endbefore=def_end_marker
@op(ins={"input_string": In(String)})
def my_op(context, input_string):
    context.log.info(f"input string: {input_string}")


@job
def my_job():
    my_op()
```

The <PyObject module="dagster" object="String" /> dagster type has a dagster type loader that allows it to load inputs from config:

```python file=/concepts/io_management/load_from_config.py startafter=execute_start_marker endbefore=execute_end_marker
my_job.execute_in_process(
        run_config={"ops": {"my_op": {"inputs": {"input_string": {"value": "marmot"}}}}}
    )
```

### Loading a custom dagster type from config

When you have an op at the beginning of your job that operates on a dagster type that you've defined, you can write your own <PyObject module="dagster" object="DagsterTypeLoader" /> to define how to load that input via run config.

```python file=/concepts/io_management/load_custom_type_from_config.py startafter=def_start_marker endbefore=def_end_marker
@dagster_type_loader(
    config_schema={"diameter": float, "juiciness": float, "cultivar": str}
)
def apple_loader(_context, config):
    return Apple(
        diameter=config["diameter"],
        juiciness=config["juiciness"],
        cultivar=config["cultivar"],
    )


@usable_as_dagster_type(loader=apple_loader)
class Apple:
    def __init__(self, diameter, juiciness, cultivar):
        self.diameter = diameter
        self.juiciness = juiciness
        self.cultivar = cultivar


@op(ins={"input_apple": In(Apple)})
def my_op(context, input_apple):
    context.log.info(f"input apple diameter: {input_apple.diameter}")


@job
def my_job():
    my_op()
```

With this, the input can be specified via config as below:

```python file=/concepts/io_management/load_custom_type_from_config.py startafter=execute_start_marker endbefore=execute_end_marker
my_job.execute_in_process(
        run_config={
            "ops": {
                "my_op": {
                    "inputs": {
                        "input_apple": {
                            "diameter": 2.4,
                            "juiciness": 6.0,
                            "cultivar": "honeycrisp",
                        }
                    }
                }
            }
        },
    )
```

### Providing an input manager for an unconnected input

When you have an op at the beginning of a job that operates on data from an external source, you might wish to separate that I/O from your op's business logic, in the same way you would with an IO manager if the op were loading from an upstream output.

To accomplish this, you can define an <PyObject module="dagster" object="IOManager" /> with special logic for loading this input.

```python file=/concepts/io_management/input_managers.py startafter=start_load_unconnected_input endbefore=end_load_unconnected_input
class Table1IOManager(IOManager):
    def load_input(self, context):
        return read_dataframe_from_table(name="table1")

    def handle_output(self, context, obj):
        # if you wish to have custom output handling logic you can put it here and also use
        # this IO manager like any other IO Manager
        pass


@io_manager
def table_1_manager():
    return Table1IOManager()


@op(ins={"dataframe": In(input_manager_key="load_input_manager")})
def my_op(dataframe):
    """Do some stuff"""
    dataframe.head()


@job(resource_defs={"load_input_manager": table_1_manager})
def load_table_job():
    my_op()
```

Setting the `input_manager_key` on an `In` controls how that input is loaded.

### Providing per-input config to an input manager

When launching a run, you might want to parameterize how particular inputs are loaded.

To accomplish this, you can define an `input_config_schema` on the IO manager definition. The `load_input` function can access this config when storing or loading data, via the <PyObject module="dagster" object="InputContext" />.

```python file=/concepts/io_management/input_managers.py startafter=start_per_input_config endbefore=end_per_input_config
class MyConfigurableInputLoader(MyIOManager):
    def load_input(self, context):
        return read_dataframe_from_table(name=context.config["table"])


@io_manager(input_config_schema={"table": str})
def my_configurable_input_loader():
    return MyConfigurableInputLoader()
```

Then, when executing a job, you can pass in this per-input config.

```python file=/concepts/io_management/input_managers.py startafter=start_per_input_config_exec endbefore=end_per_input_config_exec
load_table_job.execute_in_process(
    run_config={"ops": {"my_op": {"inputs": {"dataframe": {"table": "table1"}}}}},
)
```

### Using a input manager with subselection

You might want to execute a subset of ops in your job and control how the inputs of those ops are loaded. Custom input managers also help in these situations, because the inputs at the beginning of the subset become unconnected inputs.

For example, you might have `op1` that normally produces a table that `op2` consumes. To debug `op2`, you might want to run it on a different table than the one normally produced by `op1`.

To accomplish this, you can set up the `input_manager_key` on `op2`'s `In` to point to an input manager with the desired loading behavior. As in the previous example, setting the `input_manager_key` on an `In` controls how that input is loaded and you can write custom loading logic. To ensure that the outputs of `op2` get stored as expected, you can write your new input manager as a subclass of the main IO manager.

```python file=/concepts/io_management/input_managers.py startafter=start_load_input_subset endbefore=end_load_input_subset
class MyIOManager(IOManager):
    def handle_output(self, context, obj):
        table_name = context.name
        write_dataframe_to_table(name=table_name, dataframe=obj)

    def load_input(self, context):
        return read_dataframe_from_table(name=context.upstream_output.name)


@io_manager
def my_io_manager(_):
    return MyIOManager()


class MyInputLoader(MyIOManager):
    def load_input(self, context):
        return read_dataframe_from_table(name="table_1")


@io_manager
def my_input_loader():
    return MyInputLoader()


@op
def op1():
    """Do stuff"""


@op(ins={"dataframe": In(input_manager_key="my_input_manager")})
def op2(dataframe):
    """Do stuff"""
    dataframe.head()


@job(
    resource_defs={
        "io_manager": my_io_manager,
        "my_input_manager": my_input_loader,
    }
)
def my_subselection_job():
    op2(op1())
```

So far, this is set up so that `op2` always loads `table_1` even if you execute the full job. To write this so that `op2` only loads `table_1` when no input is provided from an upstream op, we can rewrite the input manager as follows:

```python file=/concepts/io_management/input_managers.py startafter=start_better_load_input_subset endbefore=end_better_load_input_subset
class MyNewInputLoader(MyIOManager):
    def load_input(self, context):
        if context.upstream_output is None:
            return read_dataframe_from_table(name="table_1")
        else:
            return super().load_input(context)
```

Now, when running the full job, `op2`'s input will be loaded using the IO manager on the output of `op1`. When running the job subset, `op2`'s input has no upstream output, so `table_1` will be loaded.

```python file=/concepts/io_management/input_managers.py startafter=start_execute_subselection endbefore=end_execute_subselection
my_subselection_job.execute_in_process(
    op_selection=["op2"],
)
```

## Root Input Managers

Root input managers will be deprecated in 0.16.0. This documentation will remain as a reference for those with existing root input managers, but moving forward, all users should prefer to write input managers and specify them via `input_manager_key`.

### Providing an input manager for a root input <Experimental />

When you have an op at the beginning of a job that operates on data from external source, you might wish to separate that I/O from your op's business logic, in the same way you would with an IO manager if the op were loading from an upstream output.

To accomplish this, you can define an <PyObject module="dagster" object="RootInputManager" />.

```python file=/concepts/io_management/root_input_manager.py startafter=start_marker endbefore=end_marker
@op(ins={"dataframe": In(root_manager_key="my_root_manager")})
def my_op(dataframe):
    """Do some stuff"""


@root_input_manager
def table1_loader(_):
    return read_dataframe_from_table(name="table1")


@job(resource_defs={"my_root_manager": table1_loader})
def my_job():
    my_op()
```

Setting the `root_manager_key` on an `In` controls how that input is loaded in jobs where it has no upstream output.

The <PyObject module="dagster" object="root_input_manager" /> decorator behaves nearly identically to the <PyObject module="dagster" object="resource" /> decorator. It yields an <PyObject module="dagster" object="RootInputManagerDefinition" />, which is a <PyObject module="dagster" object="ResourceDefinition" /> that will produce an <PyObject module="dagster" object="RootInputManager" />.

### Providing per-input config to a root input manager <Experimental />

When launching a run, you might want to parameterize how particular root inputs are loaded.

To accomplish this, you can define an `input_config_schema` on the input manager definition. The load function can access this config when storing or loading data, via the <PyObject module="dagster" object="InputContext" />.

```python file=/concepts/io_management/config_input_manager.py startafter=def_start_marker endbefore=def_end_marker
@root_input_manager(input_config_schema={"table_name": str})
def table_loader(context):
    return read_dataframe_from_table(name=context.config["table_name"])
```

Then, when executing a job, you can pass in this per-input config.

```python file=/concepts/io_management/config_input_manager.py startafter=execute_start_marker endbefore=execute_end_marker
@job(resource_defs={"my_root_manager": table_loader})
    def my_job():
        my_op()

    my_job.execute_in_process(
        run_config={
            "ops": {"my_op": {"inputs": {"dataframe": {"table_name": "table1"}}}}
        },
    )
```

### Using a root input manager with subselection <Experimental />

You might want to execute a subset of ops in your job and control how the inputs of those ops are loaded. Root input managers also help in these situations, because the inputs at the beginning of the subset become the new "roots".

For example, you might have `op1` that normally produces a table that `op2` consumes. To debug `op2`, you might want to run it on a different table than the one normally produced by `op1`.

To accomplish this, you can set up the `root_manager_key` on `op2`'s `In` to point to an input manager with the desired loading behavior. As before, setting the `root_manager_key` on an `In` controls how that input is loaded when it has no upstream output.

```python file=/concepts/io_management/subselection.py startafter=start_marker endbefore=end_marker
@root_input_manager(input_config_schema={"table_name": str})
def my_root_input_manager(context):
    return read_dataframe_from_table(name=context.config["table_name"])


class MyIOManager(IOManager):
    def handle_output(self, context, obj):
        table_name = context.name
        write_dataframe_to_table(name=table_name, dataframe=obj)

    def load_input(self, context):
        return read_dataframe_from_table(name=context.upstream_output.name)


@io_manager
def my_io_manager(_):
    return MyIOManager()


@op(out=Out(io_manager_key="my_io_manager"))
def op1():
    """Do stuff"""


@op(ins={"dataframe": In(root_manager_key="my_root_input_manager")})
def op2(dataframe):
    """Do stuff"""


@job(
    resource_defs={
        "my_io_manager": my_io_manager,
        "my_root_input_manager": my_root_input_manager,
    }
)
def my_job():
    op2(op1())
```

When running the full job, `op2`'s input will be loaded using the IO manager on the output of `op1`. When running the job subset, `op2`'s input has no upstream output, so the input manager corresponding to its `root_manager_key` is used.

```python file=/concepts/io_management/subselection.py startafter=start_execute_subselection endbefore=end_execute_subselection
my_job.execute_in_process(
        run_config={
            "ops": {"op2": {"inputs": {"dataframe": {"table_name": "tableX"}}}}
        },
        op_selection=["op2"],
    )
```
