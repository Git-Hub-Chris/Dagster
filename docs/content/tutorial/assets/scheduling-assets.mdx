---
title: Building Graphs of Assets | Dagster
description: Software-defined assets can depend on other software-defined assets
---

# Scheduling Assets

Orchestrators help you build the right data assets in the right order and at the right time. Dagster assets accomplish this goal by allowing you to define _what_ you want to exist instead of specifying step by step _how_ the assets should be built. The asset tutorial so far has addressed building the right assets in the right order. This section of the tutorial covers the final piece: building assets at the right time. While it is possible to specify when things should run step by step using traditional [cron-based schedules](/concepts/partitions-schedules-sensors/schedules), we recommend trying freshness policies instead. Freshness policies allow you to declare SLAs for the assets you deliver to stakeholders and Dagster determines when everything else needs to run to meet those SLAs. To understand freshness policies it helps to run a few experiments.

---

## Step 1: Cron Scheduling

<Note>

To run each code snippet, follow [these instructions](https://docs.dagster.io/getting-started/hello-dagster).

</Note>

Begin with a regular schedule:

```python file=/intro_tutorial/scheduling/freshness_1.py
from dagster import (
    AssetSelection,
    ScheduleDefinition,
    asset,
    define_asset_job,
    repository,
)


@asset
def a():
    pass


@asset
def b(a):
    pass


update_job = define_asset_job(
    name="update_job", selection=AssetSelection.keys("a", "b")
)

update_job_schedule = ScheduleDefinition(
    name="update_job_schedule", job=update_job, cron_schedule="* * * * *"
)


@repository
def my_repo():
    return [[a, b], [update_job_schedule]]
```

This schedule runs a job every minute called `update_job` that materialiazes asset `a` and then asset `b`. This setup represents a traditional cron based schedule. To ensure asset `b` is updated with fresh data, you tell the orchestrator to run a job targetting asset `a` and `b`, and Dagster knows to run `a` before `b` because `a` is an input to the asset function `b`.

<Image
src="/images/tutorial/assets/scheduling-assets/freshness_1.png"
width={2060}
height={1456}
/>

---

## Step 2: Introducing a Reconciliation Sensor

Now take a step towards a more declarative approach to scheduling that describes _what_ assets you want to exist.

```python file=/intro_tutorial/scheduling/freshness_2.py
from dagster import (
    AssetSelection,
    ScheduleDefinition,
    asset,
    build_asset_reconciliation_sensor,
    define_asset_job,
    repository,
)


@asset
def a():
    pass


@asset
def b(a):
    pass


update_job = define_asset_job(name="update_job", selection=AssetSelection.keys("a"))

update_sensor = build_asset_reconciliation_sensor(
    name="update_sensor", asset_selection=AssetSelection.all()
)

update_job_schedule = ScheduleDefinition(
    name="update_job_schedule", job=update_job, cron_schedule="* * * * *"
)


@repository
def my_repo():
    return [[a, b], [update_job_schedule], [update_sensor]]
```

This example adds a reconciliation sensor called `update_sensor` and modifies the scheduled job to only target asset `a`. The result is that whenever asset `a` is updated by the scheduled job, asset `b` is marked as stale. The reconciliation sensor keeps track of all of the assets, and automatically determines that `b` needs to be updated when `b` is marked stale. A run is started to materialize asset `b`. This approach is more declarative, you are stating that "`b` should be as up-to-date as possible" and Dagster figures out when `b` needs to run.

<Image
src="/images/tutorial/assets/scheduling-assets/freshness_2.png"
width={960}
height={540}
/>

---

## Step 3: Introducing Freshness Policies

Now introduce another asset, `c`. What if you don't need `c` to be as up-to-date as `a` and `b`? In traditional cron based schedules this requirement quickly becomes confusing. Shoud the scheduled job target `c` and try to re-use the last value of `a`? Should the scheduled job instead run `a` and `c`? Does scheduling `c` create any side-effects that will impact `b`? In Dagster you can avoid all of these questions and instead declare how fresh you want `c` to be, and Dagster figures out the rest. This declaration is done through a freshness policy:

```python file=/intro_tutorial/scheduling/freshness_3.py
from dagster import (
    AssetSelection,
    FreshnessPolicy,
    ScheduleDefinition,
    asset,
    build_asset_reconciliation_sensor,
    define_asset_job,
    repository,
)


@asset
def a():
    pass


@asset
def b(a):
    pass


@asset(freshness_policy=FreshnessPolicy(maximum_lag_minutes=2))
def c(a):
    pass


update_job = define_asset_job(name="update_job", selection=AssetSelection.keys("a"))

update_sensor = build_asset_reconciliation_sensor(
    name="update_sensor", asset_selection=AssetSelection.all()
)

update_job_schedule = ScheduleDefinition(
    name="update_job_schedule", job=update_job, cron_schedule="* * * * *"
)


@repository
def my_repo():
    return [[a, b, c], [update_job_schedule], [update_sensor]]
```

One way to think about a freshness policy is that it adds a tolerance to the reconciliation sensor. When `a` is updated, the reconciliation sensor immediately knows that `b` is stale and then creates a run to refresh `b`. The freshness policy tells the reconciliation sensor that `c` can tolerate being stale for up to 2 minutes. Instead of creating a run to update `c` immediately, the reconciliation sensor will wait until `c` is more than 2 minutes stale and then will create a run to update `c`.

1. First `a` is updated by the schedule. `c` is marked stale but is not violating the freshness policy:

<Image
src="/images/tutorial/assets/scheduling-assets/freshness_3a.png"
width={2466}
height={1278}
/>

2. After 2 minutes, `c` is marked late because the freshness policy is violated. A run is started to update `c`.

<Image
src="/images/tutorial/assets/scheduling-assets/freshness_3b.png"
width={2326}
height={1236}
/>

3. Once the run completes `c` is both on-time and fresh.

<Image
src="/images/tutorial/assets/scheduling-assets/freshness_3c.png"
width={2336}
height={1228}
/>

---

## Step 4: Removing the cron Schedule

So far, a schedule still updates `a`. The final step is to remove this schedule:

```python file=/intro_tutorial/scheduling/freshness_4a.py
from dagster import (
    AssetSelection,
    FreshnessPolicy,
    asset,
    build_asset_reconciliation_sensor,
    repository,
)


@asset
def a():
    pass


@asset
def b(a):
    pass


@asset(freshness_policy=FreshnessPolicy(maximum_lag_minutes=2))
def c(a):
    pass


update_sensor = build_asset_reconciliation_sensor(
    name="update_sensor", asset_selection=AssetSelection.all()
)


@repository
def my_repo():
    return [[a, b, c], [update_sensor]]
```

Here is where the reconciliation sensor and freshness policies become really powerful. Dagster will determine that after 2 minutes asset `c` is late and violating its freshness policy. Dagster _will also determine_ that in order for `c` to be fresh, asset `a` needs to be updated as well. Dagster will create a run to update both `a` and `c`.

1. `c` is late because it was last updated more than 2 minutes ago thus violating the freshness policy

<Image
src="/images/tutorial/assets/scheduling-assets/freshness_4a.png"
width={2404}
height={1224}
/>

2. A run is triggered that updates `a` and `c`

<Image
src="/images/tutorial/assets/scheduling-assets/freshness_4b.png"
width={2406}
height={1182}
/>

3. `a` and `c` are both updated. Asset `b` is now stale and will be updated based on it's policy.

<Image
src="/images/tutorial/assets/scheduling-assets/freshness_4c.png"
width={2444}
height={1228}
/>

In the current code asset `b` has no policy, but `b` is monitored by the reconciliation sensor. As a result, as soon as `a` is updated, asset `b` will be marked as stale and then a run will be started to update `b`. This immediate update may not be desirable, luckily adding a freshness policy to `b` is easy:

```python file=/intro_tutorial/scheduling/freshness_4b.py
from dagster import (
    AssetSelection,
    FreshnessPolicy,
    asset,
    build_asset_reconciliation_sensor,
    repository,
)


@asset
def a():
    pass


@asset(freshness_policy=FreshnessPolicy(maximum_lag_minutes=5))
def b(a):
    pass


@asset(freshness_policy=FreshnessPolicy(maximum_lag_minutes=2))
def c(a):
    pass


update_sensor = build_asset_reconciliation_sensor(
    name="update_sensor", asset_selection=AssetSelection.all()
)


@repository
def my_repo():
    return [[a, b, c], [update_sensor]]
```

When multiple freshness policies exist Dagster figures out the minimal amount of work needed to meet all of the policies. In this example, `a` is refreshed every 2 minutes by `c`, so `b` can be refreshed without re-running `a` again. In contrast, a simple cron scheduler would redundantly run `a` for each run of `b` and `c`. Freshness policies reduce the work done by the scheduler!

The data assets are now fully declarative. You tell Dagster how fresh `c` should be and Dagster does the rest. Asset `a` is updated when it needs to be, not any more or less frequently. This approach to scheduling simplifies how data pipelines are built, and it helps data engineers meet the needs of their stakeholders. Freshness policies can map to data SLAs. An executive dashboard with KPIs might have a strict SLA and freshness policy with a low lag time, whereas retraining a ML model may accept a greater lag.

---

## Step 5: Code Changes

One final concept is important when considering how assets become stale. So far this tutorial has focused on time passing and assets becoming stale because new data is available. Assets can also become stale if their _definition_ changes because you have updated your code.

In Dagster it is possible to indicate that an asset is stale by updating it's `op_version`. Existing code in production might be labelled with version 0.1:

```python file=/intro_tutorial/scheduling/freshness_5a.py
from dagster import AssetSelection, asset, build_asset_reconciliation_sensor, repository


@asset
def a():
    pass


@asset(op_version="0.1")
def b(a):
    pass


@asset
def c(b):
    pass


update_sensor = build_asset_reconciliation_sensor(
    name="update_sensor", asset_selection=AssetSelection.all()
)


@repository
def my_repo():
    return [[a, b, c], [update_sensor]]
```

These assets would be managed by the reconciliation scheduler and be considered fresh when all three have been materialized:

<Image
src="/images/tutorial/assets/scheduling-assets/freshness_5a.png"
width={2084}
height={1362}
/>

If you make a substantial change to our code, you can increment the `op_version`:

```python file=/intro_tutorial/scheduling/freshness_5b.py
from dagster import AssetSelection, asset, build_asset_reconciliation_sensor, repository


@asset
def a():
    pass


@asset(op_version="0.2")
def b(a):
    return "significant change"


@asset
def c(b):
    pass


update_sensor = build_asset_reconciliation_sensor(
    name="update_sensor", asset_selection=AssetSelection.all()
)


@repository
def my_repo():
    return [[a, b, c], [update_sensor]]
```

When the new asset definitions are loaded, `b` and the downstream asset `c` will be flagged as stale.

<Image
src="/images/tutorial/assets/scheduling-assets/freshness_5b.png"
width={2208}
height={1404}
/>

In testing environments the stale assets can be manually materialized to verify the code change.

<Image
src="/images/tutorial/assets/scheduling-assets/freshness_5c.png"
width={1520}
height={1158}
/>

In production, a reconciliation sensor will launch runs to refresh the stale assets taking into account any of their freshness policies.
