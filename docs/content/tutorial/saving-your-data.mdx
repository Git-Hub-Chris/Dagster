---
title: "Tutorial, part six: Saving your data | Dagster Docs"
description: Learn how to use I/O managers to save your data.
---

# Tutorial, part six: Using Dagster to save and load your data

You've written a data pipeline using assets that runs every hour. Currently, you are manually storing and loading your data using the `json` and `pandas` libraries at the beginning and end of each asset. Dagster provides I/O managers that abstract away reading and writing data to storage. I/O managers allow you to reduce boilerplate reading and writing code and easily change where your data is stored.

By the end of this section, you will:

- Store your files in a file system using an I/O manager
- Store your data frames as tables in a database
- Learn about I/O managers
- Understand when to use I/O managers

---

## What are I/O managers?

I/O stands for **input** and **output.** I/O managers are Dagster objects that control how Dagster reads and writes data to specific external services, such as Snowflake or Amazon Web Services (AWS) S3.

They manage **input** by reading an asset from where it’s stored and loading it into memory to be used by a dependent asset. For example, `most_frequent_words` needs `topstories`. Therefore, you could use an I/O manager to load the `topstories` data into memory so that the `most_frequent_words` asset can use it.

I/O managers control **output** by writing the asset's to the location configured. By the end of this section, you’ll configure an I/O manager to write the `topstories` DataFrame into a database as a table.

---

## When should you use I/O managers?

### TODO WRITE THIS SECTION

Points to hit:

- I/O managers are useful when you need access to the contents of an upstream asset in memory
- When your assets can all be stored in a consistent way (ie in the same schema in a database)
- I/O managers are not useful if you are simply calling out to external services, running SQL queries

---

## Step 1: Writing files to storage

Now, you'll use the and I/O to read and write your data. For this tutorial, you’ll be writing to a directory in your file system by using the `FilesystemIOManager`, which is bundled with the core `dagster` package.

In `__init__.py`, add the following snippet anywhere above the `def = Definitions(...)` line:

```python file=/tutorial/saving/add_fs_io_manager.py startafter=start_imports_and_definitions endbefore=end_imports_and_definitions
from dagster import (
    AssetSelection,
    Definitions,
    ScheduleDefinition,
    define_asset_job,
    load_assets_from_modules,
    FilesystemIOManager,  # Update the imports at the top of the file to also include this
)

hackernews_job = define_asset_job("hackernews_job", selection=AssetSelection.all())

hackernews_schedule = ScheduleDefinition(
    job=hackernews_job, cron_schedule="0 * * * *"  # every hour
)

io_manager = FilesystemIOManager(
    base_dir="data",  # Path is built relative to where `dagster dev` is run
)
```

Next, update your `Definitions` (shown below) to attach the I/O manager to your code location by defining it as a **resource**.

```python file=/tutorial/saving/add_fs_io_manager.py startafter=start_update_defs endbefore=end_update_defs
defs = Definitions(
    assets=all_assets,
    schedules=[hackernews_schedule],
    resources={
        "io_manager": io_manager,
    },
)
```

<Note>
  <strong>About resources</strong>
  <br />
  I/O managers are a special type of resource, a Dagster concept for{" "}
  <strong>connecting</strong> to external services. I/O managers are resources
  made specifically to control reading and writing to external services. Refer
  to the <a href="/concepts/resources">Resources documentation</a> for more
  information.
</Note>

Now, you will modify your three assets `topstory_ids`, `topstories`, and `most_frequent_words` to use the I/O manager to read and write data rather than direct calls to the `json` and `pandas` libraries. To do this, you will need to modify how the dependencies between assets are defined, and remove manual reading and writing of data.

When using an I/O manager, rather than manually save your data, you return it and the I/O manager will write it to storage. To define dependencies between assets you use upstream asset name as the name of one of the arguments on the decorated function. The I/O manager will then load the data associated with that asset into memory.

Modify `topstory_ids` to use an I/O manager:

```python file=/tutorial/saving/update_asset_to_use_io.py startafter=start_topstory_ids_asset endbefore=end_topstory_ids_asset
ENOENT: no such file or directory, open '/Users/jamie/dev/dagster/examples/docs_snippets/docs_snippets/tutorial/saving/update_asset_to_use_io.py'
```

Modify `topstories` to use an I/O manager:

```python file=/tutorial/saving/update_asset_to_use_io.py startafter=start_topstories_asset endbefore=end_topstories_asset
ENOENT: no such file or directory, open '/Users/jamie/dev/dagster/examples/docs_snippets/docs_snippets/tutorial/saving/update_asset_to_use_io.py'
```

Modify `most_frequent_words` to use an I/O manager:

```python file=/tutorial/saving/update_asset_to_use_io.py startafter=start_most_frequent_words_asset endbefore=end_most_frequent_words_asset
ENOENT: no such file or directory, open '/Users/jamie/dev/dagster/examples/docs_snippets/docs_snippets/tutorial/saving/update_asset_to_use_io.py'
```

To see the effect of this change, reload your code location in the Dagster UI and materialize your assets. When you click on an asset, you’ll see the `path` metadata pointing to the `data` directory on your computer.

### TODO - some kind of closing statement that make sense

---

## Step 2: Writing to databases

In data engineering, a common asset is tabular data. These are often seen as database tables or DataFrames in-memory. In Dagster, I/O managers define the relationship between how data is saved as a table in the database and what in-memory data format to represent the data as.

Dagster includes out-of-the-box I/O managers for common databases, such as DuckDB. When a DataFrame is returned in an asset’s definition, it is translated into a database-compatible format. The I/O manager then runs a SQL query to write the data into the database.

Let’s modify our pipeline to store our `topstories` DataFrame as a table in DuckDB. We’ll use Dagster’s out-of-the-box I/O manager for DuckDB, an easy-to-set-up database that runs on your computer without any setup on your end.

### Setting up I/O managers

Adding an I/O manager for a database is similar to connecting it to file storage. The biggest difference is you must specify what **type** of DataFrame to use when the data is loaded into Dagster’s memory. In this tutorial, you’ll use DuckDB to store the data and Pandas DataFrames when transforming with the data.

Update your `__init__.py` to reflect the changes below:

```python file=/tutorial/saving/add_db_io_manager.py startafter=start_imports_and_definitions endbefore=end_imports_and_definitions
from dagster_duckdb_pandas import DuckDBPandasIOManager

# Add the imports to the top
# These imports let you define how Dagster communicates with DuckDB

# Insert this section anywhere above your `defs = Definitions(...)`
database_io_manager = DuckDBPandasIOManager(database="analytics.hackernews")

# Update your Definitions
defs = Definitions(
    assets=all_assets,
    schedules=[hackernews_schedule],
    resources={
        "io_manager": io_manager,
        "database_io_manager": database_io_manager,  # Define the I/O manager here
    },
)
```

There are three changes in this snippet:

- `DuckDBPandasIOManager` is imported
- A DuckDB+Pandas I/O manager is defined
- The new I/O manager is added as a resource under the key `database_io_manager`

### Choosing an I/O manager for each asset

You now have two I/O managers configured within a code location. By default, all assets will use the I/O manager under the `io_manager` key. Because you updated the `io_manager` key, the `FilesytemIOManager` could plug and play without any additional changes. Overriding and priority are given to the I/O manager key that is most specific to the asset.

To have the `topstories` asset store its data in DuckDB, modify its asset function in `assets.py` by specifying the asset’s `io_manager_key` to be `"database_io_manager"`, as shown below:

```python file=/tutorial/saving/assets.py startafter=start_update_asset endbefore=end_update_asset
@asset(
    group_name="hackernews",
    io_manager_key="database_io_manager",  # Addition: `io_manager_key` specified
)
def topstories(context: AssetExecutionContext, topstory_ids: List) -> pd.DataFrame:
    logger = get_dagster_logger()

    results = []
    for item_id in topstory_ids:
        item = requests.get(
            f"https://hacker-news.firebaseio.com/v0/item/{item_id}.json"
        ).json()
        results.append(item)

        if len(results) % 20 == 0:
            logger.info(f"Got {len(results)} items so far.")

    df = pd.DataFrame(results).drop(
        ["kids"], axis=1
    )  # TODO - why are we dropping kids column here, but not in the other versions of this asset?

    context.add_output_metadata(
        metadata={
            "num_records": len(df),
            "preview": MetadataValue.md(df.head().to_markdown()),
        }
    )

    return df
```

To validate that this worked, materialize your entire asset graph. The I/O manager should store the DataFrame in a table called `analytics.hackernews.topstories` and continue to store the IDs and most frequent words in a directory called `data`.

Despite the changes in where the data is stored, the developer experience hasn’t changed. I/O managers deal with reading/writing data from storage for you so you can focus on the core logic and computation.

---

## Next steps

This tutorial section shows some of the simplest possible uses of I/O managers. However, this is the tip of the iceberg of what I/O managers can do.

Once you’ve outgrown being able to load all your data into memory in one chunk, you should learn about [partitions](/concepts/partitions-schedules-sensors/partitions) and how they are used to load only slices of your data into memory.

We also briefly mentioned resources. A property of resources is that they can be configured depending on the use case, such as using DuckDB as your data warehouse during development and Databricks during production. Next, you'll learn how to build more robustness, reusability, and flexibility when [connecting to external services](/tutorial/connecting-to-external-services) by using resources.
