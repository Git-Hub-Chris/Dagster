---
title: Transitioning Data Pipelines from Development to Production | Dagster
description: This guide walks through how to transition your data pipelines from local development to production
---

# Transitioning Data Pipelines from Development to Production

In this guide, we'll walk through how to transition your data pipelines from local development to staging and production deployments.

Let's say we’ve been tasked with fetching data from Hacker News and splitting it into two datasets: one containing all of the data about stories and one containing all of the data about comments. We have two additional requirements:

- We must be able to run our data pipeline in local, staging, and production environments.
- We need to be confident that data won't be accidentally overwritten (for example because a user forgot to change a configuration value).

Using a few Dagster concepts, we can easily tackle this task! If you’re new to Dagster, here’s an overview of the main concepts we’ll be using in this guide (if you’re already familiar with Dagster, feel free to skip to the next paragraph):

- [Asset](/concepts/software-defined-assets) - an asset is a software object that models a data asset. The prototypical example is a table in a database or a file in cloud storage.
- [Resource](/concepts/resource) - a resource is an object that models a connection to a (typically) external service. Resources can be shared between assets and different implementations of resources can be swapped in depending on environment. For example, a resource may provide methods to send messages in Slack.
- [IO manager](/concepts/io-management/io-managers) - an IO manager is a special kind of resource that handles storing and loading assets. For example, if we wanted to store assets in S3, we would use Dagster’s built in S3 IO manager.
- [Run config](/concepts/configuration/config-schema) - assets and resources sometimes require configuration to set certain values (like the password to a database). Run config allows you to set these values at run time. In this guide, we will also use an API to set some default run configuration.

Using these Dagster concepts we will:

- Write three assets: the entire Hacker News dataset, data about comments, and data about stories.
- Write a resource to fetch data from Hacker News.
- Use Dagster's Snowflake IO manager to store the datasets in [Snowflake](https://www.snowflake.com/).
- Set up our repository so that the configuration for the Snowflake IO manager is automatically supplied based on the environment where the code is running.

<Note>
  At the end of this guide, you'll find an optional section that demonstrates a
  pattern for mocking resources that can be helpful when writing unit tests.
</Note>

### Setup

If you want to follow along with this guide you have two options:

#### Clone the example code:

This will give you the final version of the code we will write in this guide

```bash
pip install dagster, dagit
dagster new-project --example local_to_prod
cd local_to_prod
pip install .
```

#### Start with a scaffolded project:

This will create an empty project where you can copy the code as we write it in this guide

```bash
pip install dagster, dagit
dagster new-project --quick-start local_to_prod
cd local_to_prod
pip install pandas pytest dagster-snowflake dagster-snowflake-pandas
```

### Part one: Local development

In this section we will:

- Write our assets.
- Write a resource to fetch data from Hacker News.
- Add the assets to our [repository](/concepts/repositories-workspaces/repositories).
- Add run configuration for the Snowflake IO manager.
- Materialize assets in Dagit.

Let’s start by writing our three assets. We'll use Pandas DataFrames to interact with the data.

```python file=/guides/dagster/local_to_prod_tutorial/assets.py startafter=start_assets endbefore=end_assets
# assets.py


@asset(
    required_resource_keys={"hn_client"},
)
def items(context) -> pd.DataFrame:
    """Items from the Hacker News API: each is a story or a comment on a story."""
    hn_client = context.resources.hn_client

    rows = []
    for item_id in range(1, hn_client.fetch_max_item_id()):
        rows.append(hn_client.fetch_item_by_id(item_id))

    result = pd.DataFrame(rows, columns=hn_client.item_field_names).drop_duplicates(
        subset=["id"]
    )
    result.rename(columns={"by": "user_id"}, inplace=True)
    return result


@asset
def comments(items: pd.DataFrame) -> pd.DataFrame:
    """Comments from the Hacker News API."""
    return items[items["type"] == "comment"]


@asset
def stories(items: pd.DataFrame) -> pd.DataFrame:
    """Stories from the Hacker News API."""
    return items[items["type"] == "story"]
```

This results in an asset graph that looks like this:

<Image
alt="alt"
src="/images/guides/local_to_production/hacker_news_asset_graph.png"
width={1538}
height={730}
/>

Now let's write the resource to fetch data from Hacker News.

<Note>

In many cases, it is often most convenient to interact with an external service directly in assets or ops, rather than write a resource. We recommend that you refactor your code to use resources in the following cases:

- you need multiple assets or ops to interact with the service in a consistent way
- you want to interact with a different service in certain scenarios (ie. a staging environment, or unit tests)

</Note>

```python file=/guides/dagster/local_to_prod_tutorial/resources/resources_v1.py startafter=start_resource endbefore=end_resource
# resources.py


class HNAPIClient:
    """
    Hacker News client that fetches live data
    """

    def fetch_item_by_id(self, item_id: int) -> Optional[Dict[str, Any]]:
        """Fetches a single item from the Hacker News API by item id."""

        item_url = f"https://hacker-news.firebaseio.com/v0/item/{item_id}.json"
        item = requests.get(item_url, timeout=5).json()
        return item

    def fetch_max_item_id(self) -> int:
        return requests.get(
            "https://hacker-news.firebaseio.com/v0/maxitem.json", timeout=5
        ).json()


@resource
def hn_api_client():
    return HNAPIClient()
```

<Note>
  For the sake of brevity in this guide, we've omitted the implementation of
  <code>item_field_names</code>. You can find the full implementation of this resource
  in the <a href="https://github.com/dagster-io/dagster/tree/master/examples/local_to_production">
    full code example
  </a> on GitHub.
</Note>

Now we can add these assets to our repository and materialize them via Dagit as part of our local development workflow. We will use the [configured API](/concepts/configuration/configured) to add configuration for the `snowflake_io_manager`.

```python file=/guides/dagster/local_to_prod_tutorial/repository/repository_v2.py startafter=start endbefore=end
# repository.py


@repository
def repo():
    resource_defs = {
        "hn_client": hn_api_client,
        "io_manager": snowflake_io_manager.configured(
            {
                "account": "abc1234.us-east-1",
                "user": "me@company.com",
                "password": "my_super_secret_password",
                "database": "SANDBOX",
                "schema": "ALICE",
            }
        ),
    }

    return [*with_resources([items, comments, stories], resource_defs=resource_defs)]
```

Now we can materialize the assets the Dagit and ensure that the data appears in Snowflake as we expect:

\[SCREENSHOT]

<!-- \[Image: Screen Shot 2022-07-19 at 3.21.32 PM.png] -->

While we define our assets as Pandas DataFrames, the Snowflake IO manager automatically translates the asset data to and from Snowflake tables. The Python asset name determines the Snowflake table name. In this case three tables will be created: `ITEMS`, `COMMENTS` and `STORIES`.

### Part two: Deployment

In this section we will:

- Modify the configuration for the Snowflake IO manager to handle staging and production environments.
- Discuss different options for managing a staging environment.

Now that our assets work locally, we can start the deployment process! We’re going to get our assets set up for our production deployment first, and then circle back and discuss the options for our staging environment.

#### Production

We want to store the assets in a production Snowflake database, so we need to update the configuration for the `snowflake_io_manager`. But if we were to simply update out the values we set for local development, we run in an issue: the next time a developer wants to work on these assets, they will need to remember to change the configuration back to the local values. This leaves room for a developer to accidentally overwrite the production asset during local development.

Instead, we can set up the repository to attach resources to assets based on the environment:

```python file=/guides/dagster/local_to_prod_tutorial/repository/repository_v4.py startafter=start endbefore=end
# repository.py

# Note: there are multiple issues with how this config is specified, mainly that
# passwords are being stored in code. This will be addressed next.
@repository
def repo():
    resource_defs = {
        "local": {
            "hn_client": hn_api_client,
            "io_manager": snowflake_io_manager.configured(
                {
                    "account": "abc1234.us-east-1",
                    "user": "me@company.com",
                    "password": "my_super_secret_password",  # password in config is bad practice
                    "database": "SANDBOX",
                    "schema": "ALICE",
                }
            ),
        },
        "production": {
            "hn_client": hn_api_client,
            "io_manager": snowflake_io_manager.configured(
                {
                    "account": "abc1234.us-east-1",
                    "user": "dev@company.com",
                    "password": "company_super_secret_password",  # password in config is bad practice
                    "database": "PRODUCTION",
                    "schema": "HACKER_NEWS",
                }
            ),
        },
    }
    deployment_name = os.getenv("DAGSTER_DEPLOYMENT", "local")

    return [
        with_resources(
            [items, comments, stories], resource_defs=resource_defs[deployment_name]
        )
    ]
```

Now, we can set the environment variable `DAGSTER_DEPLOYMENT=production` in our deployment and the correct resources will be applied to the assets.

<Note>
If you're using <a href="https://dagster.io/cloud">Dagster Cloud</a>, this environment variable will be set for you. Additionally, if you're using <a href="https://docs.dagster.cloud/guides/branch-deployments">Branch Deployments</a> <code>DAGSTER_DEPLOYMENT</code> will be automatically set to <code>VAR</code>.

</Note>

We still have some problems with this setup:

1. Developers needs to remember to change `user` and `password` to their credentials and `schema` to their name when developing locally.
2. Passwords are being stored in code.
3. If we want to change the database where we store development or production data, we need to make a code change.

We can easily solve all of these problems because the Snowflake IO manager accepts configuration from environment variables using the <PyObject object="StringSource" /> configuration type. This allows us to store configuration values as environment variables, and point Dagster to those environment variables in the run configuration:

```python file=/guides/dagster/local_to_prod_tutorial/repository/repository_v5.py startafter=start endbefore=end
# repository.py


@repository
def repo():
    resource_defs = {
        "local": {
            "hn_client": hn_api_client,
            "io_manager": snowflake_io_manager.configured(
                {
                    "account": "abc1234.us-east-1",
                    "user": {"env": "DEV_SNOWFLAKE_USER"},
                    "password": {"env": "DEV_SNOWFLAKE_PASSWORD"},
                    "database": "SANDBOX",
                    "schema": {"env": "DEV_SNOWFLAKE_SCHEMA"},
                }
            ),
        },
        "production": {
            "hn_client": hn_api_client,
            "io_manager": snowflake_io_manager.configured(
                {
                    "account": "abc1234.us-east-1",
                    "user": "system@company.com",
                    "password": {"env": "SYSTEM_SNOWFLAKE_PASSWORD"},
                    "database": "PRODUCTION",
                    "schema": "HACKER_NEWS",
                }
            ),
        },
    }
    deployment_name = os.getenv("DAGSTER_DEPLOYMENT", "local")

    return [
        with_resources(
            [items, comments, stories], resource_defs=resource_defs[deployment_name]
        )
    ]
```

#### Staging

Depending on your organization’s Dagster setup, there are a couple of options for a staging environment.

For Dagster Cloud users, we recommend using [Branch Deployments](https://docs.dagster.cloud/guides/branch-deployments) as your staging step. A Branch Deployment is a new Dagster deployment that is automatically generated for each git branch. Check out our \[comprehensive guide to Branch Deployments]\(link to claire’s doc) to learn how to use Branch Deployments to verify data pipelines before deploying them to production.

For a self hosted staging deployment of Dagster, we’ve already done all of the necessary work to run our assets in staging! Since we get the configuration for our assets from environment variables, we can simply set the variables to new values in the staging deployment, and we’re all set.

### Conclusion

This guide demonstrates how we recommend that you write your assets and jobs so that they transition from local development to staging and production environments without requiring code changes at each step. While we focused on assets in this guide, the same concepts and APIs can be used to swap out run configuration for jobs.

### Optional: Testing

You may have noticed a missing step in the development workflow presented in this guide— unit tests! While the main purpose of the guide is to help you transition your code from local development to a production deployment, unit testing is still an important part of the development cycle, and we’d like to take this time to explore a pattern you may find useful when writing your own unit tests: stubbing resources.

The general idea behind stubbing or mocking a Dagster resource is relatively simple: since Dagster’s resource system allows you to supply different implementations of a resource to an asset or op, we can make a simplified version of a “real” resource and use it in unit tests!

<Note>
  Determining when it makes sense to stub a resource for a unit test can be a
  topic of much debate. There are certainly some resources where it would be too
  complicated to write and maintain a stub. In general, if a resource is
  relatively simple, writing a stub can be really helpful for unit testing the
  assets and ops that use the resource.
</Note>

Since the Hacker News resource we wrote in our example fetches live data and we only need the values from a few columns in our assets, it’s a great candidate for writing a stub. With a stubbed resource we can have full control over the data our assets see during unit tests, which in turn can allow us to make more specific assertions.

We’ll start by writing the stubbed version of the Hacker News resource. We want to make sure the stub has implementations for each method the real `HackerNewsClient` implements:

```python file=/guides/dagster/local_to_prod_tutorial/resources/resources_v2.py startafter=start_mock endbefore=end_mock
# resources.py


class StubHNClient:
    """
    Hacker News Client that returns fake data
    """

    def __init__(self):
        self.data = {
            1: {
                "id": 1,
                "type": "comment",
                "title": "the first comment",
                "by": "user1",
            },
            2: {"id": 2, "type": "story", "title": "an awesome story", "by": "user2"},
        }

    def fetch_item_by_id(self, item_id: int) -> Optional[Dict[str, Any]]:
        return self.data.get(item_id)

    def fetch_max_item_id(self) -> int:
        return len(self.data.items())

    @property
    def item_field_names(self):
        return ["id", "type", "title", "by"]


@resource
def stub_hn_client():
    return StubHNClient()
```

<Note>
  Since the stub Hacker News resource and the real Hacker News resource need to
  implement the same methods, this would be a great time to write an interface.
  We’ll skip the implementation in this guide, but you can find it in the{" "}
  <a href="https://github.com/dagster-io/dagster/tree/master/examples/local_to_production">
    full code example
  </a>
  .
</Note>

Now we can use the stub Hacker News resource to test that the items asset transforms the data in the way we expect:

```python file=/guides/dagster/local_to_prod_tutorial/test_assets.py startafter=start endbefore=end
# test_assets.py


def test_items():
    context = build_op_context(resources={"hn_client": stub_hn_client})
    hn_dataset = items(context)
    assert isinstance(hn_dataset, pd.DataFrame)

    expected_data = pd.DataFrame(StubHNClient().data.values()).rename(
        columns={"by": "user_id"}
    )

    assert (hn_dataset == expected_data).all().all()
```
