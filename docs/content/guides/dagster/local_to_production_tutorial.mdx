---
title: Development to Production Workflow | Dagster
description: This guide walks through how to transition your code from local development to production
---

# Development to Production Workflow

In this example, we will walk through how to transition your assets and jobs from local development to unit testing and production deployments. We will cover swapping out resources and IO managers and setting run config based on the environment where your code will be executed.

If you’re new to dagster, here’s an overview of the main concepts we’ll be using in this example (if you’re already familiar with dagster, feel free to skip to the next section):

- asset - an asset is a software object that models a data asset. The prototypical example is a table in a database or a file in cloud storage.
- resource - a resource is an object that models a connection to a (typically) external service. Resources can be shared between assets and different implementations of resources can be swapped in depending on environment. For example, a resource may provide methods to send messages in Slack.
- IO manager - an IO manager is a special kind of resource that handles storing and loading assets. For example, if we wanted to store assets in S3, we would use dagster’s built in S3 IO manager.
- run config - assets and resources sometimes require configuration to set certain values (like the password to a database). Run config allows you to set these values at run time. In this example, we will also use an API to set some default run configuration.

## Problem Statement

Let’s say we’ve been tasked with fetching data from Hacker News and splitting it into two datasets: one containing all of the data about stories and one containing all of the data about comments. We have the following additional requirements:

- For local development, we want to store the datasets as tables in a development snowflake database called `SANDBOX` that has a schema for each user.
- For production, we want to store the datasets as tables in a production snowflake database called `PRODUCTION` that has a schema called `HACKER_NEWS`.
- In our staging environment, we want to store the dataset as tables in a snowflake database called `STAGING` that has a schema called `HACKER_NEWS`

At the end of this example, you'll find an optional section that demonstrates a pattern for mocking resources that can be helpful when writing unit tests.

### Setup

If you want to follow along with this example, you can either:

Clone the example (this will give you the final version of the code):

    dagster new-project --example local_to_prod
    cd local_to_prod
    pip install .

Start with a scaffolded project and fill in the code as we go

    dagster new-project --quick-start local_to_prod
    cd local_to_prod
    pip install dagster dagit pandas pytest dagster-snowflake dagster-snowflake-pandas

### Part One: Local Development

Let’s start by writing the assets

```python file=/guides/dagster/local_to_prod_tutorial/assets.py startafter=start_assets endbefore=end_assets
# assets.py

@asset(
    required_resource_keys={"hn_client"},
)
def items(context) -> pd.DataFrame:
    """Items from the Hacker News API: each is a story or a comment on a story."""

    hn_client = context.resources.hn_client

    rows = []
    for item_id in range(1, 100 + 1):
        rows.append(hn_client.fetch_item_by_id(item_id))

    result = pd.DataFrame(rows, columns=hn_client.item_field_names).drop_duplicates(subset=["id"])
    result.rename(columns={"by": "user_id"}, inplace=True)

    return result


@asset
def comments(items: pd.DataFrame) -> pd.DataFrame:
    """Comments from the Hacker News API."""
    return items[items["type"] == "comment"]


@asset
def stories(items: pd.DataFrame) -> pd.DataFrame:
    """Stories from the Hacker News API."""
    return items[items["type"] == "story"]
```

We’ll also need to write the resource to fetch data from Hacker News

<Note>

When you’re in the beginning development stages, you may find it easier to write the code to fetch data directly in your assets or ops. This is great for initial development, but eventually you may want to refactor your code to use resources to support things like testing. In this case we’re going to jump right into defining a resource because we're going to use it again in the optional section at the end of this example.

</Note>

```python file=/guides/dagster/local_to_prod_tutorial/resources/resources_v1.py startafter=start_resource endbefore=end_resource
# resources.py

class HNAPIClient:
    """
    Hacker News client that fetches live data
    """

    @property
    def item_field_names(self):
        return [
            "id",
            "parent",
            "time",
            "type",
            "by",
            "text",
            "kids",
            "score",
            "title",
            "descendants",
            "url",
        ]

    def fetch_item_by_id(self, item_id: int) -> Optional[Dict[str, Any]]:
        """Fetches a single item from the Hacker News API by item id."""

        item_url = f"https://hacker-news.firebaseio.com/v0/item/{item_id}.json"
        item = requests.get(item_url, timeout=5).json()
        return item

    def fetch_max_item_id(self) -> int:
        return requests.get("https://hacker-news.firebaseio.com/v0/maxitem.json", timeout=5).json()

@resource
def hn_api_client():
    return HNAPIClient()
```

Now we can add these assets to our repository and materialize them via Dagit as part of our local development workflow

```python file=/guides/dagster/local_to_prod_tutorial/repository/repository_v1.py startafter=start endbefore=end
# repository.py

@repository
def repo():
    return [
        *with_resources(
            [items, comments, stories],
            resource_defs={"hn_client": hn_api_client, "io_manager": snowflake_io_manager}
        )
    ]
```

When we materialize the assets in Dagit we can add configuration for the `snowflake_io_manager` in the launchpad or we can use the `configured` API and update our repository to look like this:

```python file=/guides/dagster/local_to_prod_tutorial/repository/repository_v2.py startafter=start endbefore=end
# repository.py

@repository
def repo():
    resource_defs = {
        "hn_client": hn_api_client,
        "io_manager": snowflake_io_manager.configured(
            {
                "account": "abc1234.us-east-1",
                "user": "me@company.com",
                "password": "my_super_secret_password",
                "database": "SANDBOX",
                "schema": "JAMIE"
            }
        ),
    }

    return [*with_resources([items, comments, stories], resource_defs=resource_defs)]
```

Now we can materialize the assets the Dagit and ensure that the data appears in snowflake as we expect:

<!-- \[Image: Screen Shot 2022-07-19 at 3.21.32 PM.png] -->

<Note>

While we define our assets as pandas DataFrames, the snowflake IO manager automatically translates the asset data to and from snowflake tables. The Python asset name determines the snowflake table name. In this case three tables will be created: ITEMS, COMMENTS, and STORIES

</Note>

### Part Two: Deployment

Now that our assets work locally, we can start the deployment process! We’re going to get our assets set up for our production deployment first, and then circle back and discuss the options for our staging environment.

#### Production

We want to store the assets in a production Snowflake database, so we need to update the configuration for the `snowflake_io_manager`

```python file=/guides/dagster/local_to_prod_tutorial/repository/repository_v3.py startafter=start endbefore=end
# repository.py

@repository
def repo():
    resource_defs = {
        "hn_client": hn_api_client,
        "io_manager": snowflake_io_manager.configured(
            {
                "account": "abc1234.us-east-1",
                "user": "dev@company.com",
                "password": "company_super_secret_password",
                "database": "PRODUCTION",
                "schema": "HACKER_NEWS"
            }
        ),
    }

    return [*with_resources([items, comments, stories], resource_defs=resource_defs)]
```

Here we run into an issue. The next time a developer wants to work on the assets, they will have to remember to  change the configuration for the snowflake_io_manager. This leaves room for a developer to accidentally override the production asset during local development. Instead we can set up the repository to attach resources to assets based on the environment

```python file=/guides/dagster/local_to_prod_tutorial/repository/repository_v4.py startafter=start endbefore=end
# repository.py

@repository
def repo():
    resource_defs = {
        "local": {
            "hn_client": hn_api_client,
            "io_manager": snowflake_io_manager.configured({
                    "account": "abc1234.us-east-1",
                    "user": "me@company.com",
                    "password": "my_super_secret_password",
                    "database": "SANDBOX",
                    "schema": "JAMIE"
            })
        },
        "production": {
            "hn_client": hn_api_client,
            "io_manager": snowflake_io_manager.configured({
                    "account": "abc1234.us-east-1",
                    "user": "dev@company.com",
                    "password": "company_super_secret_password",
                    "database": "PRODUCTION",
                    "schema": "HACKER_NEWS"
            })
        }
    }
    deployment_name = os.environ.get("DAGSTER_DEPLOYMENT", "local")

    return [
        with_resources([items, comments, stories], resource_defs=resource_defs[deployment_name])
    ]
```

Now we just set the environment variable `DAGSTER_DEPLOYMENT=production` in our deployment and the correct resources will be applied to the assets.

<Note>
If you are using [Dagster Cloud](link to dagster cloud page explaining this?), this environment variable will be set for you. Additionally, if you are using [Branch Deployments](link to branch deployments page explaining this?) DAGSTER_DEPLOYMENT will be automatically set to <VAR>.

</Note>

We still have some problems with this setup:

1. Each developer still needs to remember to change `user` and `password` to their credentials and `schema` to their name when they develop locally.
2. Passwords are being stored in code.
3. If we want to change the database where we store development or production data, we need to make a code change.

We can easily solve all of these problems because the snowflake IO manager accepts configuration from environment variables using the <PyObject object="StringSource" /> configuration type. This allows us to store configuration values as environment variables, and point dagster to those environment variables in the run configuration:

```python file=/guides/dagster/local_to_prod_tutorial/repository/repository_v5.py startafter=start endbefore=end
# repository.py

@repository
def repo():
    resource_defs = {
        "hn_client": hn_api_client,
        "io_manager": snowflake_io_manager.configured(
            {
                "account": {"env": "SNOWFLAKE_ACCOUNT"},
                "user": {"env": "SNOWFLAKE_USER"},
                "password": {"env": "SNOWFLAKE_PASSWORD"},
                "database": {"env": "SNOWFLAKE_DATABASE"},
                "schema": {"env": "SNOWFLAKE_SCHEMA"},
            }
        ),
    }

    return [*with_resources([items, comments, stories], resource_defs=resource_defs)]
```

<Note>

Since all of the snowflake IO manager configuration is being set through environment variables, we no longer need to select the resource configuration based on deployment! If we later add more configuration that varies between local and production, we can go back to the previous pattern of selecting config based on the deployment name.

</Note>

#### Staging

Depending on your organization’s dagster setup, there are a couple of options for a staging environment.

If you are a dagster cloud user, we recommend using Branch Deployments as your staging step. Check out our [comprehensive guide to Branch Deployments](link to claire’s doc) to learn how to set up Branch Deployments for your organization.

If you host a staging deployment of dagster, we’ve already done all of the necessary work to run our assets in staging! Since we get the configuration for our assets from environment variables, we can simply set the variables to new values in the staging environment, and we’re all set.

### Conclusion

This example demonstrates how we recommend that you write your assets and jobs so that they transition from local development to staging and production environments without requiring code changes at each step.  While we focused on assets in this example, the same concepts and APIs can be used to swap out run configuration for jobs.


### Optional:  Testing

You may have noticed a missing step in the development workflow presented in this guide—  unit tests! While the main purpose of the guide is to help you transition your code from local development to a production deployment, unit testing is still an important part of the development cycle, and we’d like to take this time to explore a pattern you may find useful when writing your own unit tests: stubbing resources.

The general idea behind stubbing or mocking a dagster resource is relatively simple: since dagster’s resource system allows you to supply different implementations of a resource to an asset or op, we can make a simplified version of a “real” resource and use it in unit tests!

<Note>
Determining when it makes sense to stub a resource for a unit test can be a topic of much debate. There are certainly some resources where it would be too complicated to write and maintain a stub. In general, if a resource is relatively simple, writing a stub can be really helpful for unit testing the assets and ops that use the resource.
</Note>

Since the Hacker News resource we wrote in our example fetches live data and we only use a few columns in our assets, it’s a great candidate for writing a stub. With a stubbed resource we can have full control over the data our assets see during unit tests, which in turn can allow us to make more specific assertions.

We’ll start by writing the stubbed version of the HackerNews resource. We want to make sure the stub has implementations for each method the real HackerNews resource implements:

```python file=/guides/dagster/local_to_prod_tutorial/resources/resources_v2.py startafter=start_mock endbefore=end_mock
# resources.py

class MockHNClient(HNClient):
    """
    Hacker News Client that returns mock data
    """

    def __init__(self):
        self.data = {
            1: {
                "id": 1,
                "type": "comment",
                "title": "the first comment",
                "by": "user1",
            },
            2: {
                "id": 2,
                "type": "story",
                "title": "an awesome story",
                "by": "user2",
            },
        }

    @property
    def item_field_names(self):
        return ["id", "type", "title", "by"]

    def fetch_item_by_id(self, item_id: int) -> Optional[Dict[str, Any]]:
        return self.data.get(item_id)

    def fetch_max_item_id(self) -> int:
        return len(self.data.items())


@resource
def mock_hn_client():
    return MockHNClient()
```

<Note>
Since the stub Hacker News resource and the real Hacker News resource need to implement the same methods, this would be a great time to write an interface. We’ll skip the implementation in this guide, but you can find it in the [full code](link to github)
</Note>

Now we can use the stub Hacker News resource to test that the items asset transforms the data in the way we expect:
```python file=/guides/dagster/local_to_prod_tutorial/test_assets.py startafter=start endbefore=end
# test_assets.py

def test_items():
    context = build_op_context(resources={"hn_client": mock_hn_client})
    hn_dataset = items(context)
    assert isinstance(hn_dataset, pd.DataFrame)

    expected_data = pd.DataFrame(MockHNClient().data.values()).rename(columns={"by": "user_id"})

    assert (hn_dataset == expected_data).all().all()


def test_comments():
    mock_data = pd.DataFrame({"id": [1, 2, 3], "type": ["comment", "story", "comment"]})
    out_data = comments(mock_data)

    out_ids = out_data["id"].tolist()

    assert 1 in out_ids
    assert 2 not in out_ids
    assert 3 in out_ids


def test_stories():
    mock_data = pd.DataFrame({"id": [1, 2, 3], "type": ["story", "story", "comment"]})
    out_data = stories(mock_data)

    out_ids = out_data["id"].tolist()

    assert 1 in out_ids
    assert 1 in out_ids
    assert 3 not in out_ids
```

