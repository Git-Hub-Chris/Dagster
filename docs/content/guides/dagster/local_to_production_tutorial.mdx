---
title: Development to Production Workflow | Dagster
description: This guide walks through how to transition your code from local development to production
---

# Development to Production Workflow

In this example, we will walk through how to transition your assets and jobs from local development, to unit testing, and production deployments. We will cover swapping out resources and IO managers and setting run config based on the environment where your code will be executed.

If you’re new to dagster, here’s an overview of the main concepts we’ll be using in this example (if you’re already familiar with dagster, feel free to skip to the next section):

- asset - an asset is a software object that models a data asset. The prototypical example is a table in a database or a file in cloud storage.
- resource - a resource is an object that models a connection to a (typically) external service. Resources can be shared between assets and different implementations of resources can be swapped in depending on environment. For example, a resource may provide methods to send messages in Slack.
- IO manager - an IO manager is a special kind of resource that handles storing and loading assets. For example, if we wanted to store assets in S3, we would use dagster’s built in S3 IO manager.
- run config - assets and resources sometimes require configuration to set certain values (like the password to a database). Run config allows you to set these values at run time. In this example, we will also use an API to set some default run configuration.

## Problem Statement

Let’s say we’ve been tasked with fetching data from Hacker News and splitting it into two datasets: one containing all of the data about stories and one containing all of the data about comments. We have the following additional requirements:

- For local development, we want to store the datasets as tables in a development snowflake database called `SANDBOX` that has a schema for each user.
- For production, we want to store the datasets as tables in a production snowflake database called `PRODUCTION` that has a schema called `HACKER_NEWS`.
- For testing, we don’t want to fetch data from Hacker News, instead we want to provide seed data so we can assert that the data was split into the stories and comments datasets correctly.

### Setup

If you want to follow along with this example, you can either:

Clone the example (this will give you the final version of the code):

    dagster new-project --example local_to_prod
    cd local_to_prod
    pip install .

Start with a scaffolded project and fill in the code as we go

    dagster new-project --quick-start local_to_prod
    cd local_to_prod
    pip install dagster dagit pandas pytest dagster-snowflake dagster-snowflake-pandas

### Part One: Local Development

Let’s start by writing the assets

```python file=/guides/dagster/local_to_prod_tutorial/assets.py startafter=start_assets endbefore=end_assets
# assets.py

@asset(
    required_resource_keys={"hn_client"},
)
def items(context) -> pd.DataFrame:
    """Items from the Hacker News API: each is a story or a comment on a story."""

    hn_client = context.resources.hn_client

    rows = []
    for item_id in range(1, 100 + 1):
        rows.append(hn_client.fetch_item_by_id(item_id))

    result = pd.DataFrame(rows, columns=hn_client.item_field_names).drop_duplicates(subset=["id"])
    result.rename(columns={"by": "user_id"}, inplace=True)

    return result


@asset
def comments(items: pd.DataFrame) -> pd.DataFrame:
    """Comments from the Hacker News API."""
    return items[items["type"] == "comment"]


@asset
def stories(items: pd.DataFrame) -> pd.DataFrame:
    """Stories from the Hacker News API."""
    return items[items["type"] == "story"]
```

We’ll also need to write the resource to fetch data from Hacker News

<Note>

When you’re in the beginning development stages, you may find it easier to write the code to fetch data directly in your assets or ops. This is great for initial development, but eventually you may want to refactor your code to use resources to support things like testing. In this case we’re going to jump right into defining a resource because we know we’ll want to write a mock resource shortly.

</Note>

```python file=/guides/dagster/local_to_prod_tutorial/resources/resources_v1.py startafter=start_resource endbefore=end_resource
# resources.py

class HNAPIClient:
    """
    Hacker News client that fetches live data
    """

    @property
    def item_field_names(self):
        return [
            "id",
            "parent",
            "time",
            "type",
            "by",
            "text",
            "kids",
            "score",
            "title",
            "descendants",
            "url",
        ]

    def fetch_item_by_id(self, item_id: int) -> Optional[Dict[str, Any]]:
        """Fetches a single item from the Hacker News API by item id."""

        item_url = f"https://hacker-news.firebaseio.com/v0/item/{item_id}.json"
        item = requests.get(item_url, timeout=5).json()
        return item

    def fetch_max_item_id(self) -> int:
        return requests.get("https://hacker-news.firebaseio.com/v0/maxitem.json", timeout=5).json()

@resource
def hn_api_client():
    return HNAPIClient()
```

Now we can add these assets to our repository and materialize them via Dagit as part of our local development workflow

```python file=/guides/dagster/local_to_prod_tutorial/repository/repository_v1.py startafter=start endbefore=end
# repository.py

@repository
def repo():
    return [
        *with_resources(
            [items, comments, stories],
            resource_defs={"hn_client": hn_api_client, "io_manager": snowflake_io_manager}
        )
    ]
```

When we materialize the assets in Dagit we can add configuration for the `snowflake_io_manager` in the launchpad or we can use the `configured` API and update our repository to look like this:

```python file=/guides/dagster/local_to_prod_tutorial/repository/repository_v2.py startafter=start endbefore=end
# repository.py

@repository
def repo():
    resource_defs = {
        "hn_client": hn_api_client,
        "io_manager": snowflake_io_manager.configured(
            {
                "account": "abc1234.us-east-1",
                "user": "me@company.com",
                "password": "my_super_secret_password",
                "database": "SANDBOX",
                "schema": "JAMIE"
            }
        ),
    }

    return [*with_resources([items, comments, stories], resource_defs=resource_defs)]
```

Now we can materialize the assets the Dagit and ensure that the data appears in snowflake as we expect:

<!-- \[Image: Screen Shot 2022-07-19 at 3.21.32 PM.png] -->

<Note>

While we define our assets as pandas DataFrames, the snowflake IO manager automatically translates the asset data to and from snowflake tables. The Python asset name determines the snowflake table name. In this case three tables will be created: ITEMS, COMMENTS, and STORIES

</Note>

### Part Two: Testing

Let’s write some tests! We don’t want to rely on external calls to the Hacker News API during out unit tests, so the first thing we will need to do is write a version of the Hacker News API client that will serve consistent data.

<Note>

At Dagster we believe one of the main barriers to testing data applications is that business logic depends on external, heavy-weight dependencies that are difficult to run in a testing scenario. This is why we developed the concept of [resources](/concepts/resources).

When testing an asset or op with an external dependency (either a resource or IO manager), we recommend using a lightweight version of the resource that implements all of the methods used in the asset or op. You can use a Dagster-provided resource (like the in memory IO manager), or write your own implementation.

For more information and recommendations on testing, see our [testing documentation](/concepts/testing).

</Note>

We want to ensure that the version of the Hacker News Client we write for testing follows the API of the real Hacker News Client, so at this point, it would be a good idea to write an interface for the testing resource and `HNAPIClient` to implement:

```python file=/guides/dagster/local_to_prod_tutorial/resources/resources_v2.py startafter=start_abc endbefore=end_abc
# resources.py

class HNClient(ABC):
    """
    Base class for a Hacker News Client
    """

    @property
    def item_field_names(self):
        pass

    def fetch_item_by_id(self, item_id: int) -> Optional[Dict[str, Any]]:
        pass

    def fetch_max_item_id(self) -> int:
        pass
```

then we can write the version of the resource we will use for testing:

```python file=/guides/dagster/local_to_prod_tutorial/resources/resources_v2.py startafter=start_mock endbefore=end_mock
# resources.py

class MockHNClient(HNClient):
    """
    Hacker News Client that returns mock data
    """

    def __init__(self):
        self.data = {
            1: {
                "id": 1,
                "type": "comment",
                "title": "the first comment",
                "by": "user1",
            },
            2: {
                "id": 2,
                "type": "story",
                "title": "an awesome story",
                "by": "user2",
            },
        }

    @property
    def item_field_names(self):
        return ["id", "type", "title", "by"]

    def fetch_item_by_id(self, item_id: int) -> Optional[Dict[str, Any]]:
        return self.data.get(item_id)

    def fetch_max_item_id(self) -> int:
        return len(self.data.items())


@resource
def mock_hn_client():
    return MockHNClient()
```

Now, let’s write tests to verify that the assets produce the datasets we expect:

```python file=/guides/dagster/local_to_prod_tutorial/test_assets.py startafter=start endbefore=end
# test_assets.py

def test_items():
    context = build_op_context(resources={"hn_client": mock_hn_client})
    hn_dataset = items(context)
    assert isinstance(hn_dataset, pd.DataFrame)

    expected_data = pd.DataFrame(MockHNClient().data.values()).rename(columns={"by": "user_id"})

    assert (hn_dataset == expected_data).all().all()


def test_comments():
    mock_data = pd.DataFrame({"id": [1, 2, 3], "type": ["comment", "story", "comment"]})
    out_data = comments(mock_data)

    out_ids = out_data["id"].tolist()

    assert 1 in out_ids
    assert 2 not in out_ids
    assert 3 in out_ids


def test_stories():
    mock_data = pd.DataFrame({"id": [1, 2, 3], "type": ["story", "story", "comment"]})
    out_data = stories(mock_data)

    out_ids = out_data["id"].tolist()

    assert 1 in out_ids
    assert 1 in out_ids
    assert 3 not in out_ids
```

### Part Three: Production

Now that our assets work locally and can be tested, let’s set up for the production environment. We want to store the assets in a production Snowflake database, so we need to update the configuration for the `snowflake_io_manager`:

```python file=/guides/dagster/local_to_prod_tutorial/repository/repository_v3.py startafter=start endbefore=end
# repository.py

@repository
def repo():
    resource_defs = {
        "hn_client": hn_api_client,
        "io_manager": snowflake_io_manager.configured(
            {
                "account": "abc1234.us-east-1",
                "user": "dev@company.com",
                "password": "company_super_secret_password",
                "database": "PRODUCTION",
                "schema": "HACKER_NEWS"
            }
        ),
    }

    return [*with_resources([items, comments, stories], resource_defs=resource_defs)]
```

Here we run into an issue. The next time a developer wants to work on the assets, they will have to remember to change the configuration for the `snowflake_io_manager`. This leaves room for a developer to accidentally override the production asset during local development. Instead we can set up the repository to attach resources to assets based on the environment:

```python file=/guides/dagster/local_to_prod_tutorial/repository/repository_v4.py startafter=start endbefore=end
# repository.py

@repository
def repo():
    resource_defs = {
        "local": {
            "hn_client": hn_api_client,
            "io_manager": snowflake_io_manager.configured({
                    "account": "abc1234.us-east-1",
                    "user": "me@company.com",
                    "password": "my_super_secret_password",
                    "database": "SANDBOX",
                    "schema": "JAMIE"
            })
        },
        "production": {
            "hn_client": hn_api_client,
            "io_manager": snowflake_io_manager.configured({
                    "account": "abc1234.us-east-1",
                    "user": "dev@company.com",
                    "password": "company_super_secret_password",
                    "database": "PRODUCTION",
                    "schema": "HACKER_NEWS"
            })
        }
    }
    deployment_name = os.environ.get("DAGSTER_DEPLOYMENT", "local")

    return [
        with_resources([items, comments, stories], resource_defs=resource_defs[deployment_name])
    ]
```

Now we just set the environment variable `DAGSTER_DEPLOYMENT=production` in our deployment and the correct resources will be applied to the assets.

<Note>

If you are using \[Dagster Cloud]\(link to dagster cloud page explaining this?), this environment variable will be set for you. Additionally, if you are using \[Branch Deployments]\(link to branch deployments page explaining this?) `DAGSTER_DEPLOYMENT` will be automatically set to `VAR`.

</Note>

We still have some problems with this setup:

1. Each developer still needs to remember to change `user` and `password` to their credentials and `schema` to their name when they develop locally.
2. Passwords are being stored in code.
3. If we want to change the database where we store development or production data, we need to make a code change.

We can easily solve all of these problems because the snowflake IO manager accepts configuration from environment variables using the <PyObject object="StringSource" /> configuration type. This allows us to store configuration values as environment variables, and point dagster to those environment variables in the run configuration:

```python file=/guides/dagster/local_to_prod_tutorial/repository/repository_v5.py startafter=start endbefore=end
# repository.py

@repository
def repo():
    resource_defs = {
        "hn_client": hn_api_client,
        "io_manager": snowflake_io_manager.configured(
            {
                "account": {"env": "SNOWFLAKE_ACCOUNT"},
                "user": {"env": "SNOWFLAKE_USER"},
                "password": {"env": "SNOWFLAKE_PASSWORD"},
                "database": {"env": "SNOWFLAKE_DATABASE"},
                "schema": {"env": "SNOWFLAKE_SCHEMA"},
            }
        ),
    }

    return [*with_resources([items, comments, stories], resource_defs=resource_defs)]
```

<Note>

Since all of the snowflake IO manager configuration is being set through environment variables, we no longer need to select the resource configuration based on deployment! If we later add more configuration that varies between local and production, we can go back to the previous pattern of selecting config based on the deployment name.

</Note>

### Conclusion

This example demonstrates how we recommend that you write your assets and jobs so that they transition from local development to testing to production without requiring code changes at each step. While we focused on assets in this example, the same concepts and APIs can be used to swap out resources and run configuration for jobs.

If you'd like to see the full code from this example, you can find it in our examples [here](https://github.com/dagster-io/dagster/tree/master/examples/local_to_production)