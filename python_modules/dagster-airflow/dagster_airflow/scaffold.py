# What we want to do here is consume a pipeline definition and a config, and then generate an
# Airflow DAG definition, each of whose nodes corresponds to one step of the execution plan.

from airflow import DAG
from airflow.operators.dagster_plugin import DagsterOperator
from dagster import check, PipelineDefinition
from dagster.core.execution import create_execution_plan

from .storage import Storage

# Use a shared Storage for s3 access
# need to pass requirements to and from each step


# dag = DAG(
#     'minimal_dockerized_dagster_airflow_demo',
#     default_args=default_args,
#     schedule_interval=timedelta(minutes=10),
# )

# t1 = DagsterOperator(
#     api_version='1.21',
#     docker_url=os.getenv('DOCKER_HOST'),
#     command='pipeline execute demo_pipeline -e env.yml',
#     image='dagster-airflow-demo:latest',
#     network_mode='bridge',
#     task_id='minimal_dockerized_dagster_airflow_node',
#     dag=dag,
#     host_tmp_dir='/tmp/airflow',
#     tmp_dir='/tmp/airflow',
#     tls_client_ert=os.getenv('DOCKER_CERT_PATH'),
# )


def scaffold_airflow_dag(pipeline, env_config, storage=Storage, **dag_kwargs):
    check.inst_param(pipeline, 'pipeline', PipelineDefinition)
    check.opt_dict_param(env_config, 'env_config', key_type=str)

    execution_plan = create_execution_plan(pipeline, env_config)

    dag = DAG(
        dag_id=pipeline.name,
        description='***Generated by dagster-airflow***\n' + (pipeline.description or ''),
        **dag_kwargs
    )

    step_task_lookup = {}

    for step in execution_plan.topological_steps():
        step_task = DagsterOperator(step=step, dag=dag)
        step_task_lookup[step.key] = step_task

        for step_input in step.step_inputs:
            prev_step = step_input.prev_output_handle.step
            step_task_lookup[prev_step].set_downstream(step_task)

    return dag

    # Operator:
    # 1. grab inputs from s3
    # 2. grab execution_context from s3
    # 3. execute step
    # 4. write outputs to s3

    # Need to figure out what goes in the Dockerfile, and how that's constructed
    # Pickle the pipeline, serialize the config, into the Dockerfile (mounted volume)
    # Inside the dockerfile, generate the execution plan, grab the inputs, execute only the given
    # step, then write it back to s3 (pickle everything)
    # User is responsible for requirements
    # Rename scaffold
