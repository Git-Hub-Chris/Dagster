'''Scaffolding machinery for dagster-airflow.

Entrypoint is scaffold_airflow_dag, which consumes a pipeline definition and a config, and
generates an Airflow DAG definition each of whose nodes corresponds to one step of the execution
plan.
'''

import os

from copy import copy
from datetime import datetime, timedelta
from textwrap import TextWrapper

from airflow import DAG
from six import StringIO

from dagster import check, PipelineDefinition
from dagster.core.execution import create_execution_plan

from .utils import IndentingBlockPrinter



def normalize_key(key):
    return key.replace('_', '__').replace('.', '_')





def _make_editable_scaffold(pipeline_name, pipeline_description, env_config, static_scaffold, start_date):
    with IndentingBlockPrinter() as printer:
        pass
        # printer.
    pass


EDITABLE_SCAFFOLD = """
'''Editable scaffolding autogenerated by dagster-airflow from pipeline {pipeline_name} with config:

{env_config}

By convention, users should attempt to isolate post-codegen changes and customizations to this
"editable" file, rather than changing the definitions in the "static" {static_scaffold}.py. Please
let us know if you are encountering use cases where it is necessary to make changes to the static
file.
'''

import datetime

from {static_scaffold} import make_dag


# Arguments to be passed to the ``default_args`` parameter of the ``airflow.DAG`` constructor.
# You can override these with values of your choice.
DEFAULT_ARGS = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': {start_date},
    'email': ['airflow@example.com'],
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': datetime.timedelta(0, 300),
}

# Any additional keyword arguments to be passed to the ``airflow.DAG`` constructor. You can
# override these with values of your choice.
DAG_KWARGS = {{}}

# Any additional keyword arguments to be passed to the ``ModifiedDockerOperator`` constructor.
# You can override these with values of your choice.
MODIFIED_DOCKER_OPERATOR_KWARGS = {{}}


# The name of the autogenerated DAG. By default, this is just the name of the Dagster pipeline
# from which the Airflow DAG was generated. You may want to override this if, for instance, you
# want to schedule multiple DAGs corresponding to different configurations of the same Dagster
# pipeline.
DAG_ID = '{pipeline_name}'

# The description of the autogenerated DAG. By default, this is the description of the Dagster
# pipeline from which the Airflow DAG was generated. You may want to override this, as with the
# DAG_ID parameter
DAG_DESCRIPTION = '{pipeline_description}'

DOCKER_ARGUMENTS = {

}

# Set your S3 connection id here, if you do not want to use the default `aws_default` connection
S3_CONN_ID = 'aws_default'

dag = make_dag(
    dag_id=DAG_ID,
    dag_description=DAG_DESCRIPTION,
    dag_kwargs=dict(default_args=DEFAULT_ARGS, **DAG_KWARGS),
    s3_conn_id=S3_CONN_ID,
    modified_docker_operator_kwargs=MODIFIED_DOCKER_OPERATOR_KWARGS,
)
"""

STATIC_SCAFFOLD = """
'''Static scaffolding autogenerated by dagster-airflow from pipeline {pipeline_name} with config:

{env_config}

By convention, users should attempt to isolate post-codegen changes and customizations to the
"editable" {editable_scaffold}.py file, rather than changing the definitions in this "static" file.
Please let us know if you are encountering use cases where it is necessary to make changes to the
static file.
'''

from airflow import DAG
from airflow.operators.dagster_plugin import DagsterOperator

def make_dag(dag_id, dag_description, dag_kwargs, s3_conn_id, modified_docker_operator_kwargs):
    dag = DAG(
        dag_id=dag_id,
        description=dag_description,
        **dag_kwargs,
    )

    {step_definitions}

    {step_dependencies}
"""


def scaffold_airflow_dag(
    pipeline, env_config, image, output_path=None,
):
    '''Scaffold a new Airflow DAG based on a PipelineDefinition and config.


    '''
    check.inst_param(pipeline, 'pipeline', PipelineDefinition)
    check.opt_dict_param(env_config, 'env_config', key_type=str)

    output_path = os.path.abspath(os.path.expanduser(output_path))

    execution_plan = create_execution_plan(pipeline, env_config)

    # default_args = dict(DEFAULT_ARGS, **(dag_kwargs.pop('default_args', {})))

    # dag_file = DAG_SCAFFOLD.format(
    #     pipeline_name=pipeline.name,
    #     pipeline_description='***Autogenerated by dagster-airflow***'
    #     + (
    #         '\\n{description}'.format(description=pipeline.description)
    #         if pipeline.description
    #         else ''
    #     ),
    #     printed_env_config=str(env_config),
    #     default_args=str(default_args),
    #     dag_kwargs=(
    #         ',\n    '
    #         + '\n    '.join(
    #             [
    #                 '{key}={str_value}'.format(key=key, str_value=str(value))
    #                 for key, value in dag_kwargs.items()
    #             ]
    #         )
    #         if dag_kwargs
    #         else ''
    #     ),
    #     step_definitions='\n'.join(
    #         [
    #             '{step_key}_task = DagsterOperator(step=\'{step_key}\', dag=dag, '
    #             'image=\'{image}\', task_id=\'{step_key}\', s3_conn_id=S3_CONN_ID)'.format(
    #                 step_key=normalize_key(step.key), image=image
    #             )
    #             for step in execution_plan.topological_steps()
    #         ]
    #     ),
    #     step_dependencies='\n'.join(
    #         [
    #             '{prev_step_key}_task.set_downstream({step_key}_task)'.format(
    #                 prev_step_key=normalize_key(step_input.prev_output_handle.step.key),
    #                 step_key=normalize_key(step.key),
    #             )
    #             for step in execution_plan.topological_steps()
    #             for step_input in step.step_inputs
    #         ]
    #     ),
    # )

    # if output_path:
    #     with open(output_path, 'w') as fd:
    #         fd.write(dag_file)

    # return dag_file


## NEED TO CODEGEN TWO FILES
# TODO
# - Need to figure out how to scaffold an S3 hook so that the DagsterOperator has access to S3 for
#   persisting outputs
# - Need to figure out how to scaffold a Docker hook so that we can use a custom registry


# t1 = DagsterOperator(
#     api_version='1.21',
#     docker_url=os.getenv('DOCKER_HOST'),
#     command='pipeline execute demo_pipeline -e env.yml',
#     image='dagster-airflow-demo:latest',
#     network_mode='bridge',
#     task_id='minimal_dockerized_dagster_airflow_node',
#     dag=dag,
#     host_tmp_dir='/tmp/airflow',
#     tmp_dir='/tmp/airflow',
#     tls_client_ert=os.getenv('DOCKER_CERT_PATH'),
# )
