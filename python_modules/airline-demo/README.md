# Airline demo
This repository is intended as a realistic demo of Dagster's capabilities. It defines three
realistic data pipelines corresponding to stereotyped download, ingest, and analysis phases of
typical data science workflows, using real-world airline data. Although the view of the pipelines
provided by the Dagster tooling is unified, in typical practice we expect that each pipeline is
likely to be the responsibility of individuals with more or less clearly distinguished roles.

Use the airline demo to familiarize yourself with the features of the Dagster tooling in a more
fleshed-out context than the introductory tutorial, and as a reference when building your own
first production pipelines in the system. Comments and suggestions are enthusiastically encouraged!

### Requirements 
To run the airline demo pipelines locally, you'll need

- An Internet connection
- AWS credentials in the ordinary [boto3 credential chain](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/configuration.html)
- An [Apache Spark](https://spark.apache.org/downloads.html) install
- A running Postgres database available at `postgresql://test:test@127.0.0.1:5432/test`. (A
  docker-compose file is provided in this repo; run `docker-compose up` from the root of the
  airline demo.)

Use pip to install the demo's Python requirements:

    pip install -e .

### Pipelines and config

The demo defines a single repository with three pipelines, in `airline_demo/pipelines.py`:

- **airline_demo_download_pipeline** grabs data archives from S3 and unzips them.
- **airline_demo_ingest_pipeline** reads the raw data into Spark, performs some typical
  manipulations on the data, and then loads tables into a data warehouse.
- **airline_demo_warehouse_pipeline** performs typical in-warehouse analysis and manipulations
  using SQL, and then generates and archives analysis artifacts and plots using Jupyter notebooks.

Once you've installed the requirements, you can run `dagit` from the root of the demo and browse
the pipelines.

## The download pipeline

![Download pipeline](img/download_pipeline.png)

The `airline_demo_download_pipeline` models the first stage of most data science workflows, in
which raw data is consumed from a variety of sources. In practice, these might be files in S3 or
other cloud storage systems; publicly available datasets downloaded over http; or batch files in an
SFTP drop.

For demo purposes, we've put our source files in a 

### Running tests
You won't want to suppress test output if you want to see loglines from dagster:

    pytest -s

We use [pytest marks](https://docs.pytest.org/en/latest/example/markers.html#mark-examples) to
identify useful subsets of tests. For instance, to run only those tests that do not require a
running Spark cluster, you can run:

    pytest -m "not spark"

### Orchestrating AWS resources
The pipelines defined in this repository can run against a local Spark cluster
and Postgres instance, or they can run against a Redshift cluster and Spark
running on Amazon EMR.

We manage AWS resources with [pulumi](https://github.com/pulumi/pulumi).
First, install the pulumi CLI:

    brew install pulumi

Pulumi resources are divided by "project". To spin resources up for a demo run,
use the `demo_resources` project.

First, make sure you have the requirements. (You'll need to have already
installed TypeScript.)

    cd pulumi/demo_resources
    npm install

Then, you can bring up resources by running:

    pulumi up

This will take between 4 and 5 minutes.

To destroy resources, run:

    pulumi destroy

<small>
*Warning*: Currently we are unable to cleanly tear down the VPC using pulumi
because of dependency errors like the following:

    error: Plan apply failed: deleting urn:pulumi:demo_resources-dev::demo_resources::aws-infra:network:Network$aws:ec2/vpc:Vpc::airline_demo_vpc: DependencyViolation: The vpc 'vpc-01a7f7c286196cdc0' has dependencies and cannot be deleted.
        status code: 400, request id: 03a586f3-e17e-41e2-b5fd-c4275226df30

It's unclear whether this is an issue with pulumi/aws-infra or something else,
maybe the autogenerated EMR slave and master security groups (? -- but if so,
see https://github.com/pulumi/pulumi/issues/1691 -- or, if this is in fact an issue
with the underlying terraform provider, see https://github.com/terraform-providers/terraform-provider-aws/issues/3465,
and many other issues suggesting the EMR resource in the AWS provider is not
fully mature). There is no good way to find out *which* dependency is causing
this error (generated in the AWS CLI), and no way to force delete dependencies
from the command line (See: https://github.com/aws/aws-cli/issues/1721,
https://forums.aws.amazon.com/thread.jspa?threadID=223412).

Right now the workaround is to wait for a timeout (about 15 minutes), then
manually delete the VPC from the console, which will force-delete dependencies.
</small>

### TODOs

- Flesh out unit tests for solids
- Wire Spark up to EMR cluster
- Add config option for local ingestion (Postgres)
- Add config option for Spark running on EMR cluster
- Document running the pipeline and tests (e.g., postgres requirements)
- Add expectations
- Maybe implement templated SQL handling
- Add sub-DAG tests

### Issues with general availability
- Right now the pulumi spec, as well as the sample config, expect that you will
be able to orchestrate a Redshift cluster at `db.dagster.io` and an EMR cluster
at `spark.dagster.io`. If you are running this demo and you do not control
`dagster.io`, you will need to edit both the pulumi project and the config to
point these at DNS you do control.
